{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 06 - RNNs, part 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "P59NYU98GCb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "6511e974-176a-4064-f825-5772a55308e3"
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip install -qq bokeh==0.13.0\n",
        "!pip install -qq gensim==3.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x58dcc000 @  0x7fe1d929f2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8sVtGHmA9aBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6CNKM3b4hT1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Рекуррентные нейронные сети, часть 2"
      ]
    },
    {
      "metadata": {
        "id": "O_XkoGNQUeGm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "metadata": {
        "id": "QFEtWrS_4rUs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы уже посмотрели на применение рекуррентных сетей для классификации.\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg =x250)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Перейдем к ещё одному варианту - sequence labeling (последняя картинка).\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "metadata": {
        "id": "EPIkKdFlHB-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "metadata": {
        "id": "TiA2dGmgF1rW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "009d593c-c9ba-4049-a32c-f2297227cf1e"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d93g_swyJA_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "metadata": {
        "id": "QstS4NO0L97c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "a08c5d53-c35b-4def-c897-2033ead601f4"
      },
      "cell_type": "code",
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "epdW8u_YXcAv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "metadata": {
        "id": "xTai8Ta0lgwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1507fe46-1fb5-4ad0-8bc8-eb8276f7e502"
      },
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eChdLNGtXyP0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "metadata": {
        "id": "pCjwwDs6Zq9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f162fc88-7de1-4977-c8ff-ceb7be8ebef5"
      },
      "cell_type": "code",
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'ADP', 'VERB', 'DET', 'NOUN', 'CONJ', 'ADJ', 'PRON', '.', 'X', 'NUM', 'PRT', 'ADV'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "URC1B2nvPGFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "85385c86-6550-443b-cb9d-4d238331dff0"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAEvCAYAAAAEpLawAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCmAokyRJKoChTJIkqQCGMkmSpAL8f7diav3wsPnbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9e7b2a0390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gArQwbzWWkgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png =x150)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "metadata": {
        "id": "5rWmSToIaeAo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f6c184b-921c-40e3-b7ec-42bffd9a4bd0"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "07Ymb_MkbWsF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "metadata": {
        "id": "vjz_Rk0bbMyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1bd40b9-f86c-440a-db14-06d26e763cb1"
      },
      "cell_type": "code",
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uWMw6QHvbaDd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "metadata": {
        "id": "8XCuxEBVbOY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4442d6ef-6569-4c68-d53b-5ddbbf4f2158"
      },
      "cell_type": "code",
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4t3xyYd__8d-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Делаемся рекуррентнее\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:\n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png =x400)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "metadata": {
        "id": "RtRbz1SwgEqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DhsTKZalfih6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4XsRII5kW5x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1088
        },
        "outputId": "d4fe6cb1-13a0-4b87-fc68-9da9b127efb2"
      },
      "cell_type": "code",
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch, y_batch"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[34628., 14750., 14270., 28768.],\n",
              "        [17090., 12203.,   460., 24701.],\n",
              "        [30266.,   462.,  9901., 34916.],\n",
              "        [28768., 30914., 18986., 41509.],\n",
              "        [ 5183., 32119., 30266.,   462.],\n",
              "        [  462., 34426., 22808.,  1468.],\n",
              "        [28768., 31402.,   462., 11961.],\n",
              "        [38620., 20903., 25977., 23294.],\n",
              "        [41044., 43688., 13008., 39582.],\n",
              "        [10047., 28768., 28768., 10206.],\n",
              "        [  462., 19888.,  8160.,   462.],\n",
              "        [29898., 39819., 18845., 39582.],\n",
              "        [ 8604.,   462., 28768.,  7917.],\n",
              "        [10626.,  9834., 37950., 19336.],\n",
              "        [    0., 24088., 17189.,   266.],\n",
              "        [    0., 17427.,   462., 20307.],\n",
              "        [    0., 41194., 28768.,   460.],\n",
              "        [    0., 18845.,  4238., 27230.],\n",
              "        [    0., 28768., 18845., 41050.],\n",
              "        [    0., 35018., 37679.,  3142.],\n",
              "        [    0., 18845., 24709., 30266.],\n",
              "        [    0.,  2200., 42862., 15072.],\n",
              "        [    0., 10626., 29534., 10626.],\n",
              "        [    0.,     0., 35140.,     0.],\n",
              "        [    0.,     0., 41097.,     0.],\n",
              "        [    0.,     0., 13043.,     0.],\n",
              "        [    0.,     0., 29898.,     0.],\n",
              "        [    0.,     0.,   266.,     0.],\n",
              "        [    0.,     0.,  3917.,     0.],\n",
              "        [    0.,     0., 26619.,     0.],\n",
              "        [    0.,     0.,  2970.,     0.],\n",
              "        [    0.,     0., 10626.,     0.]]), array([[ 9.,  4.,  2.,  3.],\n",
              "        [ 4.,  4.,  1.,  4.],\n",
              "        [ 1.,  8.,  4.,  2.],\n",
              "        [ 3.,  7., 12.,  6.],\n",
              "        [ 4.,  2.,  1.,  8.],\n",
              "        [ 8.,  4.,  4.,  3.],\n",
              "        [ 3.,  9.,  8.,  4.],\n",
              "        [ 4.,  9.,  4.,  6.],\n",
              "        [ 1.,  1.,  2.,  5.],\n",
              "        [ 4.,  3.,  3.,  2.],\n",
              "        [ 8.,  2.,  4.,  8.],\n",
              "        [ 7.,  4.,  1.,  5.],\n",
              "        [ 2.,  8.,  3., 11.],\n",
              "        [ 8.,  2.,  6.,  2.],\n",
              "        [ 0., 10.,  4., 12.],\n",
              "        [ 0., 12.,  8.,  2.],\n",
              "        [ 0.,  6.,  3., 11.],\n",
              "        [ 0.,  1.,  4.,  2.],\n",
              "        [ 0.,  3.,  1.,  6.],\n",
              "        [ 0.,  4.,  3.,  4.],\n",
              "        [ 0.,  1.,  3.,  1.],\n",
              "        [ 0.,  4.,  4.,  7.],\n",
              "        [ 0.,  8.,  2.,  8.],\n",
              "        [ 0.,  0.,  2.,  0.],\n",
              "        [ 0.,  0.,  2.,  0.],\n",
              "        [ 0.,  0.,  2.,  0.],\n",
              "        [ 0.,  0.,  7.,  0.],\n",
              "        [ 0.,  0., 12.,  0.],\n",
              "        [ 0.,  0.,  3.,  0.],\n",
              "        [ 0.,  0.,  6.,  0.],\n",
              "        [ 0.,  0.,  4.,  0.],\n",
              "        [ 0.,  0.,  8.,  0.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "C5I9E9P6eFYv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "metadata": {
        "id": "WVEHju54d68T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self._out_layer(self._lstm(self._emb(inputs))[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q_HA8zyheYGH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Научитесь считать accuracy (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "metadata": {
        "id": "jbrxsZ2mehWB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1893d71d-656f-448a-e156-db91b83e77ad"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "correct_count = (preds == y_batch).float().sum().item()\n",
        "total_count = np.prod(y_batch.shape).astype('float')\n",
        "accuracy = (correct_count / total_count).item()\n",
        "\n",
        "print('Accuracy = {:.2%}'.format(accuracy))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 3.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UQWCys0s5bvL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5e448e1-bdda-465b-c550-96b0bd73cf29"
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.transpose(2, 1), y_batch)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5651, grad_fn=<NllLoss2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "nSgV3NPUpcjH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "metadata": {
        "id": "FprPQ0gllo7b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                cur_correct_count = (preds == y_batch).float().sum().item()\n",
        "                cur_sum_count = np.prod(y_batch.shape).astype('float')\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pqfbeh1ltEYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "70054002-2a1c-439e-d8f0-c23f574927a4"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 20] Train: Loss = 0.31951, Accuracy = 90.96%: 100%|██████████| 572/572 [00:15<00:00, 36.23it/s]\n",
            "[1 / 20]   Val: Loss = 0.10774, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 35.95it/s]\n",
            "[2 / 20] Train: Loss = 0.10206, Accuracy = 96.80%: 100%|██████████| 572/572 [00:15<00:00, 34.48it/s]\n",
            "[2 / 20]   Val: Loss = 0.07690, Accuracy = 97.93%: 100%|██████████| 13/13 [00:00<00:00, 35.53it/s]\n",
            "[3 / 20] Train: Loss = 0.06832, Accuracy = 97.86%: 100%|██████████| 572/572 [00:15<00:00, 36.82it/s]\n",
            "[3 / 20]   Val: Loss = 0.07002, Accuracy = 98.27%: 100%|██████████| 13/13 [00:00<00:00, 36.58it/s]\n",
            "[4 / 20] Train: Loss = 0.05117, Accuracy = 98.38%: 100%|██████████| 572/572 [00:15<00:00, 36.59it/s]\n",
            "[4 / 20]   Val: Loss = 0.06833, Accuracy = 98.42%: 100%|██████████| 13/13 [00:00<00:00, 36.34it/s]\n",
            "[5 / 20] Train: Loss = 0.04108, Accuracy = 98.69%: 100%|██████████| 572/572 [00:15<00:00, 36.90it/s]\n",
            "[5 / 20]   Val: Loss = 0.06911, Accuracy = 98.52%: 100%|██████████| 13/13 [00:00<00:00, 35.90it/s]\n",
            "[6 / 20] Train: Loss = 0.03346, Accuracy = 98.92%: 100%|██████████| 572/572 [00:15<00:00, 36.83it/s]\n",
            "[6 / 20]   Val: Loss = 0.06444, Accuracy = 98.64%: 100%|██████████| 13/13 [00:00<00:00, 35.42it/s]\n",
            "[7 / 20] Train: Loss = 0.02788, Accuracy = 99.09%: 100%|██████████| 572/572 [00:15<00:00, 37.06it/s]\n",
            "[7 / 20]   Val: Loss = 0.06568, Accuracy = 98.64%: 100%|██████████| 13/13 [00:00<00:00, 35.06it/s]\n",
            "[8 / 20] Train: Loss = 0.02308, Accuracy = 99.25%: 100%|██████████| 572/572 [00:15<00:00, 36.85it/s]\n",
            "[8 / 20]   Val: Loss = 0.06516, Accuracy = 98.71%: 100%|██████████| 13/13 [00:00<00:00, 33.95it/s]\n",
            "[9 / 20] Train: Loss = 0.01924, Accuracy = 99.37%: 100%|██████████| 572/572 [00:15<00:00, 36.35it/s]\n",
            "[9 / 20]   Val: Loss = 0.07315, Accuracy = 98.63%: 100%|██████████| 13/13 [00:00<00:00, 35.58it/s]\n",
            "[10 / 20] Train: Loss = 0.01615, Accuracy = 99.47%: 100%|██████████| 572/572 [00:15<00:00, 36.60it/s]\n",
            "[10 / 20]   Val: Loss = 0.07542, Accuracy = 98.56%: 100%|██████████| 13/13 [00:00<00:00, 39.09it/s]\n",
            "[11 / 20] Train: Loss = 0.01338, Accuracy = 99.57%: 100%|██████████| 572/572 [00:15<00:00, 36.51it/s]\n",
            "[11 / 20]   Val: Loss = 0.06880, Accuracy = 98.75%: 100%|██████████| 13/13 [00:00<00:00, 34.26it/s]\n",
            "[12 / 20] Train: Loss = 0.01116, Accuracy = 99.65%: 100%|██████████| 572/572 [00:15<00:00, 36.55it/s]\n",
            "[12 / 20]   Val: Loss = 0.07720, Accuracy = 98.67%: 100%|██████████| 13/13 [00:00<00:00, 35.36it/s]\n",
            "[13 / 20] Train: Loss = 0.00907, Accuracy = 99.71%: 100%|██████████| 572/572 [00:15<00:00, 37.13it/s]\n",
            "[13 / 20]   Val: Loss = 0.07431, Accuracy = 98.73%: 100%|██████████| 13/13 [00:00<00:00, 33.32it/s]\n",
            "[14 / 20] Train: Loss = 0.00754, Accuracy = 99.77%: 100%|██████████| 572/572 [00:15<00:00, 36.71it/s]\n",
            "[14 / 20]   Val: Loss = 0.07663, Accuracy = 98.74%: 100%|██████████| 13/13 [00:00<00:00, 33.60it/s]\n",
            "[15 / 20] Train: Loss = 0.00624, Accuracy = 99.81%: 100%|██████████| 572/572 [00:15<00:00, 36.50it/s]\n",
            "[15 / 20]   Val: Loss = 0.08493, Accuracy = 98.66%: 100%|██████████| 13/13 [00:00<00:00, 34.52it/s]\n",
            "[16 / 20] Train: Loss = 0.00497, Accuracy = 99.86%: 100%|██████████| 572/572 [00:15<00:00, 38.21it/s]\n",
            "[16 / 20]   Val: Loss = 0.08475, Accuracy = 98.71%: 100%|██████████| 13/13 [00:00<00:00, 34.25it/s]\n",
            "[17 / 20] Train: Loss = 0.00423, Accuracy = 99.88%: 100%|██████████| 572/572 [00:15<00:00, 36.80it/s]\n",
            "[17 / 20]   Val: Loss = 0.09083, Accuracy = 98.65%: 100%|██████████| 13/13 [00:00<00:00, 34.78it/s]\n",
            "[18 / 20] Train: Loss = 0.00357, Accuracy = 99.90%: 100%|██████████| 572/572 [00:15<00:00, 36.84it/s]\n",
            "[18 / 20]   Val: Loss = 0.10354, Accuracy = 98.56%: 100%|██████████| 13/13 [00:00<00:00, 34.99it/s]\n",
            "[19 / 20] Train: Loss = 0.00293, Accuracy = 99.92%: 100%|██████████| 572/572 [00:15<00:00, 39.18it/s]\n",
            "[19 / 20]   Val: Loss = 0.10194, Accuracy = 98.65%: 100%|██████████| 13/13 [00:00<00:00, 34.31it/s]\n",
            "[20 / 20] Train: Loss = 0.00256, Accuracy = 99.93%: 100%|██████████| 572/572 [00:15<00:00, 36.46it/s]\n",
            "[20 / 20]   Val: Loss = 0.10755, Accuracy = 98.59%: 100%|██████████| 13/13 [00:00<00:00, 35.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "m0qGetIhfUE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "metadata": {
        "id": "gH85It8v9nBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd90d500-6346-44b7-c23f-3a76ee46a8ca"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "mask = (y_batch != 0).float()\n",
        "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "total_count = mask.sum().item()\n",
        "accuracy = (correct_count / total_count)\n",
        "\n",
        "print('Accuracy = {:.2%}'.format(accuracy))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 5.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N1yN1XUn-Top",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d69116d-7774-4289-806a-0f3f5927c777"
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "criterion(logits.transpose(2, 1), y_batch)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5934, grad_fn=<NllLoss2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "mDAERZL_-QOY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                mask = (y_batch != 0).float()\n",
        "                cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "                cur_sum_count = mask.sum().item()\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kMM8fD1593_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "02b4d814-48f6-4bc0-8847-9a5c196823f7"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 20] Train: Loss = 0.68432, Accuracy = 78.37%: 100%|██████████| 572/572 [00:15<00:00, 36.21it/s]\n",
            "[1 / 20]   Val: Loss = 0.36245, Accuracy = 88.30%: 100%|██████████| 13/13 [00:00<00:00, 35.03it/s]\n",
            "[2 / 20] Train: Loss = 0.27451, Accuracy = 90.89%: 100%|██████████| 572/572 [00:15<00:00, 36.40it/s]\n",
            "[2 / 20]   Val: Loss = 0.24577, Accuracy = 92.24%: 100%|██████████| 13/13 [00:00<00:00, 35.62it/s]\n",
            "[3 / 20] Train: Loss = 0.18544, Accuracy = 93.87%: 100%|██████████| 572/572 [00:15<00:00, 35.44it/s]\n",
            "[3 / 20]   Val: Loss = 0.19940, Accuracy = 93.90%: 100%|██████████| 13/13 [00:00<00:00, 35.60it/s]\n",
            "[4 / 20] Train: Loss = 0.13808, Accuracy = 95.41%: 100%|██████████| 572/572 [00:15<00:00, 36.26it/s]\n",
            "[4 / 20]   Val: Loss = 0.17781, Accuracy = 94.68%: 100%|██████████| 13/13 [00:00<00:00, 38.43it/s]\n",
            "[5 / 20] Train: Loss = 0.10724, Accuracy = 96.41%: 100%|██████████| 572/572 [00:15<00:00, 36.51it/s]\n",
            "[5 / 20]   Val: Loss = 0.16920, Accuracy = 94.92%: 100%|██████████| 13/13 [00:00<00:00, 34.57it/s]\n",
            "[6 / 20] Train: Loss = 0.08572, Accuracy = 97.11%: 100%|██████████| 572/572 [00:15<00:00, 36.41it/s]\n",
            "[6 / 20]   Val: Loss = 0.16291, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 35.40it/s]\n",
            "[7 / 20] Train: Loss = 0.06911, Accuracy = 97.66%: 100%|██████████| 572/572 [00:15<00:00, 38.36it/s]\n",
            "[7 / 20]   Val: Loss = 0.16022, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 37.06it/s]\n",
            "[8 / 20] Train: Loss = 0.05611, Accuracy = 98.11%: 100%|██████████| 572/572 [00:15<00:00, 37.83it/s]\n",
            "[8 / 20]   Val: Loss = 0.16309, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 35.01it/s]\n",
            "[9 / 20] Train: Loss = 0.04593, Accuracy = 98.45%: 100%|██████████| 572/572 [00:15<00:00, 36.25it/s]\n",
            "[9 / 20]   Val: Loss = 0.16998, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 35.08it/s]\n",
            "[10 / 20] Train: Loss = 0.03779, Accuracy = 98.74%: 100%|██████████| 572/572 [00:15<00:00, 36.41it/s]\n",
            "[10 / 20]   Val: Loss = 0.17649, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 35.67it/s]\n",
            "[11 / 20] Train: Loss = 0.03099, Accuracy = 98.98%: 100%|██████████| 572/572 [00:15<00:00, 36.06it/s]\n",
            "[11 / 20]   Val: Loss = 0.18763, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 34.91it/s]\n",
            "[12 / 20] Train: Loss = 0.02543, Accuracy = 99.18%: 100%|██████████| 572/572 [00:15<00:00, 36.49it/s]\n",
            "[12 / 20]   Val: Loss = 0.19550, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 35.35it/s]\n",
            "[13 / 20] Train: Loss = 0.02085, Accuracy = 99.34%: 100%|██████████| 572/572 [00:15<00:00, 36.33it/s]\n",
            "[13 / 20]   Val: Loss = 0.20732, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 34.22it/s]\n",
            "[14 / 20] Train: Loss = 0.01692, Accuracy = 99.48%: 100%|██████████| 572/572 [00:15<00:00, 36.51it/s]\n",
            "[14 / 20]   Val: Loss = 0.22047, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 35.68it/s]\n",
            "[15 / 20] Train: Loss = 0.01409, Accuracy = 99.57%: 100%|██████████| 572/572 [00:15<00:00, 37.53it/s]\n",
            "[15 / 20]   Val: Loss = 0.23155, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 34.35it/s]\n",
            "[16 / 20] Train: Loss = 0.01148, Accuracy = 99.65%: 100%|██████████| 572/572 [00:15<00:00, 36.33it/s]\n",
            "[16 / 20]   Val: Loss = 0.24358, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 35.66it/s]\n",
            "[17 / 20] Train: Loss = 0.00957, Accuracy = 99.73%: 100%|██████████| 572/572 [00:15<00:00, 36.30it/s]\n",
            "[17 / 20]   Val: Loss = 0.25426, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 36.06it/s]\n",
            "[18 / 20] Train: Loss = 0.00848, Accuracy = 99.75%: 100%|██████████| 572/572 [00:15<00:00, 36.51it/s]\n",
            "[18 / 20]   Val: Loss = 0.26797, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 33.59it/s]\n",
            "[19 / 20] Train: Loss = 0.00708, Accuracy = 99.79%: 100%|██████████| 572/572 [00:15<00:00, 36.40it/s]\n",
            "[19 / 20]   Val: Loss = 0.27776, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 37.63it/s]\n",
            "[20 / 20] Train: Loss = 0.00652, Accuracy = 99.80%: 100%|██████████| 572/572 [00:15<00:00, 36.56it/s]\n",
            "[20 / 20]   Val: Loss = 0.28434, Accuracy = 95.21%: 100%|██████████| 13/13 [00:00<00:00, 34.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nAfV2dEOfHo5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Посчитайте качество модели на тесте"
      ]
    },
    {
      "metadata": {
        "id": "xwwENLfD9afm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6ca35be3-1cef-49ec-cb2d-3e6ebb2d3824"
      },
      "cell_type": "code",
      "source": [
        "do_epoch(model, criterion, (X_test, y_test), 256)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.28378, Accuracy = 95.28%: 100%|██████████| 56/56 [00:01<00:00, 54.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2837821585791452, 0.9527763720052742)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "PXUTSFaEHbDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png =x450)  \n",
        "from [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "metadata": {
        "id": "OyPdFaqm_7Fz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self._out_layer(self._lstm(self._emb(inputs))[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hy1XAKmtAKfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "3d0a6dee-53e3-4bd4-8572-ed34ddf5f8d4"
      },
      "cell_type": "code",
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 20] Train: Loss = 0.55103, Accuracy = 82.46%: 100%|██████████| 572/572 [00:19<00:00, 28.27it/s]\n",
            "[1 / 20]   Val: Loss = 0.27885, Accuracy = 91.06%: 100%|██████████| 13/13 [00:00<00:00, 29.47it/s]\n",
            "[2 / 20] Train: Loss = 0.20631, Accuracy = 93.43%: 100%|██████████| 572/572 [00:19<00:00, 29.50it/s]\n",
            "[2 / 20]   Val: Loss = 0.17814, Accuracy = 94.39%: 100%|██████████| 13/13 [00:00<00:00, 29.02it/s]\n",
            "[3 / 20] Train: Loss = 0.13118, Accuracy = 95.96%: 100%|██████████| 572/572 [00:19<00:00, 29.40it/s]\n",
            "[3 / 20]   Val: Loss = 0.14404, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 29.36it/s]\n",
            "[4 / 20] Train: Loss = 0.08945, Accuracy = 97.26%: 100%|██████████| 572/572 [00:19<00:00, 28.69it/s]\n",
            "[4 / 20]   Val: Loss = 0.12853, Accuracy = 96.06%: 100%|██████████| 13/13 [00:00<00:00, 29.34it/s]\n",
            "[5 / 20] Train: Loss = 0.06185, Accuracy = 98.15%: 100%|██████████| 572/572 [00:20<00:00, 28.56it/s]\n",
            "[5 / 20]   Val: Loss = 0.11728, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 29.04it/s]\n",
            "[6 / 20] Train: Loss = 0.04221, Accuracy = 98.78%: 100%|██████████| 572/572 [00:19<00:00, 28.53it/s] \n",
            "[6 / 20]   Val: Loss = 0.11653, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 28.66it/s]\n",
            "[7 / 20] Train: Loss = 0.02841, Accuracy = 99.22%: 100%|██████████| 572/572 [00:20<00:00, 27.76it/s]\n",
            "[7 / 20]   Val: Loss = 0.12271, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 24.79it/s]\n",
            "[8 / 20] Train: Loss = 0.01869, Accuracy = 99.53%: 100%|██████████| 572/572 [00:22<00:00, 27.39it/s]\n",
            "[8 / 20]   Val: Loss = 0.12223, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 25.09it/s]\n",
            "[9 / 20] Train: Loss = 0.01212, Accuracy = 99.72%: 100%|██████████| 572/572 [00:23<00:00, 26.96it/s] \n",
            "[9 / 20]   Val: Loss = 0.13111, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 25.58it/s]\n",
            "[10 / 20] Train: Loss = 0.00777, Accuracy = 99.84%: 100%|██████████| 572/572 [00:23<00:00, 24.72it/s]\n",
            "[10 / 20]   Val: Loss = 0.13470, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 25.63it/s]\n",
            "[11 / 20] Train: Loss = 0.00495, Accuracy = 99.92%: 100%|██████████| 572/572 [00:22<00:00, 25.23it/s] \n",
            "[11 / 20]   Val: Loss = 0.14942, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 25.43it/s]\n",
            "[12 / 20] Train: Loss = 0.00313, Accuracy = 99.95%: 100%|██████████| 572/572 [00:21<00:00, 26.00it/s]\n",
            "[12 / 20]   Val: Loss = 0.15002, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 29.48it/s]\n",
            "[13 / 20] Train: Loss = 0.00200, Accuracy = 99.98%: 100%|██████████| 572/572 [00:19<00:00, 29.21it/s]\n",
            "[13 / 20]   Val: Loss = 0.15590, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 29.78it/s]\n",
            "[14 / 20] Train: Loss = 0.00129, Accuracy = 99.99%: 100%|██████████| 572/572 [00:19<00:00, 29.24it/s]\n",
            "[14 / 20]   Val: Loss = 0.16066, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 30.21it/s]\n",
            "[15 / 20] Train: Loss = 0.00095, Accuracy = 99.99%: 100%|██████████| 572/572 [00:19<00:00, 29.12it/s]\n",
            "[15 / 20]   Val: Loss = 0.16496, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 30.38it/s]\n",
            "[16 / 20] Train: Loss = 0.00358, Accuracy = 99.90%: 100%|██████████| 572/572 [00:19<00:00, 29.32it/s]\n",
            "[16 / 20]   Val: Loss = 0.17361, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 28.84it/s]\n",
            "[17 / 20] Train: Loss = 0.00279, Accuracy = 99.93%: 100%|██████████| 572/572 [00:20<00:00, 28.07it/s]\n",
            "[17 / 20]   Val: Loss = 0.17317, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 28.85it/s]\n",
            "[18 / 20] Train: Loss = 0.00081, Accuracy = 99.99%: 100%|██████████| 572/572 [00:19<00:00, 24.81it/s] \n",
            "[18 / 20]   Val: Loss = 0.18123, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 28.31it/s]\n",
            "[19 / 20] Train: Loss = 0.00033, Accuracy = 100.00%: 100%|██████████| 572/572 [00:19<00:00, 29.05it/s]\n",
            "[19 / 20]   Val: Loss = 0.18030, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 29.49it/s]\n",
            "[20 / 20] Train: Loss = 0.00020, Accuracy = 100.00%: 100%|██████████| 572/572 [00:19<00:00, 29.32it/s]\n",
            "[20 / 20]   Val: Loss = 0.18287, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 29.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XGEQeH_wB-4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e3650dbb-3ec9-4a27-8dac-cd979b247091"
      },
      "cell_type": "code",
      "source": [
        "do_epoch(model, criterion, (X_test, y_test), 256)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.18093, Accuracy = 96.79%: 100%|██████████| 56/56 [00:01<00:00, 41.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.18092933988996915, 0.9679001890046787)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "ZTXmYGD_ANhm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "metadata": {
        "id": "uZpY_Q1xZ18h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1296df33-d9d9-431d-fdbf-b2edf7b7e9d6"
      },
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KYogOoKlgtcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "metadata": {
        "id": "VsCstxiO03oT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "daaa6ddb-625a-4188-8866-833f7f6418ef"
      },
      "cell_type": "code",
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HcG7i-R8hbY3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "metadata": {
        "id": "LxaRBpQd0pat",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._embs = nn.Embedding.from_pretrained(embeddings)\n",
        "        self._lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        if isinstance(inputs, LongTensor):\n",
        "            embs = self._embs(inputs)\n",
        "        else:\n",
        "            embs = inputs\n",
        "        return self._out_layer(self._lstm(embs)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EBtI6BDE-Fc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "63ea1325-3c84-48cc-ca30-5083acf1fa84"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.76694, Accuracy = 77.47%: 100%|██████████| 572/572 [00:11<00:00, 47.71it/s]\n",
            "[1 / 50]   Val: Loss = 0.37361, Accuracy = 89.00%: 100%|██████████| 13/13 [00:00<00:00, 39.27it/s]\n",
            "[2 / 50] Train: Loss = 0.28320, Accuracy = 91.54%: 100%|██████████| 572/572 [00:12<00:00, 47.65it/s]\n",
            "[2 / 50]   Val: Loss = 0.25508, Accuracy = 92.19%: 100%|██████████| 13/13 [00:00<00:00, 40.61it/s]\n",
            "[3 / 50] Train: Loss = 0.20778, Accuracy = 93.59%: 100%|██████████| 572/572 [00:11<00:00, 47.69it/s]\n",
            "[3 / 50]   Val: Loss = 0.20888, Accuracy = 93.43%: 100%|██████████| 13/13 [00:00<00:00, 37.87it/s]\n",
            "[4 / 50] Train: Loss = 0.17279, Accuracy = 94.55%: 100%|██████████| 572/572 [00:11<00:00, 46.18it/s]\n",
            "[4 / 50]   Val: Loss = 0.18608, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 38.06it/s]\n",
            "[5 / 50] Train: Loss = 0.15243, Accuracy = 95.08%: 100%|██████████| 572/572 [00:11<00:00, 48.07it/s]\n",
            "[5 / 50]   Val: Loss = 0.17057, Accuracy = 94.49%: 100%|██████████| 13/13 [00:00<00:00, 38.25it/s]\n",
            "[6 / 50] Train: Loss = 0.13889, Accuracy = 95.46%: 100%|██████████| 572/572 [00:11<00:00, 48.52it/s]\n",
            "[6 / 50]   Val: Loss = 0.16015, Accuracy = 94.74%: 100%|██████████| 13/13 [00:00<00:00, 37.83it/s]\n",
            "[7 / 50] Train: Loss = 0.12905, Accuracy = 95.71%: 100%|██████████| 572/572 [00:11<00:00, 48.30it/s]\n",
            "[7 / 50]   Val: Loss = 0.15460, Accuracy = 94.80%: 100%|██████████| 13/13 [00:00<00:00, 38.51it/s]\n",
            "[8 / 50] Train: Loss = 0.12148, Accuracy = 95.92%: 100%|██████████| 572/572 [00:11<00:00, 48.31it/s]\n",
            "[8 / 50]   Val: Loss = 0.14916, Accuracy = 94.98%: 100%|██████████| 13/13 [00:00<00:00, 38.96it/s]\n",
            "[9 / 50] Train: Loss = 0.11577, Accuracy = 96.09%: 100%|██████████| 572/572 [00:11<00:00, 48.05it/s]\n",
            "[9 / 50]   Val: Loss = 0.14605, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 41.25it/s]\n",
            "[10 / 50] Train: Loss = 0.11123, Accuracy = 96.19%: 100%|██████████| 572/572 [00:11<00:00, 50.65it/s]\n",
            "[10 / 50]   Val: Loss = 0.14332, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 39.74it/s]\n",
            "[11 / 50] Train: Loss = 0.10721, Accuracy = 96.33%: 100%|██████████| 572/572 [00:11<00:00, 47.89it/s]\n",
            "[11 / 50]   Val: Loss = 0.14012, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 39.15it/s]\n",
            "[12 / 50] Train: Loss = 0.10373, Accuracy = 96.43%: 100%|██████████| 572/572 [00:12<00:00, 47.55it/s]\n",
            "[12 / 50]   Val: Loss = 0.13819, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 37.69it/s]\n",
            "[13 / 50] Train: Loss = 0.10092, Accuracy = 96.53%: 100%|██████████| 572/572 [00:12<00:00, 47.21it/s]\n",
            "[13 / 50]   Val: Loss = 0.13665, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 39.60it/s]\n",
            "[14 / 50] Train: Loss = 0.09843, Accuracy = 96.59%: 100%|██████████| 572/572 [00:12<00:00, 47.24it/s]\n",
            "[14 / 50]   Val: Loss = 0.13539, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 38.01it/s]\n",
            "[15 / 50] Train: Loss = 0.09614, Accuracy = 96.66%: 100%|██████████| 572/572 [00:11<00:00, 48.20it/s]\n",
            "[15 / 50]   Val: Loss = 0.13623, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 38.97it/s]\n",
            "[16 / 50] Train: Loss = 0.09396, Accuracy = 96.72%: 100%|██████████| 572/572 [00:11<00:00, 48.38it/s]\n",
            "[16 / 50]   Val: Loss = 0.13419, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 37.79it/s]\n",
            "[17 / 50] Train: Loss = 0.09225, Accuracy = 96.78%: 100%|██████████| 572/572 [00:11<00:00, 48.25it/s]\n",
            "[17 / 50]   Val: Loss = 0.13452, Accuracy = 95.47%: 100%|██████████| 13/13 [00:00<00:00, 37.12it/s]\n",
            "[18 / 50] Train: Loss = 0.09023, Accuracy = 96.83%: 100%|██████████| 572/572 [00:11<00:00, 48.04it/s]\n",
            "[18 / 50]   Val: Loss = 0.13314, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 41.25it/s]\n",
            "[19 / 50] Train: Loss = 0.08880, Accuracy = 96.89%: 100%|██████████| 572/572 [00:11<00:00, 47.94it/s]\n",
            "[19 / 50]   Val: Loss = 0.13363, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 38.85it/s]\n",
            "[20 / 50] Train: Loss = 0.08719, Accuracy = 96.92%: 100%|██████████| 572/572 [00:11<00:00, 49.94it/s]\n",
            "[20 / 50]   Val: Loss = 0.13274, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 38.38it/s]\n",
            "[21 / 50] Train: Loss = 0.08592, Accuracy = 96.97%: 100%|██████████| 572/572 [00:11<00:00, 47.93it/s]\n",
            "[21 / 50]   Val: Loss = 0.13309, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 39.34it/s]\n",
            "[22 / 50] Train: Loss = 0.08440, Accuracy = 97.02%: 100%|██████████| 572/572 [00:11<00:00, 48.71it/s]\n",
            "[22 / 50]   Val: Loss = 0.13462, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 39.67it/s]\n",
            "[23 / 50] Train: Loss = 0.08315, Accuracy = 97.06%: 100%|██████████| 572/572 [00:11<00:00, 48.00it/s]\n",
            "[23 / 50]   Val: Loss = 0.13610, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 37.62it/s]\n",
            "[24 / 50] Train: Loss = 0.08217, Accuracy = 97.09%: 100%|██████████| 572/572 [00:11<00:00, 48.06it/s]\n",
            "[24 / 50]   Val: Loss = 0.13276, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 38.24it/s]\n",
            "[25 / 50] Train: Loss = 0.08086, Accuracy = 97.13%: 100%|██████████| 572/572 [00:11<00:00, 48.27it/s]\n",
            "[25 / 50]   Val: Loss = 0.13321, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 38.53it/s]\n",
            "[26 / 50] Train: Loss = 0.07967, Accuracy = 97.18%: 100%|██████████| 572/572 [00:11<00:00, 47.68it/s]\n",
            "[26 / 50]   Val: Loss = 0.13327, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 38.59it/s]\n",
            "[27 / 50] Train: Loss = 0.07883, Accuracy = 97.19%: 100%|██████████| 572/572 [00:12<00:00, 47.35it/s]\n",
            "[27 / 50]   Val: Loss = 0.13403, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 36.27it/s]\n",
            "[28 / 50] Train: Loss = 0.07765, Accuracy = 97.24%: 100%|██████████| 572/572 [00:11<00:00, 49.38it/s]\n",
            "[28 / 50]   Val: Loss = 0.13409, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 38.80it/s]\n",
            "[29 / 50] Train: Loss = 0.07687, Accuracy = 97.26%: 100%|██████████| 572/572 [00:12<00:00, 47.63it/s]\n",
            "[29 / 50]   Val: Loss = 0.13483, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 39.35it/s]\n",
            "[30 / 50] Train: Loss = 0.07594, Accuracy = 97.30%: 100%|██████████| 572/572 [00:11<00:00, 48.13it/s]\n",
            "[30 / 50]   Val: Loss = 0.13501, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 39.56it/s]\n",
            "[31 / 50] Train: Loss = 0.07499, Accuracy = 97.33%: 100%|██████████| 572/572 [00:11<00:00, 48.13it/s]\n",
            "[31 / 50]   Val: Loss = 0.13524, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 40.11it/s]\n",
            "[32 / 50] Train: Loss = 0.07425, Accuracy = 97.36%: 100%|██████████| 572/572 [00:11<00:00, 48.33it/s]\n",
            "[32 / 50]   Val: Loss = 0.13562, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 38.90it/s]\n",
            "[33 / 50] Train: Loss = 0.07335, Accuracy = 97.39%: 100%|██████████| 572/572 [00:11<00:00, 48.19it/s]\n",
            "[33 / 50]   Val: Loss = 0.13772, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 37.64it/s]\n",
            "[34 / 50] Train: Loss = 0.07240, Accuracy = 97.43%: 100%|██████████| 572/572 [00:11<00:00, 48.62it/s]\n",
            "[34 / 50]   Val: Loss = 0.13747, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 38.66it/s]\n",
            "[35 / 50] Train: Loss = 0.07179, Accuracy = 97.45%: 100%|██████████| 572/572 [00:11<00:00, 48.19it/s]\n",
            "[35 / 50]   Val: Loss = 0.13715, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 39.47it/s]\n",
            "[36 / 50] Train: Loss = 0.07109, Accuracy = 97.46%: 100%|██████████| 572/572 [00:11<00:00, 47.84it/s]\n",
            "[36 / 50]   Val: Loss = 0.13866, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 37.38it/s]\n",
            "[37 / 50] Train: Loss = 0.07043, Accuracy = 97.49%: 100%|██████████| 572/572 [00:12<00:00, 46.88it/s]\n",
            "[37 / 50]   Val: Loss = 0.13813, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 38.98it/s]\n",
            "[38 / 50] Train: Loss = 0.06970, Accuracy = 97.52%: 100%|██████████| 572/572 [00:12<00:00, 46.75it/s]\n",
            "[38 / 50]   Val: Loss = 0.13777, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 39.08it/s]\n",
            "[39 / 50] Train: Loss = 0.06892, Accuracy = 97.53%: 100%|██████████| 572/572 [00:12<00:00, 46.11it/s]\n",
            "[39 / 50]   Val: Loss = 0.14058, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 36.46it/s]\n",
            "[40 / 50] Train: Loss = 0.06855, Accuracy = 97.55%: 100%|██████████| 572/572 [00:12<00:00, 47.14it/s]\n",
            "[40 / 50]   Val: Loss = 0.13973, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 40.04it/s]\n",
            "[41 / 50] Train: Loss = 0.06753, Accuracy = 97.58%: 100%|██████████| 572/572 [00:12<00:00, 46.76it/s]\n",
            "[41 / 50]   Val: Loss = 0.13998, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 39.45it/s]\n",
            "[42 / 50] Train: Loss = 0.06708, Accuracy = 97.61%: 100%|██████████| 572/572 [00:12<00:00, 46.95it/s]\n",
            "[42 / 50]   Val: Loss = 0.14158, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 36.60it/s]\n",
            "[43 / 50] Train: Loss = 0.06657, Accuracy = 97.62%: 100%|██████████| 572/572 [00:12<00:00, 46.98it/s]\n",
            "[43 / 50]   Val: Loss = 0.14365, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 39.27it/s]\n",
            "[44 / 50] Train: Loss = 0.06616, Accuracy = 97.63%: 100%|██████████| 572/572 [00:12<00:00, 48.16it/s]\n",
            "[44 / 50]   Val: Loss = 0.14284, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 39.44it/s]\n",
            "[45 / 50] Train: Loss = 0.06532, Accuracy = 97.67%: 100%|██████████| 572/572 [00:12<00:00, 45.02it/s]\n",
            "[45 / 50]   Val: Loss = 0.14668, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 38.75it/s]\n",
            "[46 / 50] Train: Loss = 0.06473, Accuracy = 97.69%: 100%|██████████| 572/572 [00:12<00:00, 47.17it/s]\n",
            "[46 / 50]   Val: Loss = 0.14371, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 39.25it/s]\n",
            "[47 / 50] Train: Loss = 0.06411, Accuracy = 97.72%: 100%|██████████| 572/572 [00:12<00:00, 47.29it/s]\n",
            "[47 / 50]   Val: Loss = 0.14419, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 39.38it/s]\n",
            "[48 / 50] Train: Loss = 0.06365, Accuracy = 97.73%: 100%|██████████| 572/572 [00:12<00:00, 47.28it/s]\n",
            "[48 / 50]   Val: Loss = 0.14601, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 37.28it/s]\n",
            "[49 / 50] Train: Loss = 0.06331, Accuracy = 97.75%: 100%|██████████| 572/572 [00:11<00:00, 47.70it/s]\n",
            "[49 / 50]   Val: Loss = 0.14593, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 39.47it/s]\n",
            "[50 / 50] Train: Loss = 0.06278, Accuracy = 97.76%: 100%|██████████| 572/572 [00:12<00:00, 47.22it/s]\n",
            "[50 / 50]   Val: Loss = 0.14679, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 38.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Ne_8f24h8kg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги."
      ]
    },
    {
      "metadata": {
        "id": "Sp-JHz0xrojq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f49de6f7-d397-4396-9ef4-073dd6ac2eeb"
      },
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "for sentence in test_data:\n",
        "    embs = []\n",
        "    tags = []\n",
        "    for word, tag in sentence:\n",
        "        try:\n",
        "            word_emb = FloatTensor(w2v_model.get_vector(word.lower())).unsqueeze(0)\n",
        "        except KeyError:\n",
        "            word_emb = FloatTensor(np.zeros(w2v_model.vectors.shape[1])).unsqueeze(0)\n",
        "        embs.append(word_emb)\n",
        "        tags.append(tag2ind[tag])\n",
        "    embs = torch.cat(embs, 0).unsqueeze(1)\n",
        "\n",
        "    logits = model(embs).argmax(-1)\n",
        "\n",
        "    correct += (logits.squeeze() == LongTensor(tags)).sum().item()\n",
        "    total += len(tags)\n",
        "\n",
        "print('Test accuracy: {:.4f}'.format(correct / total))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "enF9GAPAN3RB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Дообучение предобученных векторов\n",
        "\n",
        "**Задание** Почему бы не попробовать дообучать вектора? Для этого нужно просто заменить флаг `freeze=False` в методе `from_pretrained`. Попробуйте."
      ]
    },
    {
      "metadata": {
        "id": "5AdB6olUiyf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._embs = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
        "        self._lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        if isinstance(inputs, LongTensor):\n",
        "            embs = self._embs(inputs)\n",
        "        else:\n",
        "            embs = inputs\n",
        "        return self._out_layer(self._lstm(embs)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJwNtWO2z6RJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "9450d74b-6c50-4d95-fc84-415d963fe1c8"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.53046, Accuracy = 84.63%: 100%|██████████| 572/572 [00:16<00:00, 35.59it/s]\n",
            "[1 / 50]   Val: Loss = 0.17043, Accuracy = 94.66%: 100%|██████████| 13/13 [00:00<00:00, 42.44it/s]\n",
            "[2 / 50] Train: Loss = 0.11552, Accuracy = 96.14%: 100%|██████████| 572/572 [00:16<00:00, 35.02it/s]\n",
            "[2 / 50]   Val: Loss = 0.12753, Accuracy = 95.67%: 100%|██████████| 13/13 [00:00<00:00, 41.43it/s]\n",
            "[3 / 50] Train: Loss = 0.08215, Accuracy = 97.11%: 100%|██████████| 572/572 [00:16<00:00, 37.04it/s]\n",
            "[3 / 50]   Val: Loss = 0.12000, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 45.53it/s]\n",
            "[4 / 50] Train: Loss = 0.06677, Accuracy = 97.58%: 100%|██████████| 572/572 [00:16<00:00, 35.17it/s]\n",
            "[4 / 50]   Val: Loss = 0.11711, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 41.90it/s]\n",
            "[5 / 50] Train: Loss = 0.05725, Accuracy = 97.87%: 100%|██████████| 572/572 [00:15<00:00, 35.93it/s]\n",
            "[5 / 50]   Val: Loss = 0.11948, Accuracy = 96.03%: 100%|██████████| 13/13 [00:00<00:00, 43.67it/s]\n",
            "[6 / 50] Train: Loss = 0.05053, Accuracy = 98.13%: 100%|██████████| 572/572 [00:15<00:00, 37.39it/s]\n",
            "[6 / 50]   Val: Loss = 0.12108, Accuracy = 96.10%: 100%|██████████| 13/13 [00:00<00:00, 45.06it/s]\n",
            "[7 / 50] Train: Loss = 0.04537, Accuracy = 98.31%: 100%|██████████| 572/572 [00:15<00:00, 37.63it/s]\n",
            "[7 / 50]   Val: Loss = 0.12966, Accuracy = 96.05%: 100%|██████████| 13/13 [00:00<00:00, 47.17it/s]\n",
            "[8 / 50] Train: Loss = 0.04070, Accuracy = 98.50%: 100%|██████████| 572/572 [00:15<00:00, 37.72it/s]\n",
            "[8 / 50]   Val: Loss = 0.13217, Accuracy = 96.01%: 100%|██████████| 13/13 [00:00<00:00, 47.81it/s]\n",
            "[9 / 50] Train: Loss = 0.03672, Accuracy = 98.66%: 100%|██████████| 572/572 [00:15<00:00, 37.45it/s]\n",
            "[9 / 50]   Val: Loss = 0.13705, Accuracy = 96.01%: 100%|██████████| 13/13 [00:00<00:00, 43.62it/s]\n",
            "[10 / 50] Train: Loss = 0.03278, Accuracy = 98.81%: 100%|██████████| 572/572 [00:15<00:00, 37.84it/s]\n",
            "[10 / 50]   Val: Loss = 0.14506, Accuracy = 95.96%: 100%|██████████| 13/13 [00:00<00:00, 45.68it/s]\n",
            "[11 / 50] Train: Loss = 0.02962, Accuracy = 98.94%: 100%|██████████| 572/572 [00:15<00:00, 37.94it/s]\n",
            "[11 / 50]   Val: Loss = 0.15361, Accuracy = 95.82%: 100%|██████████| 13/13 [00:00<00:00, 46.09it/s]\n",
            "[12 / 50] Train: Loss = 0.02650, Accuracy = 99.07%: 100%|██████████| 572/572 [00:15<00:00, 37.87it/s]\n",
            "[12 / 50]   Val: Loss = 0.16319, Accuracy = 95.79%: 100%|██████████| 13/13 [00:00<00:00, 47.48it/s]\n",
            "[13 / 50] Train: Loss = 0.02370, Accuracy = 99.18%: 100%|██████████| 572/572 [00:15<00:00, 37.96it/s]\n",
            "[13 / 50]   Val: Loss = 0.17116, Accuracy = 95.72%: 100%|██████████| 13/13 [00:00<00:00, 44.20it/s]\n",
            "[14 / 50] Train: Loss = 0.02134, Accuracy = 99.26%: 100%|██████████| 572/572 [00:15<00:00, 36.69it/s]\n",
            "[14 / 50]   Val: Loss = 0.18328, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 46.68it/s]\n",
            "[15 / 50] Train: Loss = 0.01934, Accuracy = 99.34%: 100%|██████████| 572/572 [00:15<00:00, 37.96it/s]\n",
            "[15 / 50]   Val: Loss = 0.19149, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 45.02it/s]\n",
            "[16 / 50] Train: Loss = 0.01730, Accuracy = 99.41%: 100%|██████████| 572/572 [00:15<00:00, 38.06it/s]\n",
            "[16 / 50]   Val: Loss = 0.19733, Accuracy = 95.50%: 100%|██████████| 13/13 [00:00<00:00, 48.36it/s]\n",
            "[17 / 50] Train: Loss = 0.01540, Accuracy = 99.48%: 100%|██████████| 572/572 [00:15<00:00, 37.90it/s]\n",
            "[17 / 50]   Val: Loss = 0.20301, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 42.52it/s]\n",
            "[18 / 50] Train: Loss = 0.01394, Accuracy = 99.54%: 100%|██████████| 572/572 [00:15<00:00, 37.54it/s]\n",
            "[18 / 50]   Val: Loss = 0.21609, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 45.76it/s]\n",
            "[19 / 50] Train: Loss = 0.01247, Accuracy = 99.59%: 100%|██████████| 572/572 [00:15<00:00, 37.75it/s]\n",
            "[19 / 50]   Val: Loss = 0.22835, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 43.79it/s]\n",
            "[20 / 50] Train: Loss = 0.01137, Accuracy = 99.63%: 100%|██████████| 572/572 [00:15<00:00, 36.83it/s]\n",
            "[20 / 50]   Val: Loss = 0.23574, Accuracy = 95.17%: 100%|██████████| 13/13 [00:00<00:00, 42.52it/s]\n",
            "[21 / 50] Train: Loss = 0.01057, Accuracy = 99.66%: 100%|██████████| 572/572 [00:15<00:00, 37.04it/s]\n",
            "[21 / 50]   Val: Loss = 0.24547, Accuracy = 95.17%: 100%|██████████| 13/13 [00:00<00:00, 47.38it/s]\n",
            "[22 / 50] Train: Loss = 0.00959, Accuracy = 99.69%: 100%|██████████| 572/572 [00:15<00:00, 37.14it/s]\n",
            "[22 / 50]   Val: Loss = 0.25449, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 44.54it/s]\n",
            "[23 / 50] Train: Loss = 0.00882, Accuracy = 99.71%: 100%|██████████| 572/572 [00:15<00:00, 37.43it/s]\n",
            "[23 / 50]   Val: Loss = 0.26256, Accuracy = 95.15%: 100%|██████████| 13/13 [00:00<00:00, 42.04it/s]\n",
            "[24 / 50] Train: Loss = 0.00831, Accuracy = 99.73%: 100%|██████████| 572/572 [00:15<00:00, 37.43it/s]\n",
            "[24 / 50]   Val: Loss = 0.27059, Accuracy = 95.15%: 100%|██████████| 13/13 [00:00<00:00, 45.97it/s]\n",
            "[25 / 50] Train: Loss = 0.00774, Accuracy = 99.74%: 100%|██████████| 572/572 [00:15<00:00, 37.73it/s]\n",
            "[25 / 50]   Val: Loss = 0.27253, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 45.70it/s]\n",
            "[26 / 50] Train: Loss = 0.00732, Accuracy = 99.76%: 100%|██████████| 572/572 [00:15<00:00, 37.88it/s]\n",
            "[26 / 50]   Val: Loss = 0.28302, Accuracy = 95.03%: 100%|██████████| 13/13 [00:00<00:00, 44.74it/s]\n",
            "[27 / 50] Train: Loss = 0.00690, Accuracy = 99.77%: 100%|██████████| 572/572 [00:15<00:00, 36.84it/s] \n",
            "[27 / 50]   Val: Loss = 0.29081, Accuracy = 95.03%: 100%|██████████| 13/13 [00:00<00:00, 43.86it/s]\n",
            "[28 / 50] Train: Loss = 0.00643, Accuracy = 99.78%: 100%|██████████| 572/572 [00:15<00:00, 36.94it/s]\n",
            "[28 / 50]   Val: Loss = 0.29786, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 43.14it/s]\n",
            "[29 / 50] Train: Loss = 0.00619, Accuracy = 99.79%: 100%|██████████| 572/572 [00:15<00:00, 39.21it/s] \n",
            "[29 / 50]   Val: Loss = 0.30328, Accuracy = 95.07%: 100%|██████████| 13/13 [00:00<00:00, 44.38it/s]\n",
            "[30 / 50] Train: Loss = 0.00596, Accuracy = 99.80%: 100%|██████████| 572/572 [00:15<00:00, 37.21it/s]\n",
            "[30 / 50]   Val: Loss = 0.30575, Accuracy = 95.06%: 100%|██████████| 13/13 [00:00<00:00, 42.29it/s]\n",
            "[31 / 50] Train: Loss = 0.00564, Accuracy = 99.80%: 100%|██████████| 572/572 [00:15<00:00, 37.49it/s]\n",
            "[31 / 50]   Val: Loss = 0.31535, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 44.89it/s]\n",
            "[32 / 50] Train: Loss = 0.00552, Accuracy = 99.80%: 100%|██████████| 572/572 [00:15<00:00, 37.16it/s]\n",
            "[32 / 50]   Val: Loss = 0.31553, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 42.22it/s]\n",
            "[33 / 50] Train: Loss = 0.00538, Accuracy = 99.81%: 100%|██████████| 572/572 [00:15<00:00, 36.93it/s]\n",
            "[33 / 50]   Val: Loss = 0.31805, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 45.34it/s]\n",
            "[34 / 50] Train: Loss = 0.00507, Accuracy = 99.82%: 100%|██████████| 572/572 [00:15<00:00, 36.41it/s]\n",
            "[34 / 50]   Val: Loss = 0.32122, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 46.04it/s]\n",
            "[35 / 50] Train: Loss = 0.00502, Accuracy = 99.82%: 100%|██████████| 572/572 [00:15<00:00, 37.42it/s]\n",
            "[35 / 50]   Val: Loss = 0.33570, Accuracy = 94.93%: 100%|██████████| 13/13 [00:00<00:00, 43.14it/s]\n",
            "[36 / 50] Train: Loss = 0.00506, Accuracy = 99.82%: 100%|██████████| 572/572 [00:15<00:00, 37.46it/s] \n",
            "[36 / 50]   Val: Loss = 0.32879, Accuracy = 94.97%: 100%|██████████| 13/13 [00:00<00:00, 44.34it/s]\n",
            "[37 / 50] Train: Loss = 0.00467, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 36.85it/s]\n",
            "[37 / 50]   Val: Loss = 0.33904, Accuracy = 94.90%: 100%|██████████| 13/13 [00:00<00:00, 46.31it/s]\n",
            "[38 / 50] Train: Loss = 0.00460, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 37.14it/s]\n",
            "[38 / 50]   Val: Loss = 0.33390, Accuracy = 94.92%: 100%|██████████| 13/13 [00:00<00:00, 46.32it/s]\n",
            "[39 / 50] Train: Loss = 0.00491, Accuracy = 99.82%: 100%|██████████| 572/572 [00:15<00:00, 36.85it/s]\n",
            "[39 / 50]   Val: Loss = 0.33534, Accuracy = 94.95%: 100%|██████████| 13/13 [00:00<00:00, 41.85it/s]\n",
            "[40 / 50] Train: Loss = 0.00446, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 36.90it/s]\n",
            "[40 / 50]   Val: Loss = 0.34753, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 42.61it/s]\n",
            "[41 / 50] Train: Loss = 0.00434, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 36.81it/s]\n",
            "[41 / 50]   Val: Loss = 0.34753, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 44.17it/s]\n",
            "[42 / 50] Train: Loss = 0.00420, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 36.80it/s]\n",
            "[42 / 50]   Val: Loss = 0.35175, Accuracy = 94.87%: 100%|██████████| 13/13 [00:00<00:00, 43.14it/s]\n",
            "[43 / 50] Train: Loss = 0.00441, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 36.91it/s]\n",
            "[43 / 50]   Val: Loss = 0.35910, Accuracy = 94.85%: 100%|██████████| 13/13 [00:00<00:00, 45.27it/s]\n",
            "[44 / 50] Train: Loss = 0.00444, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 37.07it/s]\n",
            "[44 / 50]   Val: Loss = 0.35751, Accuracy = 94.87%: 100%|██████████| 13/13 [00:00<00:00, 43.03it/s]\n",
            "[45 / 50] Train: Loss = 0.00426, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 37.48it/s]\n",
            "[45 / 50]   Val: Loss = 0.35665, Accuracy = 94.94%: 100%|██████████| 13/13 [00:00<00:00, 43.11it/s]\n",
            "[46 / 50] Train: Loss = 0.00423, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 36.89it/s]\n",
            "[46 / 50]   Val: Loss = 0.36268, Accuracy = 94.83%: 100%|██████████| 13/13 [00:00<00:00, 43.15it/s]\n",
            "[47 / 50] Train: Loss = 0.00452, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 36.32it/s]\n",
            "[47 / 50]   Val: Loss = 0.36059, Accuracy = 94.85%: 100%|██████████| 13/13 [00:00<00:00, 42.26it/s]\n",
            "[48 / 50] Train: Loss = 0.00402, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 36.83it/s]\n",
            "[48 / 50]   Val: Loss = 0.36206, Accuracy = 94.83%: 100%|██████████| 13/13 [00:00<00:00, 43.13it/s]\n",
            "[49 / 50] Train: Loss = 0.00389, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 37.25it/s]\n",
            "[49 / 50]   Val: Loss = 0.37034, Accuracy = 94.92%: 100%|██████████| 13/13 [00:00<00:00, 44.17it/s]\n",
            "[50 / 50] Train: Loss = 0.00384, Accuracy = 99.85%: 100%|██████████| 572/572 [00:15<00:00, 37.51it/s]\n",
            "[50 / 50]   Val: Loss = 0.36478, Accuracy = 94.95%: 100%|██████████| 13/13 [00:00<00:00, 43.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "V-Mgznlju4-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** На самом деле, понятно, почему это плохо - после этого нельзя использовать старые предобученные вектора (которые не попали в трейн). Проверьте, какое качество получается на тесте со старыми векторами."
      ]
    },
    {
      "metadata": {
        "id": "-2of5u-4u68v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d477682d-0377-4e43-ffb7-60711ec6d230"
      },
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "for sentence in test_data:\n",
        "    embs = []\n",
        "    tags = []\n",
        "    for word, tag in sentence:\n",
        "        try:\n",
        "            word_emb = FloatTensor(w2v_model.get_vector(word.lower())).unsqueeze(0)\n",
        "        except KeyError:\n",
        "            word_emb = FloatTensor(np.zeros(w2v_model.vectors.shape[1])).unsqueeze(0)\n",
        "        embs.append(word_emb)\n",
        "        tags.append(tag2ind[tag])\n",
        "    embs = torch.cat(embs, 0).unsqueeze(1)\n",
        "\n",
        "    logits = model(embs).argmax(-1)\n",
        "\n",
        "    correct += (logits.squeeze() == LongTensor(tags)).sum().item()\n",
        "    total += len(tags)\n",
        "\n",
        "print('Test accuracy: {:.4f}'.format(correct / total))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.6881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZVJet3RQix98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Чтобы бороться с этим, можно использовать такой прием: на предобученные вектора накладывать $l_2$-регуляризацию, чтобы они не удалялись от исходных векторов, а для слов, эмбеддинги которых мы не знаем, строить случайные вектора и учить их как обычно.\n",
        "\n",
        "**Задание** Попробуйте реализовать это."
      ]
    },
    {
      "metadata": {
        "id": "-MpbK7_yCKMI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, indicies, vocab_size, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.indicies = indicies\n",
        "        self.embs = nn.Embedding.from_pretrained(embeddings)\n",
        "        \n",
        "        self._embs = nn.Embedding(vocab_size, embeddings.shape[1])\n",
        "        self._lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        if isinstance(inputs, LongTensor):\n",
        "            embs = self._embs(inputs)\n",
        "        else:\n",
        "            embs = inputs\n",
        "        return self._out_layer(self._lstm(embs)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-O7L1UOE06X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                extra = torch.dist(model._embs.weight[indicies], model.embs.weight[indicies])\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "                loss = extra + loss\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                mask = (y_batch != 0).float()\n",
        "                cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "                cur_sum_count = mask.sum().item()\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def _fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = _do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = _do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IobkgAq8Kw3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "indicies = []\n",
        "for idx, emb in enumerate(embeddings):\n",
        "    if len(np.where(emb != 0)) > 0:\n",
        "        indicies.append(idx)\n",
        "indicies = np.array(indicies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "23abeGwPp163",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "3bcddad8-afe6-4bfa-d33f-424ff7e9dca3"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=FloatTensor(embeddings),\n",
        "    indicies=indicies,\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "_fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 1904.09551, Accuracy = 72.31%: 100%|██████████| 572/572 [00:24<00:00, 23.46it/s]\n",
            "[1 / 50]   Val: Loss = 1499.44120, Accuracy = 82.20%: 100%|██████████| 13/13 [00:00<00:00, 41.23it/s]\n",
            "[2 / 50] Train: Loss = 1163.24071, Accuracy = 85.59%: 100%|██████████| 572/572 [00:24<00:00, 23.24it/s]\n",
            "[2 / 50]   Val: Loss = 857.21266, Accuracy = 88.12%: 100%|██████████| 13/13 [00:00<00:00, 31.33it/s]\n",
            "[3 / 50] Train: Loss = 613.67737, Accuracy = 90.71%: 100%|██████████| 572/572 [00:24<00:00, 20.72it/s]\n",
            "[3 / 50]   Val: Loss = 400.08722, Accuracy = 91.74%: 100%|██████████| 13/13 [00:00<00:00, 38.41it/s]\n",
            "[4 / 50] Train: Loss = 248.33801, Accuracy = 93.25%: 100%|██████████| 572/572 [00:24<00:00, 22.89it/s]\n",
            "[4 / 50]   Val: Loss = 126.19654, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 34.77it/s]\n",
            "[5 / 50] Train: Loss = 59.86656, Accuracy = 94.18%: 100%|██████████| 572/572 [00:24<00:00, 23.15it/s]\n",
            "[5 / 50]   Val: Loss = 16.89773, Accuracy = 93.81%: 100%|██████████| 13/13 [00:00<00:00, 39.30it/s]\n",
            "[6 / 50] Train: Loss = 4.98279, Accuracy = 94.84%: 100%|██████████| 572/572 [00:24<00:00, 23.36it/s]\n",
            "[6 / 50]   Val: Loss = 0.40666, Accuracy = 94.42%: 100%|██████████| 13/13 [00:00<00:00, 39.61it/s]\n",
            "[7 / 50] Train: Loss = 0.27791, Accuracy = 95.34%: 100%|██████████| 572/572 [00:24<00:00, 23.05it/s]\n",
            "[7 / 50]   Val: Loss = 0.28349, Accuracy = 94.80%: 100%|██████████| 13/13 [00:00<00:00, 38.00it/s]\n",
            "[8 / 50] Train: Loss = 0.22645, Accuracy = 95.66%: 100%|██████████| 572/572 [00:25<00:00, 22.36it/s]\n",
            "[8 / 50]   Val: Loss = 0.23689, Accuracy = 94.92%: 100%|██████████| 13/13 [00:00<00:00, 39.53it/s]\n",
            "[9 / 50] Train: Loss = 0.19839, Accuracy = 95.88%: 100%|██████████| 572/572 [00:24<00:00, 23.18it/s]\n",
            "[9 / 50]   Val: Loss = 0.22108, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 39.56it/s]\n",
            "[10 / 50] Train: Loss = 0.18570, Accuracy = 96.07%: 100%|██████████| 572/572 [00:24<00:00, 23.21it/s]\n",
            "[10 / 50]   Val: Loss = 0.21495, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 39.83it/s]\n",
            "[11 / 50] Train: Loss = 0.17817, Accuracy = 96.21%: 100%|██████████| 572/572 [00:24<00:00, 23.11it/s]\n",
            "[11 / 50]   Val: Loss = 0.20928, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 39.44it/s]\n",
            "[12 / 50] Train: Loss = 0.17292, Accuracy = 96.32%: 100%|██████████| 572/572 [00:24<00:00, 23.11it/s]\n",
            "[12 / 50]   Val: Loss = 0.20885, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 39.25it/s]\n",
            "[13 / 50] Train: Loss = 0.16882, Accuracy = 96.40%: 100%|██████████| 572/572 [00:24<00:00, 23.23it/s]\n",
            "[13 / 50]   Val: Loss = 0.20526, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 38.41it/s]\n",
            "[14 / 50] Train: Loss = 0.16559, Accuracy = 96.51%: 100%|██████████| 572/572 [00:24<00:00, 23.21it/s]\n",
            "[14 / 50]   Val: Loss = 0.20582, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 39.45it/s]\n",
            "[15 / 50] Train: Loss = 0.16273, Accuracy = 96.57%: 100%|██████████| 572/572 [00:24<00:00, 23.67it/s]\n",
            "[15 / 50]   Val: Loss = 0.20417, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 40.84it/s]\n",
            "[16 / 50] Train: Loss = 0.16008, Accuracy = 96.65%: 100%|██████████| 572/572 [00:24<00:00, 23.06it/s]\n",
            "[16 / 50]   Val: Loss = 0.19814, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 39.78it/s]\n",
            "[17 / 50] Train: Loss = 0.15807, Accuracy = 96.72%: 100%|██████████| 572/572 [00:24<00:00, 23.11it/s]\n",
            "[17 / 50]   Val: Loss = 0.20397, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 40.51it/s]\n",
            "[18 / 50] Train: Loss = 0.15625, Accuracy = 96.77%: 100%|██████████| 572/572 [00:24<00:00, 23.04it/s]\n",
            "[18 / 50]   Val: Loss = 0.20140, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 41.31it/s]\n",
            "[19 / 50] Train: Loss = 0.15432, Accuracy = 96.83%: 100%|██████████| 572/572 [00:24<00:00, 23.18it/s]\n",
            "[19 / 50]   Val: Loss = 0.19790, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 37.82it/s]\n",
            "[20 / 50] Train: Loss = 0.15271, Accuracy = 96.88%: 100%|██████████| 572/572 [00:24<00:00, 23.84it/s]\n",
            "[20 / 50]   Val: Loss = 0.19934, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 39.95it/s]\n",
            "[21 / 50] Train: Loss = 0.15124, Accuracy = 96.94%: 100%|██████████| 572/572 [00:24<00:00, 23.24it/s]\n",
            "[21 / 50]   Val: Loss = 0.20038, Accuracy = 95.55%: 100%|██████████| 13/13 [00:00<00:00, 39.42it/s]\n",
            "[22 / 50] Train: Loss = 0.14981, Accuracy = 96.98%: 100%|██████████| 572/572 [00:24<00:00, 23.17it/s]\n",
            "[22 / 50]   Val: Loss = 0.19684, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 38.75it/s]\n",
            "[23 / 50] Train: Loss = 0.14864, Accuracy = 97.02%: 100%|██████████| 572/572 [00:24<00:00, 23.17it/s]\n",
            "[23 / 50]   Val: Loss = 0.19942, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 38.21it/s]\n",
            "[24 / 50] Train: Loss = 0.14745, Accuracy = 97.05%: 100%|██████████| 572/572 [00:24<00:00, 22.93it/s]\n",
            "[24 / 50]   Val: Loss = 0.19811, Accuracy = 95.47%: 100%|██████████| 13/13 [00:00<00:00, 40.19it/s]\n",
            "[25 / 50] Train: Loss = 0.14622, Accuracy = 97.12%: 100%|██████████| 572/572 [00:24<00:00, 23.13it/s]\n",
            "[25 / 50]   Val: Loss = 0.20035, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 38.19it/s]\n",
            "[26 / 50] Train: Loss = 0.14525, Accuracy = 97.14%: 100%|██████████| 572/572 [00:24<00:00, 23.03it/s]\n",
            "[26 / 50]   Val: Loss = 0.19857, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 40.32it/s]\n",
            "[27 / 50] Train: Loss = 0.14414, Accuracy = 97.19%: 100%|██████████| 572/572 [00:24<00:00, 23.13it/s]\n",
            "[27 / 50]   Val: Loss = 0.19967, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 39.15it/s]\n",
            "[28 / 50] Train: Loss = 0.14328, Accuracy = 97.22%: 100%|██████████| 572/572 [00:24<00:00, 23.10it/s]\n",
            "[28 / 50]   Val: Loss = 0.19962, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 36.71it/s]\n",
            "[29 / 50] Train: Loss = 0.14230, Accuracy = 97.23%: 100%|██████████| 572/572 [00:24<00:00, 23.18it/s]\n",
            "[29 / 50]   Val: Loss = 0.19767, Accuracy = 95.51%: 100%|██████████| 13/13 [00:00<00:00, 40.45it/s]\n",
            "[30 / 50] Train: Loss = 0.14139, Accuracy = 97.28%: 100%|██████████| 572/572 [00:24<00:00, 23.20it/s]\n",
            "[30 / 50]   Val: Loss = 0.20290, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 40.00it/s]\n",
            "[31 / 50] Train: Loss = 0.14046, Accuracy = 97.30%: 100%|██████████| 572/572 [00:24<00:00, 23.11it/s]\n",
            "[31 / 50]   Val: Loss = 0.20175, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 38.21it/s]\n",
            "[32 / 50] Train: Loss = 0.13955, Accuracy = 97.35%: 100%|██████████| 572/572 [00:24<00:00, 23.19it/s]\n",
            "[32 / 50]   Val: Loss = 0.20200, Accuracy = 95.60%: 100%|██████████| 13/13 [00:00<00:00, 40.04it/s]\n",
            "[33 / 50] Train: Loss = 0.13875, Accuracy = 97.36%: 100%|██████████| 572/572 [00:24<00:00, 23.18it/s]\n",
            "[33 / 50]   Val: Loss = 0.20364, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 41.24it/s]\n",
            "[34 / 50] Train: Loss = 0.13808, Accuracy = 97.40%: 100%|██████████| 572/572 [00:24<00:00, 23.16it/s]\n",
            "[34 / 50]   Val: Loss = 0.20248, Accuracy = 95.57%: 100%|██████████| 13/13 [00:00<00:00, 39.01it/s]\n",
            "[35 / 50] Train: Loss = 0.13741, Accuracy = 97.43%: 100%|██████████| 572/572 [00:24<00:00, 23.28it/s]\n",
            "[35 / 50]   Val: Loss = 0.20269, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 39.39it/s]\n",
            "[36 / 50] Train: Loss = 0.13680, Accuracy = 97.44%: 100%|██████████| 572/572 [00:24<00:00, 23.13it/s]\n",
            "[36 / 50]   Val: Loss = 0.20539, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 39.74it/s]\n",
            "[37 / 50] Train: Loss = 0.13598, Accuracy = 97.48%: 100%|██████████| 572/572 [00:24<00:00, 23.08it/s]\n",
            "[37 / 50]   Val: Loss = 0.20506, Accuracy = 95.51%: 100%|██████████| 13/13 [00:00<00:00, 41.11it/s]\n",
            "[38 / 50] Train: Loss = 0.13523, Accuracy = 97.49%: 100%|██████████| 572/572 [00:24<00:00, 23.18it/s]\n",
            "[38 / 50]   Val: Loss = 0.20493, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 40.80it/s]\n",
            "[39 / 50] Train: Loss = 0.13471, Accuracy = 97.52%: 100%|██████████| 572/572 [00:24<00:00, 23.14it/s]\n",
            "[39 / 50]   Val: Loss = 0.20628, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 40.57it/s]\n",
            "[40 / 50] Train: Loss = 0.13392, Accuracy = 97.55%: 100%|██████████| 572/572 [00:24<00:00, 23.32it/s]\n",
            "[40 / 50]   Val: Loss = 0.20485, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 40.81it/s]\n",
            "[41 / 50] Train: Loss = 0.13348, Accuracy = 97.57%: 100%|██████████| 572/572 [00:24<00:00, 23.29it/s]\n",
            "[41 / 50]   Val: Loss = 0.20574, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 39.66it/s]\n",
            "[42 / 50] Train: Loss = 0.13256, Accuracy = 97.59%: 100%|██████████| 572/572 [00:24<00:00, 22.96it/s]\n",
            "[42 / 50]   Val: Loss = 0.20645, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 38.26it/s]\n",
            "[43 / 50] Train: Loss = 0.13201, Accuracy = 97.63%: 100%|██████████| 572/572 [00:24<00:00, 23.10it/s]\n",
            "[43 / 50]   Val: Loss = 0.20935, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 38.87it/s]\n",
            "[44 / 50] Train: Loss = 0.13168, Accuracy = 97.63%: 100%|██████████| 572/572 [00:24<00:00, 23.04it/s]\n",
            "[44 / 50]   Val: Loss = 0.20802, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 37.68it/s]\n",
            "[45 / 50] Train: Loss = 0.13119, Accuracy = 97.65%: 100%|██████████| 572/572 [00:24<00:00, 23.16it/s]\n",
            "[45 / 50]   Val: Loss = 0.21087, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 38.28it/s]\n",
            "[46 / 50] Train: Loss = 0.13049, Accuracy = 97.68%: 100%|██████████| 572/572 [00:24<00:00, 23.36it/s]\n",
            "[46 / 50]   Val: Loss = 0.21338, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 40.90it/s]\n",
            "[47 / 50] Train: Loss = 0.13053, Accuracy = 97.69%: 100%|██████████| 572/572 [00:24<00:00, 23.12it/s]\n",
            "[47 / 50]   Val: Loss = 0.23194, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 40.68it/s]\n",
            "[48 / 50] Train: Loss = 0.12988, Accuracy = 97.72%: 100%|██████████| 572/572 [00:24<00:00, 23.12it/s]\n",
            "[48 / 50]   Val: Loss = 0.21342, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 36.72it/s]\n",
            "[49 / 50] Train: Loss = 0.12914, Accuracy = 97.74%: 100%|██████████| 572/572 [00:24<00:00, 23.01it/s]\n",
            "[49 / 50]   Val: Loss = 0.21468, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 38.73it/s]\n",
            "[50 / 50] Train: Loss = 0.12866, Accuracy = 97.75%: 100%|██████████| 572/572 [00:24<00:00, 23.29it/s]\n",
            "[50 / 50]   Val: Loss = 0.21795, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 38.77it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "W5lPizVZCHjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7b0120a-656f-450f-81fb-2a31ad1ea149"
      },
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "for sentence in test_data:\n",
        "    embs = []\n",
        "    tags = []\n",
        "    for word, tag in sentence:\n",
        "        try:\n",
        "            word_emb = FloatTensor(w2v_model.get_vector(word.lower())).unsqueeze(0)\n",
        "        except KeyError:\n",
        "            word_emb = FloatTensor(np.zeros(w2v_model.vectors.shape[1])).unsqueeze(0)\n",
        "        embs.append(word_emb)\n",
        "        tags.append(tag2ind[tag])\n",
        "    embs = torch.cat(embs, 0).unsqueeze(1)\n",
        "\n",
        "    logits = model(embs).argmax(-1)\n",
        "\n",
        "    correct += (logits.squeeze() == LongTensor(tags)).sum().item()\n",
        "    total += len(tags)\n",
        "\n",
        "print('Test accuracy: {:.4f}'.format(correct / total))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EfN1olf6RZne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## We need to go deeper\n",
        "\n",
        "Напомню, на прошлом занятии мы строили LSTM сеть, которая обрабатывала последовательности символов, и предсказывала, к какому языку относится слово. \n",
        "\n",
        "LSTM выступал в роли feature extractor'а, работающего с произвольного размера последовательностью символов (ну, почти произвольного - мы ограничивались максимальной длиной слова). Батч для сети имел размерность `(max_word_len, batch_size)`.\n",
        "\n",
        "Теперь мы опять хотим использовать такую же идею для извлечения признаков из последовательности символов - потому что последовательность символов же должна быть полезной для предсказания части речи, правда?\n",
        "\n",
        "Сеть должна будет запомнить, например, что `-ly` - это часто про наречие, а `-tion` - про существительное.\n",
        "\n",
        "![](https://image.ibb.co/kzbh6L/Char-Bi-LSTM.png =x400)\n",
        "\n",
        "Остальная часть сети при этом будет такой же.\n",
        "\n",
        "Найдем границу для длины слов:"
      ]
    },
    {
      "metadata": {
        "id": "SczGwL8Cy0Ws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbf06119-9dbb-4c1f-a7d4-407bb6faa795"
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter \n",
        "    \n",
        "def find_max_len(counter, threshold):\n",
        "    sum_count = sum(counter.values())\n",
        "    cum_count = 0\n",
        "    for i in range(max(counter)):\n",
        "        cum_count += counter[i]\n",
        "        if cum_count > sum_count * threshold:\n",
        "            return i\n",
        "    return max(counter)\n",
        "\n",
        "word_len_counter = Counter()\n",
        "for sent in data:\n",
        "    for word, _ in sent:\n",
        "        word_len_counter[len(word)] += 1\n",
        "    \n",
        "threshold = 0.99\n",
        "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
        "\n",
        "print('Max word len for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max word len for 99% of words is 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YlArjEvqkMGk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим алфавит:"
      ]
    },
    {
      "metadata": {
        "id": "-LWXHmXGcotd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "def get_range(first_symb, last_symb):\n",
        "    return set(chr(c) for c in range(ord(first_symb), ord(last_symb) + 1))\n",
        "\n",
        "chars = get_range('a', 'z') | get_range('A', 'Z') | get_range('0', '9') | set(punctuation)\n",
        "char2ind = {c : i + 1 for i, c in enumerate(chars)}\n",
        "char2ind['<pad>'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v0OS9WQjkO9b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Сконвертируйте данные, как в функции выше - только теперь слова должны отобразиться не в один индекс, а в последовательность.\n",
        "\n",
        "Обрезайте слова по `MAX_WORD_LEN`."
      ]
    },
    {
      "metadata": {
        "id": "k3Q3arGCmgi-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_data(data, char2ind, tag2ind): \n",
        "    X = [[[char2ind.get(symb, 0) for symb in word[:MAX_WORD_LEN]] for word, _ in sample] for sample in data] \n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data] \n",
        "    return X, y \n",
        "\n",
        "X_train, y_train = convert_data(train_data, char2ind, tag2ind) \n",
        "X_val, y_val = convert_data(val_data, char2ind, tag2ind) \n",
        "X_test, y_test = convert_data(test_data, char2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1SMmXMx5Rr5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напишем генератор батчей:"
      ]
    },
    {
      "metadata": {
        "id": "c835LEVERXzl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start: end]\n",
        "        \n",
        "        sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        word_len = max(len(word) for ind in batch_indices for word in X[ind])\n",
        "            \n",
        "        X_batch = np.zeros((sent_len, len(batch_indices), word_len))\n",
        "        y_batch = np.zeros((sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            for word_ind, word in enumerate(X[sample_ind]):\n",
        "                X_batch[word_ind, batch_ind, :len(word)] = word\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWcRRe11jFI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fc3e2a6-fb45-4af0-fd1c-50ffdb751140"
      },
      "cell_type": "code",
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((51, 4, 12), (51, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "yfY7FcXCknzX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте сеть, которая принимает батч размера `(seq_len, batch_size, word_len)` и возвращает `(seq_len, batch_size, word_emb_dim)`. Это может быть любая функция, которая умеет в последовательности произвольной длины. Мы уже смотрели на сверточные и рекуррентные сети для такой задачи - попробуйте обе."
      ]
    },
    {
      "metadata": {
        "id": "f1qs96uAY3Wd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharsEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, char_emb_dim=24, word_emb_dim=100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._embs = nn.Embedding(vocab_size, char_emb_dim)\n",
        "        self._conv = nn.Conv2d(in_channels=1, out_channels=word_emb_dim, kernel_size=(3, char_emb_dim))\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        embs = self._embs(inputs)\n",
        "        \n",
        "        seq_len, batch_size = embs.shape[:2]\n",
        "        \n",
        "        embs = embs.view(seq_len * batch_size, 1, embs.shape[2], embs.shape[3])\n",
        "        embs = self._conv(embs).squeeze(-1)\n",
        "        \n",
        "        embs = F.relu(embs)\n",
        "        \n",
        "        embs, _ = torch.max(embs, -1)\n",
        "        \n",
        "        return embs.view(seq_len, batch_size, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ag2R5sIglLhh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте теггер с эмбеддингами символьного уровня."
      ]
    },
    {
      "metadata": {
        "id": "TRB8tAOAa_YW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, char_vocab_size, tagset_size, char_emb_dim=24, \n",
        "                 word_emb_dim=128, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._embs = CharsEmbedding(char_vocab_size, char_emb_dim, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self._out_layer(self._lstm(self._embs(inputs))[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEaWjN0qjFfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "a3303ce5-8967-4985-ef54-b9ffb8161007"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(char_vocab_size=len(char2ind), tagset_size=len(tag2ind)).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20, \n",
        "    batch_size=24, val_data=(X_val, y_val), val_batch_size=32)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 20] Train: Loss = 0.42174, Accuracy = 85.78%: 100%|██████████| 1524/1524 [01:15<00:00, 21.21it/s]\n",
            "[1 / 20]   Val: Loss = 0.24902, Accuracy = 91.16%: 100%|██████████| 202/202 [00:05<00:00, 34.99it/s]\n",
            "[2 / 20] Train: Loss = 0.20631, Accuracy = 92.66%: 100%|██████████| 1524/1524 [01:14<00:00, 20.33it/s]\n",
            "[2 / 20]   Val: Loss = 0.19480, Accuracy = 93.05%: 100%|██████████| 202/202 [00:05<00:00, 34.29it/s]\n",
            "[3 / 20] Train: Loss = 0.17345, Accuracy = 93.76%: 100%|██████████| 1524/1524 [01:15<00:00, 20.24it/s]\n",
            "[3 / 20]   Val: Loss = 0.16782, Accuracy = 93.94%: 100%|██████████| 202/202 [00:05<00:00, 34.76it/s]\n",
            "[4 / 20] Train: Loss = 0.15524, Accuracy = 94.41%: 100%|██████████| 1524/1524 [01:15<00:00, 20.26it/s]\n",
            "[4 / 20]   Val: Loss = 0.15646, Accuracy = 94.39%: 100%|██████████| 202/202 [00:05<00:00, 35.67it/s]\n",
            "[5 / 20] Train: Loss = 0.14364, Accuracy = 94.80%: 100%|██████████| 1524/1524 [01:15<00:00, 22.67it/s]\n",
            "[5 / 20]   Val: Loss = 0.16606, Accuracy = 93.91%: 100%|██████████| 202/202 [00:05<00:00, 34.50it/s]\n",
            "[6 / 20] Train: Loss = 0.13414, Accuracy = 95.13%: 100%|██████████| 1524/1524 [01:15<00:00, 20.19it/s]\n",
            "[6 / 20]   Val: Loss = 0.14656, Accuracy = 94.79%: 100%|██████████| 202/202 [00:05<00:00, 33.89it/s]\n",
            "[7 / 20] Train: Loss = 0.12701, Accuracy = 95.39%: 100%|██████████| 1524/1524 [01:15<00:00, 20.19it/s]\n",
            "[7 / 20]   Val: Loss = 0.13908, Accuracy = 95.04%: 100%|██████████| 202/202 [00:05<00:00, 30.99it/s]\n",
            "[8 / 20] Train: Loss = 0.12126, Accuracy = 95.58%: 100%|██████████| 1524/1524 [01:15<00:00, 20.96it/s]\n",
            "[8 / 20]   Val: Loss = 0.13461, Accuracy = 95.23%: 100%|██████████| 202/202 [00:05<00:00, 36.83it/s]\n",
            "[9 / 20] Train: Loss = 0.11570, Accuracy = 95.78%: 100%|██████████| 1524/1524 [01:15<00:00, 20.18it/s]\n",
            "[9 / 20]   Val: Loss = 0.13425, Accuracy = 95.21%: 100%|██████████| 202/202 [00:05<00:00, 34.91it/s]\n",
            "[10 / 20] Train: Loss = 0.11196, Accuracy = 95.93%: 100%|██████████| 1524/1524 [01:15<00:00, 20.18it/s]\n",
            "[10 / 20]   Val: Loss = 0.13542, Accuracy = 95.15%: 100%|██████████| 202/202 [00:05<00:00, 34.72it/s]\n",
            "[11 / 20] Train: Loss = 0.10792, Accuracy = 96.06%: 100%|██████████| 1524/1524 [01:15<00:00, 22.10it/s] \n",
            "[11 / 20]   Val: Loss = 0.12649, Accuracy = 95.54%: 100%|██████████| 202/202 [00:05<00:00, 35.56it/s]\n",
            "[12 / 20] Train: Loss = 0.10398, Accuracy = 96.21%: 100%|██████████| 1524/1524 [01:15<00:00, 20.26it/s]\n",
            "[12 / 20]   Val: Loss = 0.12930, Accuracy = 95.46%: 100%|██████████| 202/202 [00:05<00:00, 32.57it/s]\n",
            "[13 / 20] Train: Loss = 0.10107, Accuracy = 96.29%: 100%|██████████| 1524/1524 [01:15<00:00, 20.29it/s]\n",
            "[13 / 20]   Val: Loss = 0.13317, Accuracy = 95.25%: 100%|██████████| 202/202 [00:05<00:00, 35.42it/s]\n",
            "[14 / 20] Train: Loss = 0.09767, Accuracy = 96.44%: 100%|██████████| 1524/1524 [01:15<00:00, 20.22it/s]\n",
            "[14 / 20]   Val: Loss = 0.12334, Accuracy = 95.62%: 100%|██████████| 202/202 [00:05<00:00, 33.14it/s]\n",
            "[15 / 20] Train: Loss = 0.09503, Accuracy = 96.51%: 100%|██████████| 1524/1524 [01:15<00:00, 20.32it/s]\n",
            "[15 / 20]   Val: Loss = 0.12635, Accuracy = 95.51%: 100%|██████████| 202/202 [00:05<00:00, 35.93it/s]\n",
            "[16 / 20] Train: Loss = 0.09231, Accuracy = 96.61%: 100%|██████████| 1524/1524 [01:15<00:00, 20.28it/s]\n",
            "[16 / 20]   Val: Loss = 0.12611, Accuracy = 95.60%: 100%|██████████| 202/202 [00:05<00:00, 34.85it/s]\n",
            "[17 / 20] Train: Loss = 0.08979, Accuracy = 96.70%: 100%|██████████| 1524/1524 [01:15<00:00, 20.26it/s]\n",
            "[17 / 20]   Val: Loss = 0.12176, Accuracy = 95.80%: 100%|██████████| 202/202 [00:05<00:00, 34.15it/s]\n",
            "[18 / 20] Train: Loss = 0.08756, Accuracy = 96.80%: 100%|██████████| 1524/1524 [01:15<00:00, 22.79it/s]\n",
            "[18 / 20]   Val: Loss = 0.12435, Accuracy = 95.65%: 100%|██████████| 202/202 [00:05<00:00, 34.68it/s]\n",
            "[19 / 20] Train: Loss = 0.08535, Accuracy = 96.85%: 100%|██████████| 1524/1524 [01:15<00:00, 20.23it/s]\n",
            "[19 / 20]   Val: Loss = 0.12385, Accuracy = 95.73%: 100%|██████████| 202/202 [00:05<00:00, 34.61it/s]\n",
            "[20 / 20] Train: Loss = 0.08329, Accuracy = 96.94%: 100%|██████████| 1524/1524 [01:14<00:00, 21.71it/s] \n",
            "[20 / 20]   Val: Loss = 0.12707, Accuracy = 95.52%: 100%|██████████| 202/202 [00:05<00:00, 37.25it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "OGDJqyG9lTxV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Оцените его качество."
      ]
    },
    {
      "metadata": {
        "id": "OCUj9_nqjgrL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d631383d-fa2d-41a1-a3ba-b9247af53e4c"
      },
      "cell_type": "code",
      "source": [
        "_, test_accuracy = do_epoch(model, criterion, (X_test, y_test), batch_size=32)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Loss = 0.13025, Accuracy = 95.56%: 100%|██████████| 448/448 [00:12<00:00, 35.28it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "12HYYmSzlZtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Визуализации\n",
        "\n",
        "**Задание** Посчитайте эмбеддинги символьного уровня (обученные внутри модели перед этим) для 1000 случайных слов из `word2ind`."
      ]
    },
    {
      "metadata": {
        "id": "meUYw3ikHw4f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples = np.random.choice(list(word2ind.keys()), 1000, replace=False)\n",
        "data = [[[char2ind.get(symb, 0) for symb in word[:MAX_WORD_LEN]] for word in samples]]\n",
        "dummy = [list(range(1000))]\n",
        "index2word = samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDtuTigpPII0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_batch, y_batch = next(iterate_batches((data, dummy), 1000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JcstIY8l6-yJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = model(LongTensor(X_batch))\n",
        "embeddings = embeddings.squeeze(1).detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2klT31GSWlR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        },
        "outputId": "ce45fe66-2405-4c1f-bd61-baded5b804ce"
      },
      "cell_type": "code",
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "    \n",
        "    if isinstance(color, str): \n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: \n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=100)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "    \n",
        "    \n",
        "def visualize_embeddings(embeddings, token):\n",
        "    tsne = get_tsne_projection(embeddings)\n",
        "    draw_vectors(tsne[:, 0], tsne[:, 1], token=token)\n",
        "    \n",
        "\n",
        "visualize_embeddings(embeddings, index2word)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 1000 samples in 0.001s...\n",
            "[t-SNE] Computed neighbors for 1000 samples in 0.049s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 1000\n",
            "[t-SNE] Mean sigma: 3.490536\n",
            "[t-SNE] Computed conditional probabilities in 0.064s\n",
            "[t-SNE] Iteration 50: error = 70.1427307, gradient norm = 0.2924174 (50 iterations in 2.553s)\n",
            "[t-SNE] Iteration 100: error = 69.6234131, gradient norm = 0.2761573 (50 iterations in 2.448s)\n",
            "[t-SNE] Iteration 150: error = 70.0331116, gradient norm = 0.2747684 (50 iterations in 2.331s)\n",
            "[t-SNE] Iteration 200: error = 70.4327087, gradient norm = 0.2706316 (50 iterations in 2.489s)\n",
            "[t-SNE] Iteration 250: error = 69.9566040, gradient norm = 0.2687684 (50 iterations in 2.989s)\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 69.956604\n",
            "[t-SNE] Iteration 300: error = 1.6055086, gradient norm = 0.0012919 (50 iterations in 1.890s)\n",
            "[t-SNE] Iteration 350: error = 1.5208647, gradient norm = 0.0003817 (50 iterations in 1.820s)\n",
            "[t-SNE] Iteration 400: error = 1.4922092, gradient norm = 0.0002650 (50 iterations in 1.864s)\n",
            "[t-SNE] Iteration 450: error = 1.4798012, gradient norm = 0.0002020 (50 iterations in 1.823s)\n",
            "[t-SNE] Iteration 500: error = 1.4693370, gradient norm = 0.0001940 (50 iterations in 1.817s)\n",
            "[t-SNE] Iteration 550: error = 1.4637940, gradient norm = 0.0001764 (50 iterations in 1.838s)\n",
            "[t-SNE] Iteration 600: error = 1.4602251, gradient norm = 0.0001216 (50 iterations in 1.847s)\n",
            "[t-SNE] Iteration 650: error = 1.4575399, gradient norm = 0.0001153 (50 iterations in 1.825s)\n",
            "[t-SNE] Iteration 700: error = 1.4559562, gradient norm = 0.0001302 (50 iterations in 1.833s)\n",
            "[t-SNE] Iteration 750: error = 1.4509968, gradient norm = 0.0003385 (50 iterations in 1.822s)\n",
            "[t-SNE] Iteration 800: error = 1.4498590, gradient norm = 0.0000878 (50 iterations in 1.877s)\n",
            "[t-SNE] Iteration 850: error = 1.4493273, gradient norm = 0.0000557 (50 iterations in 1.856s)\n",
            "[t-SNE] Iteration 900: error = 1.4487489, gradient norm = 0.0000486 (50 iterations in 1.800s)\n",
            "[t-SNE] Iteration 950: error = 1.4482892, gradient norm = 0.0000515 (50 iterations in 1.894s)\n",
            "[t-SNE] Iteration 1000: error = 1.4481074, gradient norm = 0.0000404 (50 iterations in 1.941s)\n",
            "[t-SNE] Error after 1000 iterations: 1.448107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
            "  warnings.warn(\"Numerical issues were encountered \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div class=\"bk-root\">\n",
              "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
              "        <span id=\"e39b320f-c78f-4024-8ab5-ca71e707c4a7\">Loading BokehJS ...</span>\n",
              "    </div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  var JS_MIME_TYPE = 'application/javascript';\n",
              "  var HTML_MIME_TYPE = 'text/html';\n",
              "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    var script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    var cell = handle.cell;\n",
              "\n",
              "    var id = cell.output_area._bokeh_element_id;\n",
              "    var server_id = cell.output_area._bokeh_server_id;\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null && id in Bokeh.index) {\n",
              "      Bokeh.index[id].model.document.clear();\n",
              "      delete Bokeh.index[id];\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            var id = msg.content.text.trim();\n",
              "            if (id in Bokeh.index) {\n",
              "              Bokeh.index[id].model.document.clear();\n",
              "              delete Bokeh.index[id];\n",
              "            }\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    var output_area = handle.output_area;\n",
              "    var output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      var bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      var script_attrs = bk_div.children[0].attributes;\n",
              "      for (var i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      var toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  \n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    var el = document.getElementById(\"e39b320f-c78f-4024-8ab5-ca71e707c4a7\");\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
              "    }\n",
              "    finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.info(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(js_urls, callback) {\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = js_urls.length;\n",
              "    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      var s = document.createElement('script');\n",
              "      s.src = url;\n",
              "      s.async = false;\n",
              "      s.onreadystatechange = s.onload = function() {\n",
              "        root._bokeh_is_loading--;\n",
              "        if (root._bokeh_is_loading === 0) {\n",
              "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
              "          run_callbacks()\n",
              "        }\n",
              "      };\n",
              "      s.onerror = function() {\n",
              "        console.warn(\"failed to load library \" + url);\n",
              "      };\n",
              "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "    }\n",
              "  };var element = document.getElementById(\"e39b320f-c78f-4024-8ab5-ca71e707c4a7\");\n",
              "  if (element == null) {\n",
              "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'e39b320f-c78f-4024-8ab5-ca71e707c4a7' but no matching script tag was found. \")\n",
              "    return false;\n",
              "  }\n",
              "\n",
              "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n",
              "\n",
              "  var inline_js = [\n",
              "    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "    \n",
              "    function(Bokeh) {\n",
              "      \n",
              "    },\n",
              "    function(Bokeh) {\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    \n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (var i = 0; i < inline_js.length; i++) {\n",
              "        inline_js[i].call(root, root.Bokeh);\n",
              "      }if (force === true) {\n",
              "        display_loaded();\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      var cell = $(document.getElementById(\"e39b320f-c78f-4024-8ab5-ca71e707c4a7\")).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(js_urls, function() {\n",
              "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"e39b320f-c78f-4024-8ab5-ca71e707c4a7\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"e39b320f-c78f-4024-8ab5-ca71e707c4a7\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'e39b320f-c78f-4024-8ab5-ca71e707c4a7' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"e39b320f-c78f-4024-8ab5-ca71e707c4a7\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div class=\"bk-root\" id=\"19516b73-0244-4658-b769-6a32c8525302\"></div>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "    \n",
              "  var docs_json = {\"943999c3-3f5c-4698-8227-9469bc5f7c3b\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"ab6950e3-1a4a-47bf-b236-1f985c656920\",\"type\":\"Selection\"},{\"attributes\":{\"formatter\":{\"id\":\"edfcaff1-e8dd-4a0f-8421-d69ad13960a2\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"6bb5ca13-5bd9-414a-8c28-446996df65cf\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"272f2be0-0951-47aa-b562-31622ee54ce4\",\"type\":\"BasicTicker\"}},\"id\":\"502ab731-63d5-484b-b2db-a155e277be98\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"data\":{\"color\":[\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\"],\"token\":[\"larks\",\"coal-railroad\",\"neocortex\",\"Hephzibah\",\"instantaneous\",\"typify\",\"King\",\"Turnpike\",\"brownish\",\"enlarge\",\"scudding\",\"self-dramatization\",\"conspicuous\",\"wall-stabilized\",\"pterygia\",\"sterilized\",\"incantation\",\"Tonal\",\"volcanic\",\"repressed\",\"Contradictions\",\"granddaughter\",\"Heidegger's\",\"driftin'\",\"nicest\",\"Symes's\",\"N.D.\",\"populous\",\"True\",\"carpenter's\",\"civil-rights\",\"Strongheart\",\"sunning\",\"sputter\",\"repressive\",\"Sheraton-Dallas\",\"smacks\",\"twitch\",\"Spenglerian\",\"christening\",\"Callas\",\"water-soluble\",\"warily\",\"Czarina's\",\"nation-state\",\"representatives\",\"bloom\",\"Behan\",\"Jon\",\"23-30\",\"adjustments\",\"machine-masters\",\"Cap\",\"nutmeg\",\"Until\",\"gun-slinger\",\"self-respect\",\"Epstein\",\"wrestlings\",\"McRoberts\",\"puppet's\",\"decorate\",\"exclusively\",\"equipotent\",\"Oedipal\",\"marginal\",\"scheming\",\"subscribed\",\"baroness\",\"Volumes\",\"cockpit\",\"Mosk\",\"utilizes\",\"Advisory\",\"hairless\",\"lustre\",\"arrangements\",\"warrant\",\"supervise\",\"poke\",\"Somewhat\",\"16th\",\"redhead\",\"pushed\",\"sheet-metal\",\"2.58\",\"territorial\",\"lizard's\",\"microfossils\",\"Coconut\",\"England\",\"dryness\",\"twist\",\"rejection\",\"alcohol\",\"sealed\",\"antisera\",\"Knowing\",\"sculptures\",\"Bascom\",\"jerk\",\"excludes\",\"minus\",\"9:00\",\"leverage\",\"fictional\",\"appreciation\",\"Dimly\",\"badly-needed\",\"Congress'\",\"logging\",\"coves\",\"gospel\",\"shavings\",\"brok\",\"meanest\",\"fames\",\"elaborate\",\"Putt\",\"standard-weight\",\"Rudkoebing\",\"entries\",\"committeewoman\",\"gearing\",\"abandoning\",\"Northwest\",\"elect\",\"mais\",\"enjoy\",\"exorbitant\",\"stirling\",\"Motors'\",\"haunts\",\"smoothbore\",\"Division's\",\"hazard\",\"Eh\",\"brook\",\"folk-tale\",\"unselfish\",\"forbid\",\"sterno-cleido\",\"gaining\",\"barrels\",\"bold\",\"Volkenstein\",\"gusher\",\"bashful\",\"chariot\",\"bathtub\",\"Aaron\",\"RDWS\",\"top-tang\",\"rents\",\"260\",\"brethren\",\"handmade\",\"structuring\",\"Mitch\",\"Morale\",\"Stalin\",\"Bear\",\"violates\",\"obsidian\",\"Capone's\",\"hopping\",\"Spruce\",\"say-so\",\"soon's\",\"aggrieved\",\"continent\",\"price-consciousness\",\"Hirsch\",\"responsiveness\",\"Alsop\",\"malted\",\"magnetically\",\"sic\",\"Alger\",\"Coffee-House\",\"manipulate\",\"anchored\",\"underestimate\",\"disputed\",\"Hiawatha\",\"Germania\",\"fooling\",\"hewed\",\"mating\",\"week-long\",\"furor\",\"hunter\",\"quok\",\"gradients\",\"Drying\",\"tonsil\",\"tokenish\",\"rambling\",\"valve\",\"thrashed\",\"interviewees\",\"oscillator\",\"Blackman\",\"Prohibition\",\"Rychard\",\"westerly\",\"Almost\",\"Gaylor\",\"corner-posts\",\"i.e.\",\"unconscious\",\"BW\",\"Against\",\"Samar\",\"spruced\",\"Sullivan\",\"5777\",\"Sox\",\"1970s\",\"Trains\",\"providential\",\"Boonton\",\"sloe\",\"Bontempo\",\"saxophonist\",\"Neiman-Marcus\",\"kilowatt-hours\",\"Unless\",\"Assembly\",\"sensual\",\"Uruguay\",\"sixty-five\",\"B-52s\",\"Sparrow-size\",\"leaks\",\"appreciably\",\"enacting\",\"chip\",\"kava\",\"footwear\",\"Reading\",\"modeled\",\"Mater\",\"wedded\",\"Phillies\",\"interdenominational\",\"hid\",\"record\",\"compass\",\"stones\",\"volunteers\",\"37-year-old\",\"evidences\",\"monasticism\",\"maples\",\"ice-filled\",\"raging\",\"Gibault\",\"Khartoum\",\"47\",\"anchor\",\"top-ranking\",\"Ruark's\",\"dyed\",\"tenor\",\"consecutive\",\"unmanageably\",\"Manu\",\"receive\",\"Rachel\",\"Ringler's\",\"Courier-Journal\",\"UGF\",\"futile\",\"dost\",\"pimples\",\"honorably\",\"choring\",\"Hays\",\"fancier\",\"arrivals\",\"opiates\",\"SCR\",\"likee\",\"coxcombs\",\"Bandstand\",\"racists\",\"fountainhead\",\"Chases\",\"Teaching\",\"carriage\",\"44\",\"blisters\",\"maid\",\"craters\",\"Cancer\",\"manpower\",\"counteracting\",\"squeaking\",\"editorially\",\"incompleteness\",\"Ahead\",\"paper\",\"1915\",\"steering\",\"prowl\",\"bleeps\",\"sensitives\",\"collarbone\",\"Lander\",\"launches\",\"hemisphere's\",\"welcome\",\"goddamit\",\"Area\",\"Development\",\"Ear-Muffs\",\"woo\",\"gainers\",\"'round\",\"Highlands\",\"groomsmen\",\"dancer\",\"faint\",\"panorama\",\"pineapple\",\"foreign\",\"threshold\",\"interstices\",\"Bridge\",\"11-month-old\",\"misanthrope\",\"Grevyles\",\"line\",\"trespasses\",\"big-ticket\",\"roared\",\"Vol.\",\"Boys\",\"indigestion\",\"1.25-cm\",\"$1.00\",\"Irradiation\",\"caliber\",\"Connections\",\"Nearly\",\"heavy-electrical-goods\",\"Cranston\",\"glaze\",\"reconnaissanace\",\"farmhouses\",\"knick-knacks\",\"0.5-mv./m.\",\"stole\",\"superhuman\",\"Riverside\",\"eyebrows\",\"Wine\",\"Revelation\",\"Jehovah's\",\"bacon\",\"nuclide\",\"widely\",\"craftsman\",\"congregate\",\"trolls\",\"boorish\",\"sceneries\",\"Founded\",\"Winner\",\"curbing\",\"classified\",\"halos\",\"slackened\",\"attained\",\"somethin\",\"fraud\",\"gavottes\",\"135\",\"oratory\",\"exploited\",\"Mfg.\",\"Daphne\",\"Grab\",\"declined\",\"lightness\",\"Carwood\",\"cumbersome\",\"state's\",\"legume\",\"maximum\",\"redress\",\"semi-major\",\"membership\",\"Oregon\",\"publisher\",\"thrilling\",\"Considering\",\"one-man\",\"sealing\",\"seas\",\"hung\",\"Arlen\",\"proportion\",\"down\",\"pronoun\",\"bulks\",\"3/8-inch\",\"2:30-:33.2\",\"accompaniment\",\"Ximenez-Vargas\",\"defunct\",\"'mon\",\"decorators\",\"bluish\",\"recit\",\"snug-fitting\",\"Wholesome\",\"gap\",\"reliability\",\"Brevet\",\"312\",\"$20\",\"Alwin\",\"implicitly\",\"Thermal\",\"Sergei\",\"Dallas-headquartered\",\"spine-chilling\",\"fresh\",\"Protogeometric\",\"philosophies\",\"Indonesia\",\"Reactionary\",\"overestimates\",\"locally\",\"Egyptian\",\"Kieffer\",\"Prague\",\"Beer\",\"Moffett\",\"carbines\",\"degassed\",\"settling\",\"Dried\",\"heal\",\"oracles\",\"deficits\",\"Propaganda\",\"re-created\",\"Seattle\",\"sleeps\",\"Monday\",\"SiH\",\"premiums\",\"marker\",\"aesthetics\",\"motley\",\"seasonally\",\"liar's\",\"Barbara\",\"light-headed\",\"Lyman\",\"prepares\",\"do-gooder\",\"appealed\",\"cooperating\",\"Cyprian\",\"1720\",\"Proves\",\"espousal\",\"kissed\",\"1881-85\",\"tonal\",\"Gun\",\"saluted\",\"iron-poor\",\"recollection\",\"amuse\",\"Neumann\",\"gunner\",\"Deal\",\"casino's\",\"surface-active\",\"Processing\",\"procedural\",\"Diocesan\",\"literalism\",\"Weatherford\",\"salubrious\",\"antiredeposition\",\"extractor\",\"fables\",\"open-meeting\",\"style\",\"lion\",\"tapping\",\"enjoyed\",\"anti-French\",\"unrestricted\",\"Keerist\",\"deteriorated\",\"lilacs\",\"boosted\",\"unfenced\",\"Prabang\",\"possum-hunting\",\"Bow\",\"self-observation\",\"Sphinx\",\"shining\",\"Jana\",\"termed\",\"Bauer-Ecsy\",\"MPH\",\"H.L.\",\"nonacid\",\"emphasizing\",\"suggestions\",\"Atlas\",\"mixing\",\"1941\",\"synce\",\"Inflow\",\"disapproves\",\"footnote\",\"Sail\",\"bittersweet\",\"accusation\",\"hosts\",\"by-passing\",\"soutane\",\"weeklies\",\"Musicians\",\"Aricaras\",\"sloop\",\"1.10.4\",\"oil-bath\",\"counts\",\"presiding\",\"x\",\"Countries\",\"repentant\",\"Walsh\",\"mute\",\"consummately\",\"memory-picture\",\"inheritors\",\"Gretchen\",\"hotter\",\"Tough\",\"rinsing\",\"tails\",\"data-processing\",\"preconceived\",\"redwood\",\"observing\",\"tolerance\",\"shutdowns\",\"secede\",\"$1,450,000,000\",\"consumptive\",\"apples\",\"rider-fashion\",\"frost-bitten\",\"peaceful\",\"Omission\",\"Well-stretched\",\"prostitute\",\"awakened\",\"Demetrius\",\"sub-human\",\"Roots\",\"off-beat\",\"Vandringsar\",\"thyroglobulin\",\"Confabulation\",\"reissue\",\"Shitts\",\"enunciation\",\"daisies\",\"combustion\",\"impact\",\"depravities\",\"waving\",\"actively\",\"Inland\",\"hard-earned\",\"take-off\",\"hunched-up\",\"integrals\",\"relict\",\"world-at-large\",\"loudest\",\"nettled\",\"impose\",\"Unsinkable\",\"enthusiasts\",\"Eternal\",\"livable\",\"dislocations\",\"WTV\",\"kind\",\"coops\",\"frightful\",\"flyers\",\"nearest\",\"dictating\",\"pitch\",\"denouncing\",\"undercurrent\",\"bull-necked\",\"Too\",\"leaping\",\"incompatible\",\"explosives\",\"bodice\",\"captivated\",\"Dictionary\",\"IOCS\",\"numbness\",\"patriot\",\"multi-state\",\"virulence\",\"XRELEASE\",\"story\",\"irrigation\",\"clambered\",\"copies\",\"Record\",\"avant\",\"natty\",\"kerygma\",\"fifty-pound\",\"Stram\",\"Figs.\",\"Burch\",\"shyly\",\"Nietzsche\",\"1-1/4''\",\"cardiovascular\",\"Syndic\",\"Szelenyi\",\"Galantuomo\",\"sonata\",\"recopied\",\"flapper\",\"Fidel\",\"rage\",\"firmness\",\"Bailly\",\"pearl\",\"tourist's\",\"heartbreaking\",\"Greece\",\"puncturing\",\"Gaveston\",\"prospective\",\"horsewoman\",\"Norris\",\"survivals\",\"Breed's\",\"$230,000\",\"Excelsior\",\"committed\",\"children\",\"wash-outs\",\"agricolas\",\"disintegration\",\"Centrale\",\"Luthuli\",\"aorta\",\"Ferro\",\"10,000\",\"Whittaker\",\"Maccabeus\",\"Stella\",\"felon\",\"fare\",\"vacuum\",\"patinas\",\"clearing\",\"irradiated\",\"Reputedly\",\"halls\",\"participant\",\"Bluthenzweig\",\"advised\",\"Salu\",\"Built\",\"carries\",\"Hooked\",\"Sermon\",\"drugless\",\"Waited\",\"Thousand\",\"London-based\",\"vogue\",\"Guardino\",\"self-appointed\",\"painter\",\"Charitable\",\"Amsterdam\",\"$8.50\",\"Kegham\",\"puncher\",\"Shah\",\"contending\",\"garaged\",\"inhabiting\",\"stucco\",\"hand\",\"aces\",\"pleased\",\"Appaloosas\",\"bucking\",\"Paris\",\"snorkle\",\"Bali\",\"agates\",\"erotic\",\"187-mile\",\"grind\",\"Assimilation\",\"layered\",\"Lees\",\"appetizing\",\"rancorous\",\"wholly-owned\",\"Brenner's\",\"Haupts'\",\"type\",\"deny\",\"commingled\",\"stamp\",\"Soviets'\",\"bondage\",\"273\",\"Guttman-type\",\"Breckenridge's\",\"portwatchers\",\"problematic\",\"sensors\",\"rebutted\",\"Speeches\",\"vitiates\",\"Sophia's\",\"glared\",\"willed\",\"commended\",\"hypophyseal\",\"monetary\",\"maximal\",\"regions\",\"backwoods-and-sand-hill\",\"undetectable\",\"dean\",\"bawh\",\"four-o'clock\",\"electric-utility\",\"shock\",\"oversize\",\"urethane\",\"brass-bound\",\"irreproducibility\",\"rubberized\",\"uproar\",\"knitted\",\"semi-autonomous\",\"refrigeration\",\"successes\",\"habe\",\"Theater\",\"Bridgewater\",\"Ogden\",\"lift\",\"Format\",\"plumb\",\"Sievers\",\"sketched\",\"self-contained\",\"hard-hit\",\"revelation\",\"jumble\",\"climbed\",\"Loyalty\",\"bastard\",\"silenced\",\"swore\",\"titian-haired\",\"mythologies\",\"sharks\",\"355\",\"St.-Pol\",\"Beaulieu\",\"ironical\",\"Jour\",\"re-activate\",\"antiques\",\"Christie\",\"camp-made\",\"improvised\",\"Andrews\",\"50-megaton\",\"White's\",\"crawls\",\"enumeration\",\"evaluation\",\"menstruation\",\"cavern\",\"Iran\",\"Frozen\",\"non\",\"dark-gray\",\"triple\",\"clings\",\"terrified\",\"1184\",\"idiom\",\"differentiated\",\"tended\",\"baggy\",\"Farley\",\"uncharted\",\"preserved\",\"citing\",\"Play\",\"torrent\",\"availabilities\",\"alveoli\",\"Beall\",\"Brieff\",\"Administrative\",\"praised\",\"Mahzeer's\",\"Bundle\",\"stumps\",\"preacher\",\"Byrd\",\"aborigines\",\"eqns.\",\"enforcement\",\"economist\",\"'nuff\",\"hire\",\"Pietro\",\"behind\",\"Fosdick's\",\"interglacial\",\"Adonis\",\"fondly\",\"bows\",\"subtracted\",\"issuance\",\"Congressman\",\"caricature\",\"finds\",\"Knight\",\"visiting\",\"mid-air\",\"sister-in-law\",\"Dulles's\",\"malnourished\",\"plasterer\",\"Bailey\",\"not-A\",\"House\",\"neural\",\"endurance\",\"interpenetrate\",\"1862\",\"socio-archaeological\",\"whiskers\",\"behavior\",\"Bosis\",\"upgrade\",\"Diane\",\"Buena\",\"modeling\",\"mile-long\",\"chips\",\"legislature's\",\"12th\",\"single-crystal\",\"explored\",\"isolate\",\"upsetting\",\"ruin\",\"birthday\",\"Botulinal\",\"Rhodesia\",\"flu\",\"Bentley\",\"Hallmark\",\"stormbound\",\"faires\",\"chess\",\"underscored\",\"Supper\",\"greased\",\"Particular\",\"Varner\",\"derrick\",\"alteration\",\"careers\",\"69\",\"coyly\",\"indicated\",\"Rosa\",\"blame\",\"rehabilitations\",\"funnel\",\"hemorrhage\",\"roadways\",\"nine-thirty\",\"Clifton\",\"curved\",\"calculate\",\"betel-stained\",\"journey's\",\"armata\",\"fragrant\",\"Neisse-Oder\",\"Crime\",\"dainty-legged\",\"prefab\",\"post-war\",\"necessitates\",\"melody\",\"canker\",\"dispatches\",\"callers\",\"load\",\"$185\",\"leaky\",\"magnificently\",\"ventilates\",\"amazons\",\"guests\",\"intense\",\"resistor\",\"caricatured\",\"overestimated\",\"reaching\",\"surviving\",\"castle\",\"Liston\",\"thence\",\"Olson\",\"stupidest\",\"Stains\",\"astronaut\",\"first\",\"1727\",\"Burnham\",\"reflects\",\"well-designed\",\"Israel\",\"Exclusive\",\"devoting\",\"Dred\",\"idioms\",\"tat\",\"flagrant\",\"$11\",\"coalition\",\"Presbyterian-St.\",\"breakup\",\"cautions\",\"proportional\",\"Octavia\",\"lust\",\"brazen\",\"Cygne\",\"beachhead\",\"widths\",\"Dashiell\",\"cleansing\",\"misplaced\",\"heathen\",\"haute\",\"15\",\"detachment\",\"communications\",\"Napoleon\",\"Oral\",\"ugliness\",\"depraved\",\"emergencies\",\"African\",\"trample\",\"Soviets\",\"steep\",\"subtle\",\"Emotionally\",\"aperture\",\"fostering\",\"Y-cell\",\"Individual\",\"days'\"],\"x\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[1000]},\"y\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[1000]}},\"selected\":{\"id\":\"ab6950e3-1a4a-47bf-b236-1f985c656920\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"569e69cf-47f6-47c7-ae32-10b36a039bce\",\"type\":\"UnionRenderers\"}},\"id\":\"9009857a-f40f-412b-b811-a1c6319e6a8c\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"renderers\":\"auto\",\"tooltips\":[[\"token\",\"@token\"]]},\"id\":\"d7892d2c-6189-42de-9529-2ed634686c92\",\"type\":\"HoverTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.25},\"fill_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.25},\"line_color\":{\"field\":\"color\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"8555f268-6346-47b5-bd33-6b362bea644d\",\"type\":\"Circle\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"245c9bcf-5204-4065-9884-b63359a271a8\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"9acb9625-d15c-4dd3-bcbc-32c1cfc0692e\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"393239de-2567-4b82-965d-c80ce162038e\",\"type\":\"SaveTool\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"f65874fd-0a7c-4da7-aac9-11df4cb4a93b\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"272f2be0-0951-47aa-b562-31622ee54ce4\",\"type\":\"BasicTicker\"},{\"attributes\":{\"below\":[{\"id\":\"502ab731-63d5-484b-b2db-a155e277be98\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"baaf2593-6453-4471-a0cc-b0247807b409\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"renderers\":[{\"id\":\"502ab731-63d5-484b-b2db-a155e277be98\",\"type\":\"LinearAxis\"},{\"id\":\"98f0a83d-5af6-48c3-a60f-5c9d3f8d8324\",\"type\":\"Grid\"},{\"id\":\"baaf2593-6453-4471-a0cc-b0247807b409\",\"type\":\"LinearAxis\"},{\"id\":\"4e6d2d73-4639-42fd-8f73-3dc4156cfa9e\",\"type\":\"Grid\"},{\"id\":\"a96a8b68-f1e5-4b0b-8151-761048bb141d\",\"type\":\"BoxAnnotation\"},{\"id\":\"0c0594a2-3377-4ca3-8f9e-8048ff958d3b\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"f65874fd-0a7c-4da7-aac9-11df4cb4a93b\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"90090990-b980-4ac0-83c7-defd07c08f63\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"3ca30fdd-96f4-4d0c-ab8c-9021a0862678\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"3d958c23-6946-4bcb-9bf0-1a98b8202ede\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"846631f4-e1a8-4222-ad1e-f5dd984c21ff\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"d90906d1-3967-45fe-9450-f1bfd5259e49\",\"type\":\"LinearScale\"}},\"id\":\"6bb5ca13-5bd9-414a-8c28-446996df65cf\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"569e69cf-47f6-47c7-ae32-10b36a039bce\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"overlay\":{\"id\":\"a96a8b68-f1e5-4b0b-8151-761048bb141d\",\"type\":\"BoxAnnotation\"}},\"id\":\"86be0c53-7f09-4e1c-87f4-f8db33c77f71\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"d90906d1-3967-45fe-9450-f1bfd5259e49\",\"type\":\"LinearScale\"},{\"attributes\":{\"data_source\":{\"id\":\"9009857a-f40f-412b-b811-a1c6319e6a8c\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"8555f268-6346-47b5-bd33-6b362bea644d\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"245c9bcf-5204-4065-9884-b63359a271a8\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"f584ac1a-a98c-47bf-82c0-edffab526ae2\",\"type\":\"CDSView\"}},\"id\":\"0c0594a2-3377-4ca3-8f9e-8048ff958d3b\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"a96a8b68-f1e5-4b0b-8151-761048bb141d\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"3d958c23-6946-4bcb-9bf0-1a98b8202ede\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"51898818-9561-46a8-8ae1-422ec76cfd68\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"36c19aa1-345f-4e23-9da9-91e013eec25a\",\"type\":\"BasicTicker\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":{\"id\":\"925aa088-6fee-4598-9ccb-34e1d3ad4f9e\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"9acb9625-d15c-4dd3-bcbc-32c1cfc0692e\",\"type\":\"PanTool\"},{\"id\":\"925aa088-6fee-4598-9ccb-34e1d3ad4f9e\",\"type\":\"WheelZoomTool\"},{\"id\":\"86be0c53-7f09-4e1c-87f4-f8db33c77f71\",\"type\":\"BoxZoomTool\"},{\"id\":\"393239de-2567-4b82-965d-c80ce162038e\",\"type\":\"SaveTool\"},{\"id\":\"9d51ee32-cba6-4366-9055-23bc732c6531\",\"type\":\"ResetTool\"},{\"id\":\"51898818-9561-46a8-8ae1-422ec76cfd68\",\"type\":\"HelpTool\"},{\"id\":\"d7892d2c-6189-42de-9529-2ed634686c92\",\"type\":\"HoverTool\"}]},\"id\":\"90090990-b980-4ac0-83c7-defd07c08f63\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"925aa088-6fee-4598-9ccb-34e1d3ad4f9e\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null},\"id\":\"3ca30fdd-96f4-4d0c-ab8c-9021a0862678\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"b03d7a30-15d5-4eaf-b042-96287c3b41bd\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"edfcaff1-e8dd-4a0f-8421-d69ad13960a2\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"source\":{\"id\":\"9009857a-f40f-412b-b811-a1c6319e6a8c\",\"type\":\"ColumnDataSource\"}},\"id\":\"f584ac1a-a98c-47bf-82c0-edffab526ae2\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"9d51ee32-cba6-4366-9055-23bc732c6531\",\"type\":\"ResetTool\"},{\"attributes\":{\"formatter\":{\"id\":\"b03d7a30-15d5-4eaf-b042-96287c3b41bd\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"6bb5ca13-5bd9-414a-8c28-446996df65cf\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"36c19aa1-345f-4e23-9da9-91e013eec25a\",\"type\":\"BasicTicker\"}},\"id\":\"baaf2593-6453-4471-a0cc-b0247807b409\",\"type\":\"LinearAxis\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"6bb5ca13-5bd9-414a-8c28-446996df65cf\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"36c19aa1-345f-4e23-9da9-91e013eec25a\",\"type\":\"BasicTicker\"}},\"id\":\"4e6d2d73-4639-42fd-8f73-3dc4156cfa9e\",\"type\":\"Grid\"},{\"attributes\":{\"plot\":{\"id\":\"6bb5ca13-5bd9-414a-8c28-446996df65cf\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"272f2be0-0951-47aa-b562-31622ee54ce4\",\"type\":\"BasicTicker\"}},\"id\":\"98f0a83d-5af6-48c3-a60f-5c9d3f8d8324\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null},\"id\":\"846631f4-e1a8-4222-ad1e-f5dd984c21ff\",\"type\":\"DataRange1d\"}],\"root_ids\":[\"6bb5ca13-5bd9-414a-8c28-446996df65cf\"]},\"title\":\"Bokeh Application\",\"version\":\"0.13.0\"}};\n",
              "  var render_items = [{\"docid\":\"943999c3-3f5c-4698-8227-9469bc5f7c3b\",\"roots\":{\"6bb5ca13-5bd9-414a-8c28-446996df65cf\":\"19516b73-0244-4658-b769-6a32c8525302\"}}];\n",
              "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        embed_document(root);\n",
              "        clearInterval(timer);\n",
              "      }\n",
              "      attempts++;\n",
              "      if (attempts > 100) {\n",
              "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
              "        clearInterval(timer);\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "tags": [],
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "6bb5ca13-5bd9-414a-8c28-446996df65cf"
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TgmDHM9Dl7W7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Посчитайте эмбеддинги для всех слов из трейна и для нескольких случайных слов из теста, которые не встречаются в трейне, найдите их ближайших соседей по их эмбеддигам символьного уровня."
      ]
    },
    {
      "metadata": {
        "id": "1bctty__mOOz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples = np.random.choice(list(word2ind.keys()), len(list(word2ind.keys())), replace=False)\n",
        "data = [[[char2ind.get(symb, 0) for symb in word[:MAX_WORD_LEN]] for word in samples]]\n",
        "dummy = [list(range(len(list(word2ind.keys()))))]\n",
        "index2word = samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G-Uq2SlzYQoC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings, tags = next(iterate_batches((data, dummy), len(list(word2ind.keys()))))\n",
        "embeddings = model(LongTensor(embeddings))\n",
        "embeddings = embeddings.squeeze(1).detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRqRXjKGum1G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_rare_words():\n",
        "    rare_words = []\n",
        "    rare_words_count = 0\n",
        "\n",
        "    for entity in test_data:\n",
        "        for word, tag in entity:\n",
        "            if not word in words:\n",
        "                rare_words.append(word)\n",
        "    return rare_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpTKIy7uvyQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rare_words = get_rare_words()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nzOsiXbv63Bt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rare_words = np.random.choice(rare_words, 5, replace=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8lADi_ALv-nq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rare_data = [[[char2ind.get(symb, 0) for symb in word[:MAX_WORD_LEN]] for word in rare_words]]\n",
        "dummy = [list(range(len(rare_words)))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZS8QSgDIwhvz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rare_embeddings, tags = next(iterate_batches((rare_data, dummy), len(rare_words)))\n",
        "rare_embeddings = model(LongTensor(rare_embeddings))\n",
        "rare_embeddings = rare_embeddings.squeeze(1).detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aPAXggkswrK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def distance(x, y):\n",
        "    return np.sum(x * y) / np.sqrt(np.sum(x ** 2) * np.sum(y ** 2))\n",
        "\n",
        "def get_distances(embeddings, rare_embeddings):\n",
        "    matrix = []\n",
        "    \n",
        "    for rare_embedding in rare_embeddings:\n",
        "        row = []\n",
        "        for embedding in embeddings:\n",
        "            row.append(distance(rare_embedding, embedding))\n",
        "        matrix.append(row)\n",
        "      \n",
        "    return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cobJ2GQaxTaz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "matrix = get_distances(embeddings, rare_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S-9dqxO00w_t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "matrix = np.argsort(matrix, axis=-1)\n",
        "best = [matrix[i][::-1][:5] for i in range(len(matrix))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41_eLUVK1uOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "80abac64-ed3a-49fe-da34-062aadda8036"
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(best)):\n",
        "    print('TEST WORD: {}'.format(rare_words[i]))\n",
        "    for j in range(len(best[0])):\n",
        "        print('\\t{}'.format(samples[best[i][j]]))"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST WORD: immodest\n",
            "\tunhealthy\n",
            "\tpresentable\n",
            "\tobsequious\n",
            "\tinsuperable\n",
            "\tawesome\n",
            "TEST WORD: Toot\n",
            "\tNewcomers\n",
            "\tTyler\n",
            "\tSteam\n",
            "\tcoworkers\n",
            "\tDetroit\n",
            "TEST WORD: gorge\n",
            "\tseek\n",
            "\ttrotted\n",
            "\twedged\n",
            "\tgoitre\n",
            "\tmovie\n",
            "TEST WORD: elucidation\n",
            "\tsubtleties\n",
            "\temasculation\n",
            "\tregulation\n",
            "\tproclamation\n",
            "\tvineyards\n",
            "TEST WORD: t'jawn\n",
            "\tApril-June\n",
            "\tDorenzo\n",
            "\tFought\n",
            "\tTudor\n",
            "\tpound\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s8WVAmMWqsrS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача\n",
        "\n",
        "[Опрос](https://goo.gl/forms/R6UqcESWIjtVSA6J3)"
      ]
    }
  ]
}