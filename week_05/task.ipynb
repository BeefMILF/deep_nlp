{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 05 - RNNs Intro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YfeVtEQwRmsR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "0374f2f7-4acf-4173-bb46-b73946dbc3a6"
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip install -qq bokeh==0.13.0\n",
        "!wget -O surnames.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ji7dhr9FojPeV51dDlKRERIqr3vdZfhu\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x59214000 @  0x7f15eb3522a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YKoTq9xW-PdW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Q5wMjxQMeQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Рекуррентные нейронные сети"
      ]
    },
    {
      "metadata": {
        "id": "NY5-j_RsRzxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Классификация фамилий\n",
        "\n",
        "Теперь - по языкам:"
      ]
    },
    {
      "metadata": {
        "id": "TPPJoWEpSN_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "82c8c578-0d25-4f71-99ee-c5729988e977"
      },
      "cell_type": "code",
      "source": [
        "data, labels = [], []\n",
        "with open('surnames.txt') as f:\n",
        "    for line in f:\n",
        "        surname, lang = line.strip().split('\\t')\n",
        "        data.append(surname)\n",
        "        labels.append(lang)\n",
        "\n",
        "for i in np.random.randint(0, len(data), 10):\n",
        "    print(data[i], labels[i])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mikhnev Russian\n",
            "Morcos Arabic\n",
            "Morrow English\n",
            "Berezinsky Russian\n",
            "Badyaev Russian\n",
            "Teale English\n",
            "Prigorodov Russian\n",
            "Greening English\n",
            "Jankevich Russian\n",
            "Bahin Russian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6bVJlxzYhWuf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Разминка\n",
        "\n",
        "Проверьте свои знания - попробуйте самостоятельно предсказать, к какому языку относится фамилия :)"
      ]
    },
    {
      "metadata": {
        "id": "G7JquuckaAGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "def test_generator():\n",
        "    classes = np.unique(labels)\n",
        "    weights = compute_class_weight('balanced', classes, labels)\n",
        "    classes = {label: ind for ind, label in enumerate(classes)}\n",
        "\n",
        "    probs = np.array([weights[classes[label]] for label in labels])\n",
        "    probs /= probs.sum()\n",
        "\n",
        "    ind = np.random.choice(np.arange(len(data)), p=probs)\n",
        "    yield data[ind]\n",
        "    \n",
        "    while True:\n",
        "        new_ind = np.random.choice(np.arange(len(data)), p=probs)\n",
        "        yield labels[ind], data[new_ind]\n",
        "        ind = new_ind\n",
        "        \n",
        "gen = test_generator()\n",
        "question = next(gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KNeNxm1hsKs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Запускайте, смотрите на фамилию, которая выведется - и выбирайте язык в выпадающем списке."
      ]
    },
    {
      "metadata": {
        "id": "-Q3OSXpAY8BS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "cellView": "form",
        "outputId": "b5157ac1-28eb-4d45-9c40-699111c2a8b8"
      },
      "cell_type": "code",
      "source": [
        "#@title Проверим себя (или адекватность данных) { run: \"auto\" }\n",
        "answer = \"Korean\" #@param [\"Arabic\", \"Chinese\", \"Czech\", \"Dutch\", \"English\", \"French\", \"German\", \"Greek\", \"Irish\", \"Italian\", \"Japanese\", \"Korean\", \"Polish\", \"Portuguese\", \"Russian\", \"Scottish\", \"Spanish\", \"Vietnamese\"]\n",
        "\n",
        "correct_answer, question = next(gen)\n",
        "\n",
        "if 'correct_count' not in globals():\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "else:\n",
        "    if answer == correct_answer:\n",
        "        print('You are correct', end=' ')\n",
        "        correct_count += 1\n",
        "    else:\n",
        "        print(\"No, it's\", correct_answer, end=' ')\n",
        "\n",
        "    total_count += 1\n",
        "    print('({} / {})'.format(correct_count, total_count))\n",
        "    \n",
        "print('Next surname:', question)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You are correct (1 / 1)\n",
            "Next surname: Bao\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Yz20u6dhll0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Разбиение данных"
      ]
    },
    {
      "metadata": {
        "id": "LfP4Sb68TWlY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Для начала нужно построить сплит данных на трейн/тест. Сложность в том, что классы распределены неравномерны, а отрезать нужно от каждого класса пропорциональное количество данных на тест. Для этого нужно использовать `stratify` параметр функции `train_test_split` (либо `StratifiedShuffleSplit`, либо, при большом желании, `GroupShuffleSplit`)."
      ]
    },
    {
      "metadata": {
        "id": "1eE-s7q7RmAM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train, data_test, labels_train, labels_test = train_test_split(\n",
        "    data, labels, test_size=0.3, stratify=labels, random_state=42\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uzX5zHobUHc0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "b8764745-ff11-414f-83a4-2b70dba5f07d"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "langs = set(labels)\n",
        "\n",
        "train_distribution = Counter(labels_train)\n",
        "train_distribution = [train_distribution[lang] for lang in langs]\n",
        "\n",
        "test_distribution = Counter(labels_test)\n",
        "test_distribution = [test_distribution[lang] for lang in langs]\n",
        "\n",
        "plt.figure(figsize=(17, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(langs)), train_distribution, bar_width, align='center', alpha=0.5, label='train')\n",
        "plt.bar(np.arange(len(langs)) + bar_width, test_distribution, bar_width, align='center', alpha=0.5, label='test')\n",
        "plt.xticks(np.arange(len(langs)) + bar_width / 2, langs)\n",
        "plt.legend()\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+AAAAEvCAYAAADFBqs1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE3BJkiRJkmpgAi5JkiRJUg3+P3yPfzXKo/bNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fab74ad94a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MgmDagAq5-mm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Бейзлайн"
      ]
    },
    {
      "metadata": {
        "id": "pDgB4iVCWbNE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Всегда надо начинать с бейзлайна - воспользуемся нашей любимой парой vectorizer-logistic regression:"
      ]
    },
    {
      "metadata": {
        "id": "PLxUE4thXyV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "36168240-400b-4bfe-a5e6-2489a3a4a995"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(analyzer='char', ngram_range=(1, 4))),\n",
        "    ('log_regression', LogisticRegression())\n",
        "])\n",
        "\n",
        "model.fit(data_train, labels_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('vectorizer', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "        ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
              "       ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "wz9ngs_lWn3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Какие метрики будем считать? Тут многоклассовая классификация, поэтому всё очень неоднозначно.\n",
        "\n",
        "Имеет смысл посмотреть на accuracy и на F1-score'ы для каждого класса."
      ]
    },
    {
      "metadata": {
        "id": "OJZt8sKM6zEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "8685142c-7649-4267-9a38-07298b5f5033"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "preds = model.predict(data_test)\n",
        "\n",
        "print('Accuracy = {:.2%}'.format(accuracy_score(labels_test, preds)))\n",
        "print('Classification report:')\n",
        "print(classification_report(labels_test, preds))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 83.45%\n",
            "Classification report:\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "     Arabic       0.90      1.00      0.95       600\n",
            "    Chinese       0.65      0.57      0.61        80\n",
            "      Czech       0.58      0.28      0.38       156\n",
            "      Dutch       0.85      0.49      0.62        89\n",
            "    English       0.72      0.84      0.78      1101\n",
            "     French       0.52      0.19      0.28        83\n",
            "     German       0.61      0.51      0.55       217\n",
            "      Greek       0.93      0.64      0.76        61\n",
            "      Irish       0.65      0.44      0.53        70\n",
            "    Italian       0.74      0.72      0.73       213\n",
            "   Japanese       0.91      0.90      0.90       297\n",
            "     Korean       0.25      0.14      0.18        28\n",
            "     Polish       0.61      0.33      0.43        42\n",
            " Portuguese       0.27      0.14      0.18        22\n",
            "    Russian       0.92      0.96      0.94      2823\n",
            "   Scottish       0.00      0.00      0.00        30\n",
            "    Spanish       0.48      0.28      0.35        89\n",
            " Vietnamese       0.50      0.18      0.27        22\n",
            "\n",
            "avg / total       0.82      0.83      0.82      6023\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VNRhUTNaW45M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "F1-score'ы можно агрегировать разными способами:\n",
        "- weighted - это как посчитал classification_report - если нам важнее предсказывать хорошо более частотные фамилии\n",
        "- macro - простое усреднение - если важно предсказывать все, независимо от того, сколько каждого класса в тестовой выборке\n",
        "- micro - обычный подсчет F1-score по суммам всех true positive, false positive и false negative\n",
        "\n",
        "Weighted и micro - две метрики, учитывающие дисбаланс классов. Но в нашем случае неочевидно, есть ли дисбаланс, да?"
      ]
    },
    {
      "metadata": {
        "id": "I0SwCbWN8ZQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "ae2fd13e-4d21-47f2-d9bf-9605995893e5"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.ticker as ticker\n",
        "\n",
        "label_names = list(set(labels_test))\n",
        "confusion = confusion_matrix(labels_test, preds, labels=label_names).astype(np.float)\n",
        "confusion /= confusion.sum(axis=-1, keepdims=True)\n",
        "\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(confusion, cmap='Reds')\n",
        "fig.colorbar(cax)\n",
        "\n",
        "ax.set_xticklabels([''] + label_names, rotation=45)\n",
        "ax.set_yticklabels([''] + label_names)\n",
        "\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAIFCAYAAADvHZakAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fab30ced128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qo0lsnzV-i-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple RNN"
      ]
    },
    {
      "metadata": {
        "id": "Hb4_VaBIMVFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Основная прелесть RNN - расшаренные параметры. Посмотрите на картинку:\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg =x250)\n",
        "\n",
        "*From [(The Unreasonable Effectiveness of Recurrent Neural Networks)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Первый пример - это обычная полносвязная сеть. Каждый следующий демонстрирует обработку некоторой последовательности произвольной длины (красные прямоугольнички) и генерацию выходной последовательности, также произвольной длины (синие прямоугольники).\n",
        "\n",
        "При этом зеленые прямоугольники в каждом рисунке - это одни и те же веса. Получается, мы, с одной стороны, обучаем очень-очень глубокую сеть (если посмотреть на неё перевернутую), а с другой - строго ограниченное количество параметров.\n",
        "\n",
        "---\n",
        "Напишем сразу простую RNN!\n",
        "\n",
        "Напомню, делает она примерно вот это:\n",
        "\n",
        "![rnn-unrolled](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png =x220)\n",
        "\n",
        "*From [(Understanding LSTM Networks)](http://colah.github.io/posts/2015-08-Understanding-LSTMs)*\n",
        "\n",
        "Вообще говоря, можно придумать много вариаций на тему такой реализации. В нашем случае, обработка будет такой:\n",
        "$$h_t = tanh(W_h [h_{t-1}; x_t] + b_h)$$\n",
        "\n",
        "$h_{t-1}$ - скрытое состояние, полученное на предыдущем шаге, $x_t$ - входной вектор. $[h_{t-1}; x_t]$ - простая конкатенация векторов. Всё как на картинке!\n",
        "\n",
        "Проверим нашу сеть на очень простой задаче: заставим её говорить индекс первого элемента в последовательности.\n",
        "\n",
        "Т.е. для последовательности `[1, 2, 1, 3]` сеть должна предсказывать `1`.\n",
        "\n",
        "Начнем с генерации батча."
      ]
    },
    {
      "metadata": {
        "id": "yOI4JGgHT-z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3fd7178e-400f-4926-cafe-cdfedce85288"
      },
      "cell_type": "code",
      "source": [
        "def generate_data(batch_size=128, seq_len=5):\n",
        "    data = torch.randint(0, 10, size=(seq_len, batch_size), dtype=torch.long)\n",
        "    return data, data[0]\n",
        "\n",
        "X_val, y_val = generate_data(16, 5)\n",
        "X_val, y_val"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1, 2, 6, 7, 8, 4, 4, 7, 8, 3, 5, 8, 0, 1, 6, 3],\n",
              "         [4, 1, 4, 2, 5, 2, 8, 9, 8, 0, 1, 1, 4, 0, 1, 7],\n",
              "         [2, 5, 9, 2, 5, 0, 4, 1, 9, 4, 4, 3, 4, 7, 5, 1],\n",
              "         [8, 9, 9, 2, 7, 9, 3, 4, 2, 5, 8, 3, 6, 8, 1, 5],\n",
              "         [2, 4, 3, 4, 1, 6, 9, 7, 3, 6, 7, 0, 2, 7, 6, 3]]),\n",
              " tensor([1, 2, 6, 7, 8, 4, 4, 7, 8, 3, 5, 8, 0, 1, 6, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "EQ0Gsr4SFNtB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Обратите внимание, что батч имеет размерность `(sequence_length, batch_size, input_size)`. Все `RNN` в pytorch работают с таким форматом по умолчанию.\n",
        "\n",
        "Сделано это из соображений производительности, но при желании можно поменять такое поведение с помощью аргумента `batch_first`."
      ]
    },
    {
      "metadata": {
        "id": "PqS7HPRhZSBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте класс `SimpleRNN`, выполняющий рассчеты по формуле выше."
      ]
    },
    {
      "metadata": {
        "id": "ed1b2TUvZRs0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self._hidden_size = hidden_size\n",
        "        self._hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        seq_len, batch_size = inputs.shape[:2]\n",
        "        \n",
        "        if hidden is None:\n",
        "            hidden = inputs.new_zeros(batch_size, self._hidden_size)\n",
        "         \n",
        "        for i in range(len(inputs)):\n",
        "            hidden = torch.tanh(self._hidden(torch.cat((hidden, inputs[i]), -1)))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KS2xw2YIZ_EU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте класс `MemorizerModel`, с последовательностью операций `Embedding -> SimpleRNN -> Linear`. Можно использовать `nn.Sequential`\n",
        "\n",
        "Чтобы сделать эмбеддинги, можно воспользоваться `nn.Embedding.from_pretrained`. Для простоты будем делать one-hot-encoding представление - для этого нужно просто инициализировать сеть единичной матрицей `torch.eye(N)`."
      ]
    },
    {
      "metadata": {
        "id": "aEUr4Xa9Z81I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.eye(10)),\n",
        "    SimpleRNN(10, 32),\n",
        "    nn.Linear(32, 10)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eiDRoQWDawaW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Запустим обучение:"
      ]
    },
    {
      "metadata": {
        "id": "IbVk7zUjUQ_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f76a4ad8-1b00-4c2b-b702-3baa225e8542"
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters())\n",
        "\n",
        "total_loss = 0\n",
        "epochs_count = 1000\n",
        "for epoch_ind in range(epochs_count):\n",
        "    X_train, y_train = generate_data(seq_len=5)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    rnn.train()\n",
        "    \n",
        "    logits = rnn(X_train)\n",
        "\n",
        "    loss = criterion(logits, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if (epoch_ind + 1) % 100 == 0:\n",
        "        rnn.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = rnn(X_val)\n",
        "            val_loss = criterion(logits, y_val)\n",
        "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
        "                                                             total_loss / 100, val_loss.item()))\n",
        "            total_loss = 0"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100/1000] Train: 2.085 Val: 1.598\n",
            "[200/1000] Train: 1.030 Val: 0.651\n",
            "[300/1000] Train: 0.331 Val: 0.150\n",
            "[400/1000] Train: 0.094 Val: 0.063\n",
            "[500/1000] Train: 0.048 Val: 0.037\n",
            "[600/1000] Train: 0.030 Val: 0.025\n",
            "[700/1000] Train: 0.021 Val: 0.018\n",
            "[800/1000] Train: 0.016 Val: 0.014\n",
            "[900/1000] Train: 0.013 Val: 0.011\n",
            "[1000/1000] Train: 0.010 Val: 0.009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dQJg3FROIq4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Посмотрите на то, как влияет длина последовательности на работу сети. \n",
        "\n",
        "Во-первых, посмотрите, с какой длиной сеть в состоянии учиться. Во-вторых, попробуйте обучить сеть с небольшой длиной последовательности, а потом применять её к более длинным."
      ]
    },
    {
      "metadata": {
        "id": "cKL4Lq_s6QgX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "83ffd69a-5acd-4e2c-fe4c-fbd38f976543"
      },
      "cell_type": "code",
      "source": [
        "rnn = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.eye(10)),\n",
        "    SimpleRNN(10, 32),\n",
        "    nn.Linear(32, 10)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters())\n",
        "\n",
        "total_loss = 0\n",
        "epochs_count = 1000\n",
        "for epoch_ind in range(epochs_count):\n",
        "    X_train, y_train = generate_data(seq_len=7)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    rnn.train()\n",
        "    \n",
        "    logits = rnn(X_train)\n",
        "\n",
        "    loss = criterion(logits, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if (epoch_ind + 1) % 100 == 0:\n",
        "        rnn.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = rnn(X_val)\n",
        "            val_loss = criterion(logits, y_val)\n",
        "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
        "                                                             total_loss / 100, val_loss.item()))\n",
        "            total_loss = 0"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100/1000] Train: 2.302 Val: 2.300\n",
            "[200/1000] Train: 1.645 Val: 2.783\n",
            "[300/1000] Train: 0.543 Val: 4.094\n",
            "[400/1000] Train: 0.172 Val: 5.404\n",
            "[500/1000] Train: 0.063 Val: 6.134\n",
            "[600/1000] Train: 0.035 Val: 6.631\n",
            "[700/1000] Train: 0.024 Val: 7.019\n",
            "[800/1000] Train: 0.017 Val: 7.333\n",
            "[900/1000] Train: 0.013 Val: 7.577\n",
            "[1000/1000] Train: 0.010 Val: 7.729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9nyWEel-6TqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "322ba5e4-b14e-4522-9ee8-d631073c85a3"
      },
      "cell_type": "code",
      "source": [
        "rnn = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.eye(10)),\n",
        "    SimpleRNN(10, 32),\n",
        "    nn.Linear(32, 10)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters())\n",
        "\n",
        "total_loss = 0\n",
        "epochs_count = 1000\n",
        "for epoch_ind in range(epochs_count):\n",
        "    X_train, y_train = generate_data(seq_len=10)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    rnn.train()\n",
        "    \n",
        "    logits = rnn(X_train)\n",
        "\n",
        "    loss = criterion(logits, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if (epoch_ind + 1) % 100 == 0:\n",
        "        rnn.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = rnn(X_val)\n",
        "            val_loss = criterion(logits, y_val)\n",
        "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
        "                                                             total_loss / 100, val_loss.item()))\n",
        "            total_loss = 0"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100/1000] Train: 2.306 Val: 2.302\n",
            "[200/1000] Train: 2.303 Val: 2.311\n",
            "[300/1000] Train: 2.278 Val: 2.660\n",
            "[400/1000] Train: 1.033 Val: 9.046\n",
            "[500/1000] Train: 0.458 Val: 10.939\n",
            "[600/1000] Train: 0.181 Val: 12.019\n",
            "[700/1000] Train: 0.089 Val: 12.571\n",
            "[800/1000] Train: 0.065 Val: 13.118\n",
            "[900/1000] Train: 0.039 Val: 13.402\n",
            "[1000/1000] Train: 0.046 Val: 13.606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K3W5viXu-4-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Утверждается, что `relu` подходит для RNN лучше. Попробуйте и её."
      ]
    },
    {
      "metadata": {
        "id": "cSEMuaJCIUHE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReluSimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self._hidden_size = hidden_size\n",
        "        self._hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        seq_len, batch_size = inputs.shape[:2]\n",
        "        \n",
        "        if hidden is None:\n",
        "            hidden = inputs.new_zeros(batch_size, self._hidden_size)\n",
        "         \n",
        "        for i in range(len(inputs)):\n",
        "            hidden = torch.relu(self._hidden(torch.cat((hidden, inputs[i]), -1)))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-uVD56A7IbP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1148bba3-0693-435d-cb7b-839daff935ee"
      },
      "cell_type": "code",
      "source": [
        "rnn = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.eye(10)),\n",
        "    ReluSimpleRNN(10, 32),\n",
        "    nn.Linear(32, 10)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters())\n",
        "\n",
        "total_loss = 0\n",
        "epochs_count = 1000\n",
        "for epoch_ind in range(epochs_count):\n",
        "    X_train, y_train = generate_data(seq_len=5)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    rnn.train()\n",
        "    \n",
        "    logits = rnn(X_train)\n",
        "\n",
        "    loss = criterion(logits, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if (epoch_ind + 1) % 100 == 0:\n",
        "        rnn.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = rnn(X_val)\n",
        "            val_loss = criterion(logits, y_val)\n",
        "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
        "                                                             total_loss / 100, val_loss.item()))\n",
        "            total_loss = 0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100/1000] Train: 2.305 Val: 2.306\n",
            "[200/1000] Train: 2.303 Val: 2.302\n",
            "[300/1000] Train: 1.959 Val: 0.720\n",
            "[400/1000] Train: 0.096 Val: 0.006\n",
            "[500/1000] Train: 0.003 Val: 0.003\n",
            "[600/1000] Train: 0.001 Val: 0.001\n",
            "[700/1000] Train: 0.001 Val: 0.001\n",
            "[800/1000] Train: 0.001 Val: 0.001\n",
            "[900/1000] Train: 0.000 Val: 0.000\n",
            "[1000/1000] Train: 0.000 Val: 0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DZNGY1IOIiHy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d3944b0e-d6ac-4147-b8c0-5070ab91cafc"
      },
      "cell_type": "code",
      "source": [
        "rnn = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.eye(10)),\n",
        "    ReluSimpleRNN(10, 32),\n",
        "    nn.Linear(32, 10)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters())\n",
        "\n",
        "total_loss = 0\n",
        "epochs_count = 1000\n",
        "for epoch_ind in range(epochs_count):\n",
        "    X_train, y_train = generate_data(seq_len=7)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    rnn.train()\n",
        "    \n",
        "    logits = rnn(X_train)\n",
        "\n",
        "    loss = criterion(logits, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if (epoch_ind + 1) % 100 == 0:\n",
        "        rnn.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = rnn(X_val)\n",
        "            val_loss = criterion(logits, y_val)\n",
        "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
        "                                                             total_loss / 100, val_loss.item()))\n",
        "            total_loss = 0"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100/1000] Train: 2.305 Val: 2.313\n",
            "[200/1000] Train: 2.303 Val: 2.315\n",
            "[300/1000] Train: 2.304 Val: 2.305\n",
            "[400/1000] Train: 2.303 Val: 2.316\n",
            "[500/1000] Train: 2.303 Val: 2.310\n",
            "[600/1000] Train: 2.303 Val: 2.315\n",
            "[700/1000] Train: 2.303 Val: 2.302\n",
            "[800/1000] Train: 2.303 Val: 2.305\n",
            "[900/1000] Train: 2.303 Val: 2.309\n",
            "[1000/1000] Train: 1.938 Val: 6.052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bMkd7cd1ImG3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "05ca50ee-e452-4196-919a-904ac76ab810"
      },
      "cell_type": "code",
      "source": [
        "rnn = nn.Sequential(\n",
        "    nn.Embedding.from_pretrained(torch.eye(10)),\n",
        "    ReluSimpleRNN(10, 32),\n",
        "    nn.Linear(32, 10)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters())\n",
        "\n",
        "total_loss = 0\n",
        "epochs_count = 1000\n",
        "for epoch_ind in range(epochs_count):\n",
        "    X_train, y_train = generate_data(seq_len=10)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    rnn.train()\n",
        "    \n",
        "    logits = rnn(X_train)\n",
        "\n",
        "    loss = criterion(logits, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if (epoch_ind + 1) % 100 == 0:\n",
        "        rnn.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = rnn(X_val)\n",
        "            val_loss = criterion(logits, y_val)\n",
        "            print('[{}/{}] Train: {:.3f} Val: {:.3f}'.format(epoch_ind + 1, epochs_count, \n",
        "                                                             total_loss / 100, val_loss.item()))\n",
        "            total_loss = 0"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[100/1000] Train: 2.305 Val: 2.306\n",
            "[200/1000] Train: 2.304 Val: 2.301\n",
            "[300/1000] Train: 2.304 Val: 2.300\n",
            "[400/1000] Train: 2.303 Val: 2.306\n",
            "[500/1000] Train: 2.303 Val: 2.303\n",
            "[600/1000] Train: 2.303 Val: 2.304\n",
            "[700/1000] Train: 2.303 Val: 2.300\n",
            "[800/1000] Train: 2.303 Val: 2.299\n",
            "[900/1000] Train: 2.303 Val: 2.301\n",
            "[1000/1000] Train: 2.303 Val: 2.295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nSHNuT5b61Ky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Обучение RNN'ок\n",
        "\n",
        "![bptt](https://image.ibb.co/cEYkw9/rnn_bptt_with_gradients.png =x400)  \n",
        "*From [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)*\n",
        "\n",
        "Если всё пошло по плану, мы должны были посмотреть на то, как RNN'ки забывают. \n",
        "\n",
        "Чтобы понять причину, стоит вспомнить, как именно происходит обучение RNN, например, здесь: [Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) или здесь - [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/).\n",
        "\n",
        "Если кратко, одна из проблем обучения рекуррентных сетей - *взрыв градиентов*. Она проявляется, когда матрица весов такова, что увеличивает норму вектора градиента при обратном проходе. В результате норма градиента экспоненциально растет и он \"взрывается\". \n",
        "\n",
        "Эту проблему можно решить с помощью клипинга градиентов: `nn.utils.clip_grad_norm_(rnn.parameters(), 1.)`."
      ]
    },
    {
      "metadata": {
        "id": "13x5erUgTjDC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTM и GRU\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PAjZh9YkYAMH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Другая проблема - *затухание градиентов*. Она связана наоборот - с экспоненциальным затуханием градиентов. И вот её решают уже более сложными способами. \n",
        "\n",
        "А именно - используют gate'овые архитектуры.\n",
        "\n",
        "Идея gate'а простая, но важная, используются они далеко не только в рекуррентных сетях.\n",
        "\n",
        "Если посмотреть на то, как работает наша SimpleRNN, можно заметить, что каждый раз память (т.е. $h_t$) перезаписывается. Хочется иметь возможность сделать эту перезапись контролируемой: не отбрасывать какую-то важную инфомацию из вектора.\n",
        "\n",
        "Заведем для этого вектор $g \\in \\{0,1\\}^n$, который будет говорить, какие ячейки $h_{t-1}$ хорошие, а вместо каких стоит подставить новые значения:\n",
        "$$h_t = g \\odot f(x_t, h_{t-1}) + (1 - g) \\odot h_{t-1}.$$\n",
        "\n",
        "Например:\n",
        "$$\n",
        " \\begin{bmatrix}\n",
        "  8 \\\\\n",
        "  11 \\\\\n",
        "  3 \\\\\n",
        "  7\n",
        " \\end{bmatrix} =\n",
        " \\begin{bmatrix}\n",
        "  0 \\\\\n",
        "  1 \\\\\n",
        "  0 \\\\\n",
        "  0\n",
        " \\end{bmatrix}\n",
        " \\odot\n",
        "  \\begin{bmatrix}\n",
        "  7 \\\\\n",
        "  11 \\\\\n",
        "  6 \\\\\n",
        "  5\n",
        " \\end{bmatrix}\n",
        " +\n",
        "  \\begin{bmatrix}\n",
        "  1 \\\\\n",
        "  0 \\\\\n",
        "  1 \\\\\n",
        "  1\n",
        " \\end{bmatrix}\n",
        " \\odot\n",
        "  \\begin{bmatrix}\n",
        "  8 \\\\\n",
        "  5 \\\\\n",
        "  3 \\\\\n",
        "  7\n",
        " \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Чтобы добиться дифференцируемости, будем использовать сигмоиду: $\\sigma(f(x_t, h_{t-1}))$.\n",
        "\n",
        "В результате сеть будет сама, глядя на входы, решать, какие ячейки своей памяти и насколько стоит перезаписывать.\n",
        "\n",
        "### LSTM\n",
        "\n",
        "Кажется, первой архитектурой, применившей данной механизм, стал LSTM (Long Short-Term Memory).\n",
        "\n",
        "В ней у нас к $h_{t-1}$ добавляется ещё и $c_{t-1}$: $h_{t-1}$ - это всё то же скрытое состояния полученное на предыдущем шаге, а $c_{t-1}$ - это вектор памяти.\n",
        "\n",
        "Схематично - как-то так:\n",
        "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png =x250)  \n",
        "*From [(Understanding LSTM Networks)](http://colah.github.io/posts/2015-08-Understanding-LSTMs)*\n",
        "\n",
        "\n",
        "Для начала мы можем точно так же, как и раньше посчитать новое скрытое состояние (обозначим его $\\tilde c_{t}$):\n",
        "$$\\tilde c_{t} = tanh(W_h [h_{t-1}; x_t] + b_h)$$\n",
        "\n",
        "В обычных RNN мы бы просто перезаписали этим значением сторое скрытое состояние. А теперь мы хотим понять, насколько нам нужна информация из $c_{t-1}$ и из $\\tilde c_{t}$. \n",
        "\n",
        "Оценим её сигмоидами:\n",
        "$$f = \\sigma(W_f [h_{t-1}; x_t] + b_f),$$\n",
        "$$i = \\sigma(W_i [h_{t-1}; x_t] + b_i).$$\n",
        "\n",
        "Первая - про то, насколько хочется забыть старую информацию. Вторая - насколько интересна новая. Тогда\n",
        "$$c_t = f \\odot c_{t-1} + i \\odot \\tilde c_t.$$\n",
        "\n",
        "Новое скрытое состояние мы также взвесим:\n",
        "$$o = \\sigma(W_o [h_{t-1}; x_t] + b_o),$$\n",
        "$$h_t = o \\odot tanh(c_t).$$\n",
        "\n",
        "Еще одна картинка:\n",
        "![](https://image.ibb.co/e6HQUU/details.png)  \n",
        "*From [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/)*\n",
        "\n",
        "Почему проблема затухающих градиентов решается? Потому что посмотрите на производную $\\frac{\\partial c_t}{\\partial c_{t-1}}$. Она пропорциональна гейту $f$. Если $f=1$ - градиенты текут без изменений. Иначе - ну, сеть сама учится, когда ей хочется что-то забыть.\n",
        "\n",
        "Настоятельно рекомендуется почитать статью: [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) для более подробного ознакомления и прикольных картинок.\n",
        "\n",
        "Зачем я выписал эти формулы? Главное - чтобы показать, насколько больше параметров нужно учить в LSTM по сравнению с обычным RNN. В четыре раза больше!\n",
        "\n",
        "Для тех, кто заснул - [видео, как забывает RNN (нижняя часть)](https://www.youtube.com/watch?v=mLxsbWAYIpw)"
      ]
    },
    {
      "metadata": {
        "id": "_9KWgbwQMatn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных"
      ]
    },
    {
      "metadata": {
        "id": "q7-fQPwJUKtV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "symbols = set(symb for word in data_train for symb in word)\n",
        "char2ind = {symb: ind + 1 for ind, symb in enumerate(symbols)}\n",
        "char2ind[''] = 0\n",
        "\n",
        "lang2ind = {lang: ind for ind, lang in enumerate(set(labels_train))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dcFgEy7YeFw0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сконвертируем датасет.\n",
        "\n",
        "**Задание** Напишите генератор батчей, который будет на лету выбирать случайный набор слов и конвертировать их в матрицы."
      ]
    },
    {
      "metadata": {
        "id": "BWWClVsTRVuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iterate_batches(data, labels, char2ind, lang2ind, batch_size):\n",
        "    # let's do the conversion part first\n",
        "    labels = np.array([lang2ind[label] for label in labels])\n",
        "    data = [[char2ind.get(symb, 0) for symb in word] for word in data]\n",
        "    \n",
        "    indices = np.arange(len(data))\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, len(data), batch_size):\n",
        "        end = min(start + batch_size, len(data))\n",
        "        \n",
        "        batch_indices = indices[start: end]\n",
        "        \n",
        "        max_word_len = max(len(data[ind]) for ind in batch_indices)\n",
        "        X = np.zeros((max_word_len, len(batch_indices)))\n",
        "        for batch_ind, data_ind in enumerate(batch_indices):\n",
        "            X[:len(data[data_ind]), batch_ind] = data[data_ind]\n",
        "            \n",
        "        yield X, labels[batch_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CfeR4B_hbH9P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Лень передавать `char2ind, lang2ind`:"
      ]
    },
    {
      "metadata": {
        "id": "uA-_jRNdaCM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "iterate_batches = partial(iterate_batches, char2ind=char2ind, lang2ind=lang2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HD5i7WmTVlGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "95758ead-3561-4009-f5db-bb29c15cadc9"
      },
      "cell_type": "code",
      "source": [
        "next(iterate_batches(data, labels, batch_size=8))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[56., 56., 68., 56.,  6., 56., 32., 55.],\n",
              "        [29., 39.,  1., 41., 57., 10., 50., 50.],\n",
              "        [82., 16., 57., 71., 39., 57., 85., 61.],\n",
              "        [57.,  1., 25., 39., 39., 82., 44.,  4.],\n",
              "        [ 0.,  0., 25.,  1., 50.,  0., 46., 59.],\n",
              "        [ 0.,  0., 57., 29.,  0.,  0., 71., 50.],\n",
              "        [ 0.,  0., 46., 25.,  0.,  0., 25., 39.],\n",
              "        [ 0.,  0.,  0.,  0.,  0.,  0., 57.,  0.],\n",
              "        [ 0.,  0.,  0.,  0.,  0.,  0., 38.,  0.],\n",
              "        [ 0.,  0.,  0.,  0.,  0.,  0., 50.,  0.],\n",
              "        [ 0.,  0.,  0.,  0.,  0.,  0., 61.,  0.]]),\n",
              " array([12,  2,  1,  4,  6,  1,  9,  1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "dnkAUetgs6Tr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте простую модель на `SimpleRnn`."
      ]
    },
    {
      "metadata": {
        "id": "OkpAYNEX_5Et",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*На семинаре мы написали сразу на `LSTM`, но я решил написать как в задании.* "
      ]
    },
    {
      "metadata": {
        "id": "Zk3OSidVS_px",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnamesClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, lstm_hidden_dim, classes_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._rnn = SimpleRNN(emb_dim, lstm_hidden_dim)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, classes_count)\n",
        "            \n",
        "    def forward(self, inputs):\n",
        "        'embed(inputs) -> prediction'\n",
        "        return self._out_layer(self.embed(inputs))\n",
        "    \n",
        "    def embed(self, inputs):\n",
        "        'inputs -> word embedding'\n",
        "        return self._rnn(self._emb(inputs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vXN-QIrZs95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None):  \n",
        "    epoch_loss = 0.\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    model.train(is_train)\n",
        "    \n",
        "    data, labels = data\n",
        "    batchs_count = math.ceil(len(data) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, labels, batch_size=batch_size)):\n",
        "            X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if is_train:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "                optimizer.step()\n",
        "\n",
        "            print('\\r[{} / {}]: Loss = {:.4f}'.format(i, batchs_count, loss.item()), end='')\n",
        "                \n",
        "    return epoch_loss / batchs_count\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
        "        batch_size=32, val_data=None, val_batch_size=None):\n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        start_time = time.time()\n",
        "        train_loss = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
        "        \n",
        "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}'\n",
        "        if not val_data is None:\n",
        "            val_loss = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            output_info += ', Val Loss = {:.4f}'\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, val_loss))\n",
        "        else:\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9vBDF2gbypR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "fdf836fc-63b6-4823-92de-de3103f59f1f"
      },
      "cell_type": "code",
      "source": [
        "model = SurnamesClassifier(vocab_size=len(char2ind), emb_dim=16, lstm_hidden_dim=64, classes_count=len(lang2ind)).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, epochs_count=50, batch_size=128, train_data=(data_train, labels_train),\n",
        "    val_data=(data_test, labels_test), val_batch_size=512)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 50, Epoch Time = 2.61s: Train Loss = 1.9422, Val Loss = 1.8447\n",
            "Epoch 2 / 50, Epoch Time = 2.43s: Train Loss = 1.8432, Val Loss = 1.8254\n",
            "Epoch 3 / 50, Epoch Time = 2.40s: Train Loss = 1.7454, Val Loss = 1.8062\n",
            "Epoch 4 / 50, Epoch Time = 2.41s: Train Loss = 1.5798, Val Loss = 1.5305\n",
            "Epoch 5 / 50, Epoch Time = 2.49s: Train Loss = 1.4851, Val Loss = 1.4819\n",
            "Epoch 6 / 50, Epoch Time = 2.52s: Train Loss = 1.4550, Val Loss = 1.4436\n",
            "Epoch 7 / 50, Epoch Time = 2.49s: Train Loss = 1.4440, Val Loss = 1.4300\n",
            "Epoch 8 / 50, Epoch Time = 2.37s: Train Loss = 1.4141, Val Loss = 1.4182\n",
            "Epoch 9 / 50, Epoch Time = 2.33s: Train Loss = 1.3763, Val Loss = 1.3513\n",
            "Epoch 10 / 50, Epoch Time = 2.33s: Train Loss = 1.3118, Val Loss = 1.3010\n",
            "Epoch 11 / 50, Epoch Time = 2.39s: Train Loss = 1.2743, Val Loss = 1.2681\n",
            "Epoch 12 / 50, Epoch Time = 2.34s: Train Loss = 1.2505, Val Loss = 1.2329\n",
            "Epoch 13 / 50, Epoch Time = 2.34s: Train Loss = 1.2191, Val Loss = 1.2506\n",
            "Epoch 14 / 50, Epoch Time = 2.32s: Train Loss = 1.2032, Val Loss = 1.2470\n",
            "Epoch 15 / 50, Epoch Time = 2.35s: Train Loss = 1.1908, Val Loss = 1.2256\n",
            "Epoch 16 / 50, Epoch Time = 2.42s: Train Loss = 1.1696, Val Loss = 1.1760\n",
            "Epoch 17 / 50, Epoch Time = 2.33s: Train Loss = 1.1586, Val Loss = 1.1933\n",
            "Epoch 18 / 50, Epoch Time = 2.35s: Train Loss = 1.1366, Val Loss = 1.1585\n",
            "Epoch 19 / 50, Epoch Time = 2.38s: Train Loss = 1.1197, Val Loss = 1.1908\n",
            "Epoch 20 / 50, Epoch Time = 2.43s: Train Loss = 1.0978, Val Loss = 1.1433\n",
            "Epoch 21 / 50, Epoch Time = 2.35s: Train Loss = 1.0902, Val Loss = 1.1326\n",
            "Epoch 22 / 50, Epoch Time = 2.36s: Train Loss = 1.0612, Val Loss = 1.1109\n",
            "Epoch 23 / 50, Epoch Time = 2.32s: Train Loss = 1.0363, Val Loss = 1.0888\n",
            "Epoch 24 / 50, Epoch Time = 2.28s: Train Loss = 1.0327, Val Loss = 1.1371\n",
            "Epoch 25 / 50, Epoch Time = 2.38s: Train Loss = 1.0126, Val Loss = 1.0887\n",
            "Epoch 26 / 50, Epoch Time = 2.34s: Train Loss = 0.9937, Val Loss = 1.0927\n",
            "Epoch 27 / 50, Epoch Time = 2.33s: Train Loss = 0.9792, Val Loss = 1.0612\n",
            "Epoch 28 / 50, Epoch Time = 2.30s: Train Loss = 0.9628, Val Loss = 1.1520\n",
            "Epoch 29 / 50, Epoch Time = 2.31s: Train Loss = 0.9488, Val Loss = 1.0777\n",
            "Epoch 30 / 50, Epoch Time = 2.41s: Train Loss = 0.9316, Val Loss = 1.0150\n",
            "Epoch 31 / 50, Epoch Time = 2.36s: Train Loss = 0.9176, Val Loss = 1.0320\n",
            "Epoch 32 / 50, Epoch Time = 2.29s: Train Loss = 0.9080, Val Loss = 0.9863\n",
            "Epoch 33 / 50, Epoch Time = 2.35s: Train Loss = 0.8894, Val Loss = 1.0322\n",
            "Epoch 34 / 50, Epoch Time = 2.33s: Train Loss = 0.8698, Val Loss = 0.9944\n",
            "Epoch 35 / 50, Epoch Time = 2.39s: Train Loss = 0.8544, Val Loss = 0.9671\n",
            "Epoch 36 / 50, Epoch Time = 2.30s: Train Loss = 0.8578, Val Loss = 0.9735\n",
            "Epoch 37 / 50, Epoch Time = 2.29s: Train Loss = 0.8348, Val Loss = 1.0220\n",
            "Epoch 38 / 50, Epoch Time = 2.26s: Train Loss = 0.8297, Val Loss = 0.9737\n",
            "Epoch 39 / 50, Epoch Time = 2.34s: Train Loss = 0.8326, Val Loss = 0.9662\n",
            "Epoch 40 / 50, Epoch Time = 2.28s: Train Loss = 0.8144, Val Loss = 0.9643\n",
            "Epoch 41 / 50, Epoch Time = 2.28s: Train Loss = 0.8002, Val Loss = 0.9689\n",
            "Epoch 42 / 50, Epoch Time = 2.34s: Train Loss = 0.7979, Val Loss = 0.9398\n",
            "Epoch 43 / 50, Epoch Time = 2.36s: Train Loss = 0.7817, Val Loss = 0.9366\n",
            "Epoch 44 / 50, Epoch Time = 2.53s: Train Loss = 0.7785, Val Loss = 0.9594\n",
            "Epoch 45 / 50, Epoch Time = 2.47s: Train Loss = 0.7752, Val Loss = 0.9437\n",
            "Epoch 46 / 50, Epoch Time = 2.45s: Train Loss = 0.7714, Val Loss = 0.9928\n",
            "Epoch 47 / 50, Epoch Time = 2.45s: Train Loss = 0.7711, Val Loss = 0.9818\n",
            "Epoch 48 / 50, Epoch Time = 2.46s: Train Loss = 0.7569, Val Loss = 0.9608\n",
            "Epoch 49 / 50, Epoch Time = 2.51s: Train Loss = 0.7487, Val Loss = 0.9291\n",
            "Epoch 50 / 50, Epoch Time = 2.52s: Train Loss = 0.7470, Val Loss = 0.9584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jC76XyGjigFx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Напишите функцию для тестирования полученной сети: пусть она принимает слово и говорит, в каком языке с какой вероятностью это может быть фамилией."
      ]
    },
    {
      "metadata": {
        "id": "clJrRQv-_6P7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def surname_proba(surname):\n",
        "    model.eval()\n",
        "    data = LongTensor([[char2ind.get(symb, 0) for symb in surname]]).view(-1, 1)\n",
        "    proba = torch.softmax(model(data), 1)\n",
        "    \n",
        "    print('\\tPROBABILITIES')\n",
        "    for lang in lang2ind:\n",
        "        print('{0}: {1:.2f}'.format(lang, proba[0][lang2ind[lang]].item()))\n",
        "    \n",
        "    pred = proba.argmax().item()\n",
        "    pred_lang = 'Elven'\n",
        "    for lang in lang2ind:\n",
        "        if lang2ind[lang] == pred:\n",
        "            pred_lang = lang\n",
        "    print('\\tPREDICTION')\n",
        "    print('{0}: {1:.2f}'.format(pred_lang, proba[0][pred].item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifru6B-gDyQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "38670e63-9153-4e0e-d983-e9399f2a8d92"
      },
      "cell_type": "code",
      "source": [
        "surname_proba('Mendeleev')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPROBABILITIES\n",
            "Chinese: 0.00\n",
            "Arabic: 0.00\n",
            "Dutch: 0.01\n",
            "German: 0.02\n",
            "Russian: 0.93\n",
            "Irish: 0.00\n",
            "Spanish: 0.00\n",
            "Italian: 0.00\n",
            "Portuguese: 0.00\n",
            "Greek: 0.00\n",
            "Vietnamese: 0.00\n",
            "French: 0.00\n",
            "Japanese: 0.00\n",
            "Polish: 0.00\n",
            "Korean: 0.00\n",
            "English: 0.01\n",
            "Czech: 0.01\n",
            "Scottish: 0.00\n",
            "\tPREDICTION\n",
            "Russian: 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vlcMr8pHFHjt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1bdd50e6-e86a-4564-c3df-c9bf8ed19118"
      },
      "cell_type": "code",
      "source": [
        "surname_proba('Kennedy')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPROBABILITIES\n",
            "Chinese: 0.00\n",
            "Arabic: 0.00\n",
            "Dutch: 0.04\n",
            "German: 0.07\n",
            "Russian: 0.18\n",
            "Irish: 0.12\n",
            "Spanish: 0.03\n",
            "Italian: 0.03\n",
            "Portuguese: 0.01\n",
            "Greek: 0.01\n",
            "Vietnamese: 0.00\n",
            "French: 0.04\n",
            "Japanese: 0.00\n",
            "Polish: 0.00\n",
            "Korean: 0.00\n",
            "English: 0.41\n",
            "Czech: 0.01\n",
            "Scottish: 0.04\n",
            "\tPREDICTION\n",
            "English: 0.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVBhwEyxFuyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5d0873f0-9c26-4358-bbc2-e2ece198c181"
      },
      "cell_type": "code",
      "source": [
        "surname_proba('Kontos')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tPROBABILITIES\n",
            "Chinese: 0.00\n",
            "Arabic: 0.11\n",
            "Dutch: 0.00\n",
            "German: 0.00\n",
            "Russian: 0.01\n",
            "Irish: 0.01\n",
            "Spanish: 0.03\n",
            "Italian: 0.10\n",
            "Portuguese: 0.04\n",
            "Greek: 0.60\n",
            "Vietnamese: 0.00\n",
            "French: 0.00\n",
            "Japanese: 0.08\n",
            "Polish: 0.01\n",
            "Korean: 0.00\n",
            "English: 0.00\n",
            "Czech: 0.01\n",
            "Scottish: 0.01\n",
            "\tPREDICTION\n",
            "Greek: 0.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aWqOPgdbIYcl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Оцените качество модели."
      ]
    },
    {
      "metadata": {
        "id": "dT2QE6IycXo9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "d67483ab-d8bb-493e-ca23-8ee04b3617ac"
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "X_test, y_test = next(iterate_batches(data_test, labels_test, batch_size=len(data_test)))\n",
        "X_test = LongTensor(X_test)\n",
        "y_pred = torch.softmax(model(X_test), 1).argmax(1)\n",
        "\n",
        "print('Accuracy = {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test, y_pred, \n",
        "                            target_names=[lang for lang, _ in sorted(lang2ind.items(), key=lambda x: x[1])]))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 73.55%\n",
            "Classification report:\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    Chinese       0.27      0.81      0.41        80\n",
            "     Arabic       0.94      0.93      0.94       600\n",
            "      Dutch       0.00      0.00      0.00        89\n",
            "     German       0.17      0.02      0.04       217\n",
            "    Russian       0.80      0.95      0.87      2823\n",
            "      Irish       0.00      0.00      0.00        70\n",
            "    Spanish       0.00      0.00      0.00        89\n",
            "    Italian       0.73      0.37      0.49       213\n",
            " Portuguese       0.00      0.00      0.00        22\n",
            "      Greek       1.00      0.02      0.03        61\n",
            " Vietnamese       0.00      0.00      0.00        22\n",
            "     French       0.00      0.00      0.00        83\n",
            "   Japanese       0.79      0.57      0.66       297\n",
            "     Polish       0.00      0.00      0.00        42\n",
            "     Korean       0.00      0.00      0.00        28\n",
            "    English       0.59      0.79      0.68      1101\n",
            "      Czech       0.00      0.00      0.00       156\n",
            "   Scottish       0.00      0.00      0.00        30\n",
            "\n",
            "avg / total       0.66      0.74      0.68      6023\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "B_FZ9x0NInft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Визуализация эмбеддингов"
      ]
    },
    {
      "metadata": {
        "id": "bJkyBV2bAK05",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.colors import RGB\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "    \n",
        "    if isinstance(color, str): \n",
        "        color = [color] * len(x)\n",
        "    if isinstance(color, np.ndarray):\n",
        "        color = [RGB(*x[:3]) for x in color]\n",
        "    print(color)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: \n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=100)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "    \n",
        "    \n",
        "def visualize_embeddings(embeddings, token, colors):\n",
        "    tsne = get_tsne_projection(embeddings)\n",
        "    draw_vectors(tsne[:, 0], tsne[:, 1], color=colors, token=token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1u-74cv7IrH9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы опять получили эмбеддинги - символьного уровня теперь.\n",
        "\n",
        "Хочется на них посмотреть\n",
        "\n",
        "**Задание** Посчитайте векторы для случайных слов и выведите их."
      ]
    },
    {
      "metadata": {
        "id": "jt6LsI0NAPAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1028
        },
        "outputId": "f49471e2-b501-4302-9e9c-efeff8f413fc"
      },
      "cell_type": "code",
      "source": [
        "word_indices = np.random.choice(np.arange(len(data_test)), 1000, replace=False)\n",
        "words = [data_test[ind] for ind in word_indices]\n",
        "word_labels = [labels_test[ind] for ind in word_indices]\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_batch, y_batch = next(iterate_batches(words, word_labels, batch_size=1000))\n",
        "    embeddings = model.embed(LongTensor(X_batch))\n",
        "\n",
        "colors = plt.cm.tab20(y_batch) * 255\n",
        "\n",
        "visualize_embeddings(embeddings, words, colors)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 1000 samples in 0.003s...\n",
            "[t-SNE] Computed neighbors for 1000 samples in 0.071s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 1000\n",
            "[t-SNE] Mean sigma: 0.026489\n",
            "[t-SNE] Computed conditional probabilities in 0.059s\n",
            "[t-SNE] Iteration 50: error = 60.5104408, gradient norm = 0.2541550 (50 iterations in 2.563s)\n",
            "[t-SNE] Iteration 100: error = 55.4744492, gradient norm = 0.1813614 (50 iterations in 1.954s)\n",
            "[t-SNE] Iteration 150: error = 54.6874352, gradient norm = 0.1861556 (50 iterations in 1.955s)\n",
            "[t-SNE] Iteration 200: error = 54.3285980, gradient norm = 0.1699236 (50 iterations in 1.967s)\n",
            "[t-SNE] Iteration 250: error = 54.1519890, gradient norm = 0.1721014 (50 iterations in 2.264s)\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 54.151989\n",
            "[t-SNE] Iteration 300: error = 0.4694872, gradient norm = 0.0006485 (50 iterations in 2.114s)\n",
            "[t-SNE] Iteration 350: error = 0.3920695, gradient norm = 0.0002957 (50 iterations in 1.896s)\n",
            "[t-SNE] Iteration 400: error = 0.3765194, gradient norm = 0.0002196 (50 iterations in 1.934s)\n",
            "[t-SNE] Iteration 450: error = 0.3695682, gradient norm = 0.0001750 (50 iterations in 1.851s)\n",
            "[t-SNE] Iteration 500: error = 0.3654631, gradient norm = 0.0001600 (50 iterations in 1.805s)\n",
            "[t-SNE] Iteration 550: error = 0.3620461, gradient norm = 0.0001549 (50 iterations in 1.793s)\n",
            "[t-SNE] Iteration 600: error = 0.3607906, gradient norm = 0.0001126 (50 iterations in 1.848s)\n",
            "[t-SNE] Iteration 650: error = 0.3589132, gradient norm = 0.0001200 (50 iterations in 1.755s)\n",
            "[t-SNE] Iteration 700: error = 0.3574842, gradient norm = 0.0001123 (50 iterations in 1.816s)\n",
            "[t-SNE] Iteration 750: error = 0.3562124, gradient norm = 0.0001405 (50 iterations in 1.759s)\n",
            "[t-SNE] Iteration 800: error = 0.3547481, gradient norm = 0.0001077 (50 iterations in 1.780s)\n",
            "[t-SNE] Iteration 850: error = 0.3537571, gradient norm = 0.0001082 (50 iterations in 1.874s)\n",
            "[t-SNE] Iteration 900: error = 0.3537666, gradient norm = 0.0001018 (50 iterations in 1.831s)\n",
            "[t-SNE] Iteration 950: error = 0.3522790, gradient norm = 0.0000857 (50 iterations in 1.801s)\n",
            "[t-SNE] Iteration 1000: error = 0.3515106, gradient norm = 0.0000896 (50 iterations in 1.776s)\n",
            "[t-SNE] Error after 1000 iterations: 0.351511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
            "  warnings.warn(\"Numerical issues were encountered \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div class=\"bk-root\">\n",
              "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
              "        <span id=\"8c85547d-f51a-45e4-8a8a-d9c08418691b\">Loading BokehJS ...</span>\n",
              "    </div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  var JS_MIME_TYPE = 'application/javascript';\n",
              "  var HTML_MIME_TYPE = 'text/html';\n",
              "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    var script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    var cell = handle.cell;\n",
              "\n",
              "    var id = cell.output_area._bokeh_element_id;\n",
              "    var server_id = cell.output_area._bokeh_server_id;\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null && id in Bokeh.index) {\n",
              "      Bokeh.index[id].model.document.clear();\n",
              "      delete Bokeh.index[id];\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            var id = msg.content.text.trim();\n",
              "            if (id in Bokeh.index) {\n",
              "              Bokeh.index[id].model.document.clear();\n",
              "              delete Bokeh.index[id];\n",
              "            }\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    var output_area = handle.output_area;\n",
              "    var output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      var bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      var script_attrs = bk_div.children[0].attributes;\n",
              "      for (var i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      var toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  \n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    var el = document.getElementById(\"8c85547d-f51a-45e4-8a8a-d9c08418691b\");\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
              "    }\n",
              "    finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.info(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(js_urls, callback) {\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = js_urls.length;\n",
              "    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      var s = document.createElement('script');\n",
              "      s.src = url;\n",
              "      s.async = false;\n",
              "      s.onreadystatechange = s.onload = function() {\n",
              "        root._bokeh_is_loading--;\n",
              "        if (root._bokeh_is_loading === 0) {\n",
              "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
              "          run_callbacks()\n",
              "        }\n",
              "      };\n",
              "      s.onerror = function() {\n",
              "        console.warn(\"failed to load library \" + url);\n",
              "      };\n",
              "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "    }\n",
              "  };var element = document.getElementById(\"8c85547d-f51a-45e4-8a8a-d9c08418691b\");\n",
              "  if (element == null) {\n",
              "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '8c85547d-f51a-45e4-8a8a-d9c08418691b' but no matching script tag was found. \")\n",
              "    return false;\n",
              "  }\n",
              "\n",
              "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n",
              "\n",
              "  var inline_js = [\n",
              "    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "    \n",
              "    function(Bokeh) {\n",
              "      \n",
              "    },\n",
              "    function(Bokeh) {\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    \n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (var i = 0; i < inline_js.length; i++) {\n",
              "        inline_js[i].call(root, root.Bokeh);\n",
              "      }if (force === true) {\n",
              "        display_loaded();\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      var cell = $(document.getElementById(\"8c85547d-f51a-45e4-8a8a-d9c08418691b\")).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(js_urls, function() {\n",
              "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"8c85547d-f51a-45e4-8a8a-d9c08418691b\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"8c85547d-f51a-45e4-8a8a-d9c08418691b\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '8c85547d-f51a-45e4-8a8a-d9c08418691b' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"8c85547d-f51a-45e4-8a8a-d9c08418691b\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[rgb(44, 160, 44), rgb(174, 199, 232), rgb(188, 189, 34), rgb(255, 152, 150), rgb(44, 160, 44), rgb(44, 160, 44), rgb(214, 39, 40), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 127, 14), rgb(44, 160, 44), rgb(174, 199, 232), rgb(255, 127, 14), rgb(44, 160, 44), rgb(255, 187, 120), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(214, 39, 40), rgb(31, 119, 180), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(31, 119, 180), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(214, 39, 40), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(188, 189, 34), rgb(44, 160, 44), rgb(44, 160, 44), rgb(214, 39, 40), rgb(188, 189, 34), rgb(199, 199, 199), rgb(44, 160, 44), rgb(227, 119, 194), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(31, 119, 180), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(255, 187, 120), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 187, 120), rgb(199, 199, 199), rgb(44, 160, 44), rgb(255, 152, 150), rgb(199, 199, 199), rgb(174, 199, 232), rgb(152, 223, 138), rgb(44, 160, 44), rgb(44, 160, 44), rgb(31, 119, 180), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(227, 119, 194), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(214, 39, 40), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(174, 199, 232), rgb(255, 152, 150), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(255, 152, 150), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(255, 127, 14), rgb(199, 199, 199), rgb(199, 199, 199), rgb(197, 176, 213), rgb(44, 160, 44), rgb(174, 199, 232), rgb(255, 187, 120), rgb(44, 160, 44), rgb(44, 160, 44), rgb(148, 103, 189), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(196, 156, 148), rgb(255, 127, 14), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 152, 150), rgb(199, 199, 199), rgb(174, 199, 232), rgb(255, 187, 120), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(199, 199, 199), rgb(255, 187, 120), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(31, 119, 180), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(255, 187, 120), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(31, 119, 180), rgb(199, 199, 199), rgb(227, 119, 194), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(188, 189, 34), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(174, 199, 232), rgb(255, 187, 120), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(188, 189, 34), rgb(44, 160, 44), rgb(255, 152, 150), rgb(174, 199, 232), rgb(174, 199, 232), rgb(197, 176, 213), rgb(44, 160, 44), rgb(255, 127, 14), rgb(44, 160, 44), rgb(44, 160, 44), rgb(197, 176, 213), rgb(219, 219, 141), rgb(44, 160, 44), rgb(255, 152, 150), rgb(199, 199, 199), rgb(199, 199, 199), rgb(174, 199, 232), rgb(199, 199, 199), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(255, 187, 120), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(214, 39, 40), rgb(44, 160, 44), rgb(199, 199, 199), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 152, 150), rgb(197, 176, 213), rgb(44, 160, 44), rgb(174, 199, 232), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(140, 86, 75), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 187, 120), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(196, 156, 148), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(255, 152, 150), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(227, 119, 194), rgb(199, 199, 199), rgb(44, 160, 44), rgb(255, 127, 14), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(255, 187, 120), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(199, 199, 199), rgb(44, 160, 44), rgb(31, 119, 180), rgb(199, 199, 199), rgb(44, 160, 44), rgb(31, 119, 180), rgb(31, 119, 180), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(31, 119, 180), rgb(152, 223, 138), rgb(199, 199, 199), rgb(174, 199, 232), rgb(255, 127, 14), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(140, 86, 75), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(188, 189, 34), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(31, 119, 180), rgb(31, 119, 180), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(31, 119, 180), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(199, 199, 199), rgb(255, 187, 120), rgb(44, 160, 44), rgb(188, 189, 34), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(188, 189, 34), rgb(196, 156, 148), rgb(44, 160, 44), rgb(255, 152, 150), rgb(196, 156, 148), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 152, 150), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(127, 127, 127), rgb(174, 199, 232), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(255, 187, 120), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(148, 103, 189), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 152, 150), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(255, 152, 150), rgb(255, 187, 120), rgb(44, 160, 44), rgb(188, 189, 34), rgb(255, 187, 120), rgb(199, 199, 199), rgb(199, 199, 199), rgb(196, 156, 148), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(174, 199, 232), rgb(197, 176, 213), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(188, 189, 34), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(188, 189, 34), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 127, 14), rgb(255, 187, 120), rgb(199, 199, 199), rgb(255, 152, 150), rgb(174, 199, 232), rgb(214, 39, 40), rgb(199, 199, 199), rgb(44, 160, 44), rgb(214, 39, 40), rgb(127, 127, 127), rgb(199, 199, 199), rgb(174, 199, 232), rgb(255, 187, 120), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(199, 199, 199), rgb(255, 187, 120), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(255, 187, 120), rgb(174, 199, 232), rgb(174, 199, 232), rgb(255, 152, 150), rgb(199, 199, 199), rgb(174, 199, 232), rgb(199, 199, 199), rgb(174, 199, 232), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(199, 199, 199), rgb(247, 182, 210), rgb(44, 160, 44), rgb(199, 199, 199), rgb(255, 127, 14), rgb(255, 187, 120), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(214, 39, 40), rgb(227, 119, 194), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 152, 150), rgb(255, 187, 120), rgb(227, 119, 194), rgb(44, 160, 44), rgb(197, 176, 213), rgb(44, 160, 44), rgb(255, 127, 14), rgb(227, 119, 194), rgb(255, 152, 150), rgb(255, 187, 120), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(255, 152, 150), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(152, 223, 138), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(174, 199, 232), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(255, 187, 120), rgb(255, 187, 120), rgb(199, 199, 199), rgb(174, 199, 232), rgb(199, 199, 199), rgb(174, 199, 232), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(255, 187, 120), rgb(196, 156, 148), rgb(44, 160, 44), rgb(44, 160, 44), rgb(188, 189, 34), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(227, 119, 194), rgb(174, 199, 232), rgb(255, 152, 150), rgb(255, 152, 150), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 152, 150), rgb(174, 199, 232), rgb(255, 187, 120), rgb(44, 160, 44), rgb(214, 39, 40), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(174, 199, 232), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 187, 120), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(196, 156, 148), rgb(140, 86, 75), rgb(188, 189, 34), rgb(199, 199, 199), rgb(227, 119, 194), rgb(255, 152, 150), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 187, 120), rgb(199, 199, 199), rgb(227, 119, 194), rgb(255, 127, 14), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(31, 119, 180), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(174, 199, 232), rgb(196, 156, 148), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(255, 187, 120), rgb(188, 189, 34), rgb(44, 160, 44), rgb(255, 152, 150), rgb(199, 199, 199), rgb(255, 152, 150), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(31, 119, 180), rgb(199, 199, 199), rgb(44, 160, 44), rgb(188, 189, 34), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 152, 150), rgb(199, 199, 199), rgb(188, 189, 34), rgb(227, 119, 194), rgb(174, 199, 232), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(188, 189, 34), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(196, 156, 148), rgb(255, 152, 150), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(196, 156, 148), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(174, 199, 232), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(188, 189, 34), rgb(255, 187, 120), rgb(227, 119, 194), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(255, 127, 14), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(255, 152, 150), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(196, 156, 148), rgb(227, 119, 194), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(199, 199, 199), rgb(255, 152, 150), rgb(174, 199, 232), rgb(199, 199, 199), rgb(174, 199, 232), rgb(174, 199, 232), rgb(44, 160, 44), rgb(188, 189, 34), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(214, 39, 40), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(31, 119, 180), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(247, 182, 210), rgb(174, 199, 232), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(188, 189, 34), rgb(214, 39, 40), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(196, 156, 148), rgb(227, 119, 194), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(255, 152, 150), rgb(199, 199, 199), rgb(199, 199, 199), rgb(152, 223, 138), rgb(174, 199, 232), rgb(255, 187, 120), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 187, 120), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(199, 199, 199), rgb(44, 160, 44), rgb(227, 119, 194), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(214, 39, 40), rgb(44, 160, 44), rgb(219, 219, 141), rgb(219, 219, 141), rgb(44, 160, 44), rgb(31, 119, 180), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(255, 152, 150), rgb(174, 199, 232), rgb(44, 160, 44), rgb(227, 119, 194), rgb(174, 199, 232), rgb(44, 160, 44), rgb(197, 176, 213), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 127, 14), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 127, 14), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(199, 199, 199), rgb(255, 127, 14), rgb(174, 199, 232), rgb(255, 152, 150), rgb(227, 119, 194), rgb(199, 199, 199), rgb(199, 199, 199), rgb(174, 199, 232), rgb(199, 199, 199), rgb(199, 199, 199), rgb(199, 199, 199), rgb(44, 160, 44), rgb(152, 223, 138), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(188, 189, 34), rgb(44, 160, 44), rgb(227, 119, 194), rgb(227, 119, 194), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(197, 176, 213), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(255, 187, 120), rgb(44, 160, 44), rgb(174, 199, 232), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(188, 189, 34), rgb(255, 152, 150), rgb(44, 160, 44), rgb(255, 187, 120), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(199, 199, 199), rgb(199, 199, 199), rgb(199, 199, 199), rgb(227, 119, 194), rgb(199, 199, 199), rgb(152, 223, 138), rgb(199, 199, 199), rgb(255, 152, 150), rgb(174, 199, 232), rgb(44, 160, 44), rgb(214, 39, 40), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(199, 199, 199), rgb(174, 199, 232), rgb(44, 160, 44), rgb(44, 160, 44), rgb(199, 199, 199), rgb(199, 199, 199), rgb(174, 199, 232), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(174, 199, 232), rgb(197, 176, 213), rgb(44, 160, 44), rgb(199, 199, 199), rgb(174, 199, 232), rgb(255, 152, 150), rgb(44, 160, 44), rgb(255, 187, 120), rgb(44, 160, 44), rgb(255, 152, 150), rgb(44, 160, 44), rgb(174, 199, 232), rgb(174, 199, 232), rgb(199, 199, 199), rgb(174, 199, 232), rgb(199, 199, 199), rgb(44, 160, 44), rgb(44, 160, 44), rgb(227, 119, 194), rgb(44, 160, 44), rgb(174, 199, 232), rgb(152, 223, 138), rgb(44, 160, 44), rgb(227, 119, 194), rgb(247, 182, 210), rgb(199, 199, 199), rgb(152, 223, 138), rgb(247, 182, 210)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div class=\"bk-root\" id=\"88bca0f8-d975-4259-ab29-879554d2fca0\"></div>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "    \n",
              "  var docs_json = {\"f6e473b1-c45b-4c97-ac52-2032e282ef8a\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"675974ad-f7f4-42e2-8526-8c18bdc1c63d\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"20bfb417-860d-43d3-9bf6-c632e95a96ab\",\"type\":\"HelpTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.25},\"fill_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.25},\"line_color\":{\"field\":\"color\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"cafa586d-f930-4d57-bb8a-98108533b265\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"8a198caa-58bc-45ab-96fb-d01f90102f0e\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"cafa586d-f930-4d57-bb8a-98108533b265\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"c1de0117-b777-4c02-9527-83934ba5a517\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"214cc4ef-3387-461a-8df8-3f2010c56095\",\"type\":\"CDSView\"}},\"id\":\"406372d1-e0bb-4db2-966d-d2f374cd86a3\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"d8936ab5-05ba-433f-a138-9b8c3fd841b2\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"formatter\":{\"id\":\"d8936ab5-05ba-433f-a138-9b8c3fd841b2\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"ab9055d0-2edf-4c57-a68c-b3b64f7e6a80\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"d6c96e52-53ba-4b94-bdea-8e2c470d1de0\",\"type\":\"BasicTicker\"}},\"id\":\"cbefe07d-800c-4a3b-adec-38a316f8749f\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"31f7ebd4-f675-435e-8d7f-e266a56e3e72\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"d6c96e52-53ba-4b94-bdea-8e2c470d1de0\",\"type\":\"BasicTicker\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"c1de0117-b777-4c02-9527-83934ba5a517\",\"type\":\"Circle\"},{\"attributes\":{\"plot\":{\"id\":\"ab9055d0-2edf-4c57-a68c-b3b64f7e6a80\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"d6c96e52-53ba-4b94-bdea-8e2c470d1de0\",\"type\":\"BasicTicker\"}},\"id\":\"7e564180-22a7-4bfa-963b-0731cc96fdcc\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"6beb4060-0812-47c5-b0fe-3b1a79962897\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"a908113a-9c56-4bc2-be4d-fad314280f33\",\"type\":\"Selection\"},{\"attributes\":{\"formatter\":{\"id\":\"31f7ebd4-f675-435e-8d7f-e266a56e3e72\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"ab9055d0-2edf-4c57-a68c-b3b64f7e6a80\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"5aec2272-7bde-493c-85b8-c68a84134fde\",\"type\":\"BasicTicker\"}},\"id\":\"02d4e978-b6d0-4df5-b27f-711c73b87b36\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"5aec2272-7bde-493c-85b8-c68a84134fde\",\"type\":\"BasicTicker\"},{\"attributes\":{\"source\":{\"id\":\"8a198caa-58bc-45ab-96fb-d01f90102f0e\",\"type\":\"ColumnDataSource\"}},\"id\":\"214cc4ef-3387-461a-8df8-3f2010c56095\",\"type\":\"CDSView\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"ab9055d0-2edf-4c57-a68c-b3b64f7e6a80\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"5aec2272-7bde-493c-85b8-c68a84134fde\",\"type\":\"BasicTicker\"}},\"id\":\"d444d535-3009-4c3d-acd8-222a7d59b12f\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"37020d50-e2be-4d1a-aa77-77a33fce1e64\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":{\"id\":\"077a8f47-8def-401a-9075-4a8eae796d5d\",\"type\":\"WheelZoomTool\"},\"active_tap\":\"auto\",\"tools\":[{\"id\":\"4ed1bbe1-4561-4703-a2ea-36bb9deac212\",\"type\":\"PanTool\"},{\"id\":\"077a8f47-8def-401a-9075-4a8eae796d5d\",\"type\":\"WheelZoomTool\"},{\"id\":\"c13a61a4-8b68-4eac-84a1-9bf47a8ded76\",\"type\":\"BoxZoomTool\"},{\"id\":\"feea1738-3611-42af-bc86-1f132ea31047\",\"type\":\"SaveTool\"},{\"id\":\"675974ad-f7f4-42e2-8526-8c18bdc1c63d\",\"type\":\"ResetTool\"},{\"id\":\"20bfb417-860d-43d3-9bf6-c632e95a96ab\",\"type\":\"HelpTool\"},{\"id\":\"af617af7-779f-44c1-9de9-76bd1f48b139\",\"type\":\"HoverTool\"}]},\"id\":\"62289067-c412-40ce-abe9-90674652c4eb\",\"type\":\"Toolbar\"},{\"attributes\":{\"callback\":null,\"data\":{\"color\":[\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(188, 189, 34)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 127, 14)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(255, 127, 14)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(31, 119, 180)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(31, 119, 180)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(188, 189, 34)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(31, 119, 180)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(152, 223, 138)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(255, 127, 14)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(197, 176, 213)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(148, 103, 189)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(196, 156, 148)\",\"rgb(255, 127, 14)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(199, 199, 199)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(197, 176, 213)\",\"rgb(44, 160, 44)\",\"rgb(255, 127, 14)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(197, 176, 213)\",\"rgb(219, 219, 141)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(255, 187, 120)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(197, 176, 213)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(140, 86, 75)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(196, 156, 148)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(255, 127, 14)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(255, 187, 120)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(31, 119, 180)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(152, 223, 138)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(255, 127, 14)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(140, 86, 75)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(31, 119, 180)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(188, 189, 34)\",\"rgb(196, 156, 148)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(196, 156, 148)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(127, 127, 127)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(148, 103, 189)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(255, 152, 150)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(255, 187, 120)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(196, 156, 148)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(197, 176, 213)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 127, 14)\",\"rgb(255, 187, 120)\",\"rgb(199, 199, 199)\",\"rgb(255, 152, 150)\",\"rgb(174, 199, 232)\",\"rgb(214, 39, 40)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(127, 127, 127)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(255, 187, 120)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(247, 182, 210)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(255, 127, 14)\",\"rgb(255, 187, 120)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(255, 187, 120)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(197, 176, 213)\",\"rgb(44, 160, 44)\",\"rgb(255, 127, 14)\",\"rgb(227, 119, 194)\",\"rgb(255, 152, 150)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(152, 223, 138)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(255, 187, 120)\",\"rgb(255, 187, 120)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(196, 156, 148)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(227, 119, 194)\",\"rgb(174, 199, 232)\",\"rgb(255, 152, 150)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(174, 199, 232)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(196, 156, 148)\",\"rgb(140, 86, 75)\",\"rgb(188, 189, 34)\",\"rgb(199, 199, 199)\",\"rgb(227, 119, 194)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(199, 199, 199)\",\"rgb(227, 119, 194)\",\"rgb(255, 127, 14)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(196, 156, 148)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(255, 187, 120)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(188, 189, 34)\",\"rgb(227, 119, 194)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(196, 156, 148)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(196, 156, 148)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(255, 187, 120)\",\"rgb(227, 119, 194)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(255, 127, 14)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(196, 156, 148)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(255, 152, 150)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(31, 119, 180)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(247, 182, 210)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(188, 189, 34)\",\"rgb(214, 39, 40)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(196, 156, 148)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(152, 223, 138)\",\"rgb(174, 199, 232)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(44, 160, 44)\",\"rgb(219, 219, 141)\",\"rgb(219, 219, 141)\",\"rgb(44, 160, 44)\",\"rgb(31, 119, 180)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(197, 176, 213)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 127, 14)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 127, 14)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(255, 127, 14)\",\"rgb(174, 199, 232)\",\"rgb(255, 152, 150)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(152, 223, 138)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(197, 176, 213)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(188, 189, 34)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(152, 223, 138)\",\"rgb(199, 199, 199)\",\"rgb(255, 152, 150)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(214, 39, 40)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(197, 176, 213)\",\"rgb(44, 160, 44)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(255, 187, 120)\",\"rgb(44, 160, 44)\",\"rgb(255, 152, 150)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(174, 199, 232)\",\"rgb(199, 199, 199)\",\"rgb(44, 160, 44)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(44, 160, 44)\",\"rgb(174, 199, 232)\",\"rgb(152, 223, 138)\",\"rgb(44, 160, 44)\",\"rgb(227, 119, 194)\",\"rgb(247, 182, 210)\",\"rgb(199, 199, 199)\",\"rgb(152, 223, 138)\",\"rgb(247, 182, 210)\"],\"token\":[\"Sugase\",\"Preece\",\"Moshnikov\",\"Zimmerman\",\"Jerebovich\",\"Aldridge\",\"Jaguzhinsky\",\"Maryltsev\",\"Maloof\",\"Bazzi\",\"Satoh\",\"Hawkins\",\"Jinov\",\"Hidirov\",\"Emmons\",\"Kouma\",\"Bishara\",\"Mokhnachev\",\"Otdelnov\",\"Awetyan\",\"Hodge\",\"Groundon\",\"Zherebin\",\"Kassab\",\"Kelly\",\"Teagan\",\"Tursky\",\"Pyrlin\",\"Zhegulin\",\"Mcgregor\",\"Mkrtchyants\",\"Jupp\",\"Badalyants\",\"Deeb\",\"Asghar\",\"Androhanov\",\"Shakhnazaryants\",\"Duncan\",\"Jagovenko\",\"Dmukhovsky\",\"Tuma\",\"Bazilevich\",\"Jalovoi\",\"Tchekoev\",\"Lennard\",\"Kouba\",\"Masin\",\"Robins\",\"Obolyaninov\",\"Agudoff\",\"Vaidanovitch\",\"Hairutdinov\",\"Renaud\",\"Gramatke\",\"Pettigrew\",\"Shamburkin\",\"Yan\",\"Leake\",\"Zhagalin\",\"Boulos\",\"Maestri\",\"Gillen\",\"Wyman\",\"Antar\",\"Mcauley\",\"O'Doherty\",\"Veligjanin\",\"Lockyer\",\"Wragg\",\"Houttum\",\"Hudilainen\",\"Holzmann\",\"Makuda\",\"Davidovich\",\"Hajjar\",\"Luppian\",\"Vaindrakh\",\"Redding\",\"Katsebin\",\"Balahonsky\",\"Shimanouchi\",\"Alferov\",\"Ajdrna\",\"Saikov\",\"Viola\",\"Gramberg\",\"Bower\",\"Riddle\",\"Yasui\",\"Gro\\u00dfel\",\"Jagupets\",\"Abovin\",\"Hitro\",\"Shadid\",\"Churshukov\",\"Chikviladze\",\"Privalov\",\"Artemev\",\"Baitchoroff\",\"Golosov\",\"Zak\",\"Fordham\",\"Brandrick\",\"Mikhailjants\",\"Karube\",\"Henderson\",\"Ciardha\",\"Poyser\",\"Yapp\",\"Zdunowski\",\"Ryoo\",\"Handzlik\",\"Zhekulin\",\"Farrelly\",\"Lazzari\",\"Jejel\",\"Hayaliev\",\"Dzhunusov\",\"Awerintsev\",\"Jagodin\",\"Kerrigan\",\"Bagashev\",\"Vakulitch\",\"Fierro\",\"Tsalko\",\"Booth\",\"Ackermann\",\"Onkov\",\"Chepasov\",\"Portnikov\",\"Yu\",\"Pohmel'Nyh\",\"Egger\",\"Bazzi\",\"Miniahhmetov\",\"Stock\",\"Hamill\",\"Baz\",\"Hurtov\",\"Holoevsky\",\"Baidukov\",\"Nardovino\",\"Salamanca\",\"Kablits\",\"Rutberg\",\"Valenti\",\"Kahler\",\"Bakurov\",\"Horoshkevich\",\"Lewerenz\",\"Vanchagov\",\"Minyukov\",\"Spoerl\",\"Astronomov\",\"Foley\",\"Oleary\",\"Zogby\",\"Graminovsky\",\"Ukhovsky\",\"Exon\",\"Harkness\",\"Lutfullin\",\"Nader\",\"Isnard\",\"Jenkyns\",\"Haber\",\"Olivier\",\"Snyders\",\"Nagai\",\"Lyjin\",\"Dubko\",\"Molev\",\"Lockhart\",\"Ghannam\",\"Pickthall\",\"Sergeant\",\"Marsh\",\"D'cruze\",\"Jekov\",\"Jminko\",\"Abboud\",\"Dubatolov\",\"Erjenkov\",\"Abezgauz\",\"Matzukevich\",\"Otroshenko\",\"Baba\",\"Jmotov\",\"Shakhbazov\",\"Gall\",\"Ganim\",\"Abanin\",\"Zabrodin\",\"Obuh\",\"Kawachi\",\"Gunter\",\"Zhulev\",\"Salib\",\"Wallis\",\"Depp\",\"Mollo\",\"Perugia\",\"Bagroff\",\"Tahan\",\"Bakhtadze\",\"Ustinovich\",\"Flannery\",\"Begum\",\"Tzelischev\",\"Tupihin\",\"Malouf\",\"Okimoto\",\"Sleiman\",\"Shaidarov\",\"Muzenitov\",\"Roscoe\",\"Zhinov\",\"Froyanov\",\"Hauer\",\"Romero\",\"Herodes\",\"Tomlin\",\"Zhilenkov\",\"Ha\",\"Emmins\",\"Fay\",\"Voigt\",\"Bagryantsev\",\"Oshin\",\"Pyhov\",\"Moghadam\",\"Higginson\",\"Roffey\",\"Unwin\",\"Toma\",\"Newham\",\"Remih\",\"Hrula\",\"Daher\",\"Allport\",\"Shirkov\",\"Vardy\",\"Parasyuk\",\"Gudenko\",\"Mordkovich\",\"Kirkbride\",\"Kools\",\"Boerio\",\"Zhiril\",\"Lis\",\"Biragov\",\"Ba\",\"Hinchin\",\"Zha\",\"Horalya\",\"Holstov\",\"Gunby\",\"Jackson\",\"Moto\",\"Arena\",\"Chino\",\"Sparrow\",\"Zhirdetsky\",\"Asfour\",\"Knightley\",\"Elisman\",\"Levin\",\"Ooka\",\"Yatzkov\",\"Birkin\",\"Shadid\",\"Chance\",\"Arbuzov\",\"Matoke\",\"Goloborodov\",\"Hinich\",\"Nuremberg\",\"Kavanagh\",\"Albero\",\"Dacey\",\"Jennings\",\"Antoschenko\",\"Sellars\",\"Eberling\",\"Andreenko\",\"Gok\",\"Bueren\",\"Furnell\",\"Dagher\",\"De campo\",\"Kalenov\",\"Akker\",\"Doolin\",\"Levitansky\",\"Durgaryan\",\"Nader\",\"Hawes\",\"Greenwood\",\"Vazhov\",\"Hole\",\"Avalov\",\"Rocco\",\"Yuschak\",\"To The First Page\",\"Vyucheisky\",\"Qureshi\",\"Ritchie\",\"Kalachov\",\"F\\u00f6rstner\",\"Agnivtsev\",\"Islamshin\",\"Asahara\",\"Verdon\",\"Tahan\",\"Kachur\",\"Grushevsky\",\"Severijns\",\"Gluhih\",\"Omura\",\"Behrend\",\"Maroun\",\"Herten\",\"Deng\",\"Jourdan\",\"Gluskin\",\"Avtsin\",\"Kamata\",\"Onopriev\",\"Haddad\",\"Cuoco\",\"Balakhowsky\",\"Dale\",\"Jia\",\"Handal\",\"Valeev\",\"Naifeh\",\"Davis\",\"Porhun\",\"Francis\",\"Obuchi\",\"Aonghus\",\"Useev\",\"Wiseman\",\"Mochanovsky\",\"Pyanov\",\"Mifsud\",\"Mears\",\"Sakata\",\"Rosario\",\"Baksheev\",\"Lohin\",\"Kr\\u00fcger\",\"Brauer\",\"Ullman\",\"Shahlin\",\"Katzenellenbogen\",\"Balagul\",\"Vistchinsky\",\"Ellison\",\"Acerbi\",\"Dernov\",\"Ven\",\"Franke\",\"Kalinich\",\"Tahan\",\"Zhui\",\"Lositsky\",\"Zhongolovich\",\"Badeschenkov\",\"Ginty\",\"Juravlenko\",\"Abrikosov\",\"Djanaev\",\"Hiranuma\",\"Elmore\",\"Temple\",\"Baba\",\"Stainton\",\"Vaskovsky\",\"Shirmankin\",\"Andreichenko\",\"Engelgardt\",\"Touma\",\"Avilov\",\"Abrosimoff\",\"Friend\",\"Abrahams\",\"Kabak\",\"Salib\",\"Eggleston\",\"Thorne\",\"Mahno\",\"Muzarev\",\"Kassis\",\"Ang\",\"Jelen\",\"Herrmann\",\"Pinter\",\"Kabisha\",\"Mihailyants\",\"Bakaleinik\",\"Amello\",\"Rivera\",\"Daher\",\"Grankov\",\"Broomfield\",\"Kirkbright\",\"Foxley\",\"Salib\",\"Nassar\",\"Saitoh\",\"Dunst\",\"Kattan\",\"Tujilkin\",\"Coiro\",\"Djavahishvili\",\"Rihter\",\"Smit\",\"Dovlatov\",\"Mikhail\",\"Yukhma\",\"Traverse\",\"Raihert\",\"Zingerman\",\"Bata\",\"Nurjanov\",\"Sassa\",\"Nishiwaki\",\"Nader\",\"Tselikov\",\"Tchamushev\",\"Paternoster\",\"Fromberg\",\"Golovatov\",\"Gachegov\",\"Sak\",\"Hlypovka\",\"Mikhailin\",\"Veitch\",\"Avdiewski\",\"Simecek\",\"Fairbrace\",\"Turyansky\",\"Ilyakhin\",\"Finney\",\"Yokoyama\",\"Hudyakov\",\"Asturias\",\"Hakimi\",\"Lozin\",\"Halifman\",\"Najjar\",\"Griboedov\",\"Nesmachko\",\"Babakhanov\",\"Nakamoto\",\"Jaminsky\",\"Petsyuha\",\"Timashov\",\"Janson\",\"Nardi\",\"Natochin\",\"Eoghan\",\"Lyvin\",\"Ihara\",\"Hlopetsky\",\"Aswad\",\"Hlgatyan\",\"Salib\",\"Kirby\",\"Zhabotinsky\",\"Rekunov\",\"Maroun\",\"Handal\",\"Tchekhanov\",\"Vitoshkin\",\"Pears\",\"Assaf\",\"Riley\",\"Paragulgov\",\"De laurentis\",\"Lichkov\",\"Djuromsky\",\"Elizondo\",\"Sabbagh\",\"Cham\",\"Katzur\",\"Water\",\"Jalybin\",\"Fertig\",\"Patrianakos\",\"Tcheklyanov\",\"Aivazovski\",\"Rovinsky\",\"Djemilev\",\"Egamberdiev\",\"Shenyavsky\",\"Iles\",\"Lubsky\",\"Kats\",\"Admoni\",\"Ippitsusai\",\"Capello\",\"Grushevenko\",\"Walter\",\"Anorin\",\"Horigome\",\"Jijikin\",\"Kenley\",\"Lebedintsev\",\"Duarte\",\"Bazovski\",\"Abbas\",\"Poliev\",\"Moseley\",\"Eberman\",\"Assaf\",\"Jujlev\",\"Grammatakakis\",\"Guzov\",\"Mochtarev\",\"Rivero\",\"Didigov\",\"Nana\",\"Zhivkovich\",\"Nader\",\"Jablokov\",\"Moretti\",\"Danas\",\"Jadan\",\"Daushev\",\"Kouri\",\"Kishi\",\"Tzipushtanov\",\"Zhitin\",\"Said\",\"Novoselsky\",\"Maloof\",\"Markushevich\",\"Good\",\"Hadden\",\"Shakhovsky\",\"Jerebin\",\"Davidson\",\"Reading\",\"Norsworthy\",\"Asghar\",\"Saks\",\"Derkachev\",\"Makurov\",\"Rahmatullin\",\"Schugorev\",\"Nyrtsev\",\"Zhirnikov\",\"Imniaminov\",\"Buchta\",\"Vyvodtsev\",\"Jasper\",\"Petrov\",\"Finyutin\",\"Evsyukov\",\"Nejinsky\",\"Ambrogi\",\"Biryukov\",\"Crouch\",\"Kalimakhi\",\"Vihirev\",\"Valk\",\"Hussein\",\"Elkin\",\"Hanania\",\"Zhadanovsky\",\"Mcgrath\",\"Aldana\",\"Agnellutti\",\"Shamaro\",\"Leeuwenhoek\",\"Gukov\",\"Deeley\",\"Noakes\",\"Carroll\",\"Wilton\",\"Bilias\",\"Djigit\",\"Lewitckyj\",\"Badyashin\",\"Vestri\",\"Aspin\",\"Esmansky\",\"Okeefe\",\"Schepetkov\",\"Tsvetnov\",\"Bithell\",\"Ilyushkin\",\"Danyushevsky\",\"Dreher\",\"Havanov\",\"Bahar\",\"Barandych\",\"Zasluev\",\"Parma\",\"Honenev\",\"Flanders\",\"Timakov\",\"Harb\",\"Yaglintsev\",\"Roux\",\"Dnishev\",\"Jurakovsky\",\"Missiakos\",\"Holmes\",\"Thompson\",\"Gomez\",\"Mizuno\",\"Pierce\",\"Hon\",\"Pochechikin\",\"Alchangyan\",\"Ughi\",\"Utley\",\"Saller\",\"Shand\",\"Isa\",\"Inoguchi\",\"Kijimuta\",\"Tchekachev\",\"Ryjov\",\"Shadid\",\"Awduloff\",\"Millward\",\"Pander\",\"Wilson\",\"Munte\",\"Keys\",\"Elston\",\"Darchiev\",\"Strohkirch\",\"Evstafiev\",\"Jagubsky\",\"Yakhlakov\",\"Elleray\",\"Rotolo\",\"Avdievsky\",\"Kamon\",\"Fosse\",\"Pavlyukovsky\",\"Heritage\",\"Judkovich\",\"Peck\",\"Bazil\",\"Svocak\",\"Achthoven\",\"Mooren\",\"Jirik\",\"Abushenko\",\"Zhitnikov\",\"Maguire\",\"Faddeev\",\"Edon\",\"Shalagin\",\"Aves\",\"Moghadam\",\"Yablonsky\",\"Schoettmer\",\"Dana\",\"Halabi\",\"Salib\",\"Ohalloran\",\"Paquet\",\"Passerini\",\"Schirmer\",\"Serafin\",\"Kachainik\",\"Fairclough\",\"Crow\",\"Montagne\",\"Eddy\",\"Kaldin\",\"To The First Page\",\"Taidhg\",\"Ganim\",\"Rogerson\",\"Sam\",\"Dunlop\",\"Fotiadi\",\"Hanika\",\"Ratcliffe\",\"Alshits\",\"Antoun\",\"Sammiya\",\"Rainov\",\"Kassab\",\"Quan\",\"Wild\",\"Gladilin\",\"G\\u00f3rski\",\"Bakerkin\",\"Faulkner\",\"Bertrand\",\"Paranin\",\"Mahovikov\",\"Mravinsky\",\"Mogilnitsky\",\"Clements\",\"Shamgulov\",\"Nespola\",\"Rollinson\",\"Jakhno\",\"Abitoff\",\"Shimura\",\"Tinker\",\"Herheulidzev\",\"Chadrantsev\",\"Frampton\",\"Garver\",\"Beklemischev\",\"Ijichi\",\"Agresta\",\"Rakhmetov\",\"Donnachie\",\"Selby\",\"Vihansky\",\"Sato\",\"Hadad\",\"Valiahmetov\",\"Ganem\",\"Chukhnovsky\",\"Jukhno\",\"Tannous\",\"Langenberg\",\"Charoshnikov\",\"Jachnik\",\"Tzelovalnikov\",\"Konda\",\"Lapochkin\",\"Hadjiev\",\"Goloborodko\",\"Ferrier\",\"Basara\",\"Toru\",\"Cloutier\",\"Krusen\",\"Pekhtin\",\"Tsalykhin\",\"Hunter\",\"Timonkin\",\"Yusuf\",\"Bairak\",\"Asghar\",\"Huggins\",\"Iskyul\",\"Pel\\u00e1ez\",\"Rowlands\",\"Prentice\",\"Zhilinsky\",\"Ryjy\",\"Tsapin\",\"Fan\",\"Glazatov\",\"Holgu\\u00edn\",\"Belobrovkin\",\"Dovgusha\",\"Bahtinoff\",\"Trinh\",\"Atiyeh\",\"Datsenko\",\"Fiore\",\"Timms\",\"Zajicek\",\"Houghton\",\"Jue\",\"Garratt\",\"Bishara\",\"Sleiman\",\"Atiyeh\",\"Mansour\",\"Eckersall\",\"Adylov\",\"Nijo\",\"Inoue\",\"Remmer\",\"Shamakhov\",\"Mojaisky\",\"Aganov\",\"Trott\",\"Vanchugov\",\"Morcos\",\"Adamou\",\"Denis\",\"Mindel\",\"Pang\",\"Bakhmetov\",\"Feng\",\"Grierson\",\"Shelting\",\"Huranov\",\"Bavtrukevitch\",\"Zhelaev\",\"Nicol\",\"Pochkaev\",\"Lang\",\"Atiyeh\",\"Rovnin\",\"Livadin\",\"Bakhmutov\",\"Tueshev\",\"Rigby\",\"Vlasek\",\"Her\",\"Tchekhluev\",\"Ghannam\",\"Ponedelnik\",\"Craven\",\"Villalobos\",\"Mathieu\",\"Inouye\",\"Schlusser\",\"Panesar\",\"Mullen\",\"Jatsuba\",\"Viner\",\"Podshivalov\",\"Veselkov\",\"Greenall\",\"Hruska\",\"Bahanoff\",\"Grodensky\",\"Cotterill\",\"De felice\",\"Turetskov\",\"Matsuoka\",\"Awad\",\"Judaev\",\"Dovgolevsky\",\"Sayegh\",\"Gefter\",\"Babuh\",\"Veasey\",\"O'Loughlin\",\"Gasman\",\"Close\",\"Lapenkov\",\"Rotmistrov\",\"Maruya\",\"Hlusov\",\"Gerges\",\"Morandi\",\"Fukushima\",\"Mcguinness\",\"Villamov\",\"Hamaev\",\"Prokoshkin\",\"Awad\",\"Mozheiko\",\"Matsotsky\",\"Beal\",\"Paraskun\",\"Said\",\"Wrenn\",\"Jagoda\",\"Modestov\",\"Rogashkov\",\"Yuan\",\"Wilkin\",\"Tannous\",\"Tchekin\",\"Ayugai\",\"Leroy\",\"Avis\",\"Eckstein\",\"Tzakh\",\"Katsushika\",\"Kerrell\",\"Jvanetsky\",\"Chelmy\",\"Fedotko\",\"Valchitsky\",\"Valdman\",\"Pavlyukov\",\"N\\u00fa\\u00f1ez\",\"Shalyapin\",\"Raizer\",\"Makul\",\"Avkhimovitch\",\"Kalinovsky\",\"Boutros\",\"Babak\",\"Prosdocimi\",\"Holl\",\"Piterskih\",\"Jamov\",\"Kattan\",\"Haibullin\",\"Astbury\",\"Tuffnell\",\"Cline\",\"Dagher\",\"Romeijnders\",\"Fujita\",\"Pirashkov\",\"Fukuzawa\",\"Aswad\",\"Renzyaev\",\"Barabolya\",\"Frankland\",\"Huan\",\"Haratyan\",\"Usikov\",\"Oldham\",\"Horev\",\"Iskortsev\",\"Coomber\",\"Poryvay\",\"Masih\",\"Hemmings\",\"Nesmelov\",\"Mckenzie\",\"Aswad\",\"Hachaturyan\",\"Letsos\",\"Kachalovsky\",\"Agapitov\",\"Touma\",\"Blair\",\"Shammas\",\"Meszes\",\"Zenbitsky\",\"Dogadkin\",\"Pegg\",\"Belorusov\",\"Gladyshev\",\"Yakir\",\"Rjevsky\",\"Vertkin\",\"Gatturov\",\"Yuzefov\",\"Livnev\",\"Nassar\",\"You\",\"Guirguis\",\"Abramovich\",\"Bahar\",\"Galkovsky\",\"Waugh\",\"Vertegel\",\"Kassab\",\"Vijonsky\",\"Koberna\",\"Likharev\",\"Ho\",\"Gudov\",\"Demetr\",\"Tsvetaev\",\"Assaf\",\"Darbyshire\",\"Isaichev\",\"Luo\",\"Murphy\",\"Waxweiler\",\"Khouri\",\"Daher\",\"Elstone\",\"Herovets\",\"Geaney\",\"Sugimura\",\"Jigalov\",\"Kimber\",\"Shaer\",\"Deshkin\",\"Mikhailushkin\",\"Shenfeldt\",\"Rahal\",\"Chilar\",\"Hapsirokov\",\"Lucey\",\"Zimin\",\"Hlopin\",\"Shadid\",\"Harnikov\",\"Djuro\",\"Kalandinsky\",\"Dorman\",\"Fofanov\",\"Sauveterre\",\"Palzewicz\",\"Basara\",\"Bekleshev\",\"Zasyadko\",\"Shalhoub\",\"Kouri\",\"Belesis\",\"Goretti\",\"Halyapin\",\"Cavanagh\",\"Mcneil\",\"Gluz\",\"Utulov\",\"Kenneth\",\"Alchevsky\",\"Makaseev\"],\"x\":{\"__ndarray__\":\"+MrQv87srD50IY8+tt01P5AfyL9OOKG/ywQ+P+S5OD87zaS/pIS7v+/N2b97nTg/DDRNv4EMdL8qJKA/kH3bv1VlpD4CRjs/BbcBPRxGLD0faaw/bcJAPxjbir+vLzg/3HJ5PuKNzr9tvrS/AiimPyN7qD8EOaq/3aO+v0GJUD/0c5o/Uouev+Hi178HM9u/tf8oP3Ep0T3kH1A9ZXYdPoVyGT90IiQ/gdQYv0JEor8YL68/J2nGv2XL0b+zXK0/ma+VPpWljT+fqKQ++EacPgQq2T7Tegy/XV3Nv4WHeT+EebE/iU15P7nNVj5CsMW7h7imPwdAMz99nq0/lBmhv+jEvL/U+xs/wJWpPxj6qL8iIrk+6g1IP1oVID/2QT8/hfWwv7xOQr9hBaU/qXa5v7yqh78lraE/y2B9v/kEoz+jGDQ/2F22v86jsL/lVte/WF+zvDdkHj+Mgx4+8iUjPSCEQT/T4ky9mQ9MP2YOmb/hUq4/tp2iP7xmrT5fwbm/5uCmv3vDxL9RuZo/40g/P0RG/T7hiTM/BNRsP0LD2D6vNo+/NNyqP1Y2eb8CY76/CYGBP61uGz+DjUM/aGuEPxbcIT8p9iw+/Bw8v9IyST+qtfw+GQixvsBYrz9V4E4/3M/VvxAu6T06yQk+NiApP1RHGb8m4ZI/nVrAPYBlOT/jeaY/H2GFv3KQQ79vjTI/wfgTP5zrgj63nX8+GO+2vw/wsb+wAqq/47RUPrAamL9Em54/DAQKPzzSgT8zZc6/yZ0Wv5KLnr/U+wc/5DuuP9G9VT+51MS/v3SQP6x4vL9Tr7O/QwuuP8/NmT97/oU/1fMNv4froD/lYdq/mjhIP7xiF7+iNDk+H4Bhv3tlRD8pRKs/y22dP59817+SS6W+6gBFPqhxmz+yE6K/boCrPwEHoD/HelA++xsoP/6nrr9C96i/ZcMKP95mtj4xhy+/8du9v56aoj9CiVA/p6JMPniKrT67zyK+n8eJP5zrgj5gfYK/ZXfbvxG5KT5rDKI/VYOuP03+0b86LFM/JnauPs8dLD5uPWc+re1qPlS+Hr9MLtC9oInLv2LTSD9IwtO86d3RPlZhmD+bRsy/SwaGv44dCD7akkM/6UCuv8gPuz6hLDi/RemePwRmuD79SEo/89Usve8+3b823w8/ico+vvK1zL+dVyM/djwJP4zYjr/5zjs+hjSlPn73qz8dt+0+u6IMPynSAT+B7qo/9J6mvzIjlL+VAI0/3M28P+Pokj+0p6g/uN8GP4m3Fb//G8y/CDFVPHHuhb8brqs/v4rCPtGEgL8Lm9G/U9/Wv0pOub4sPY+/7xZfPhtHyr8yJ6q/hkagvw1hQD8jtwU+k76av0VG/T6y7DE+/vWpv1yxnD92TaG/Tr+dv1AanD9a7Mq/hClKPjqkqr9fO6c/hc6bv0jRMj9hSsG/W1mVP00yNj4ykqk/6TiJv15jwb8YvWw+3NiFvxkTlT/dNYg/YcFYPzeeRr8owpq/ep6SvWjUhz9pDt+9RfkfP00YJT+0Qa0/ugCpv2KXOj8bJBm/dtM4PpKlij55132+P7yYvxCKljx6cCA/FVWYvzDmQj+f9Ny/0UsIPmfvBz5ggqy/c/hMP2Rygj8/Az+/t6LIvw6o0r/+6pU+H+JJP2IGnb/FioU/hFwyPvUtCD5STkg/jhadPxerDj9sFZ8/VdbcPKVBLr/wb3+/14vWvzW+h7885gk/h7aDv3g6c75BKZq/pmckPk+9s7/uDpm+BzmNv/FV+z5Ch4I/k5ezvvOprT+RthE/g6Sgvw8gtD9puqI//wtDv14ziD/Ykxq/6tazv6UK7z7IBiK/7CenPyLiTD/Zw6A/xMz0Pvi9jD6dHoG/1sc8v7wmlz4GD1k/Y0jYv2zEJb/oyVE/C4V/Pmb4lr+MhKo/doSHvpxY0L+jq7q/vp0tPkI3xj5AX7K/HCc5vlx4bT5n360/eVChPyTCyr7DaqK/GGN0P5tUw7+cZHI8ydmgv3jNpb8rB7i/3g+eP0Rkkz6nCFK/U0i0P+/ahb+xOWI+fzgsP171Mj968Kg/bnfDvhtMkT/lBki/ex0xvzYP2r/lTKU/dlilvhdkNz/ovaw/OiRBvzOoOj+2WZ0/GdthvT/DKD99CJy/nEOevyzxNz+tBpe/9xWoP6OZqD/aXag/oJosP+VX2b/P/wI/TCKBO+jccD9Qjlo/23vzPsreQLxhBjo+jbl1P9fXDD+WGBk+SFZLvEWhnz8ebtO/6vK4Pn/KTz8cQZg/UJqTv4fsbb/M2Zk/q5H1PShkML++yqC/98pePz/msL9Mm3m/kYZBPwdPtb+7XTi/7ycCv3SiRj+OFvw+3MOuvylZjD9EUz8/MO+gP4F1nr+HPgM/M7OlPy0BHT7cdaU/pLF0Pz8Hr77t0Io+MsfXv7v7VD9YipQ/qjezPWwzkr8x0s2/TKbPv84Lqj4i/Ne/LLCFP/+qdr+Qzhk/yCSgP3V81L/WqZe/mFqcP76NhT75fnK+Fh6kv80WlD6JcaA/hMJPvcT7HT6YoV8/Lt9yvysOqr9eo5a/CNpHv0IDH79pDM49aVuAP0iOOD+XemM/l4qxPn9+JT+WlJk/Wauvv9VWEz5Q9Zs/HIkJv1b8fz5kHAg/Y0uFPbexnz+IdG09BGa4Pv1NGr9YeVO+zMbFv/fPSj+tWAc+qQPoPsJKsz+pQGc+9W0Rv9tJND9YY46/BUp4P5ivrj/VPWY+Sw96v7bvxb/Tx78+BQXCv1dknb/DCIE+9MGaPxR8eb8ADb6/6LE4PzCtaz51jLQ+5K3BPgxdAD7AMZI/VWWkPk3RoT+MOAY/Vvx/Phb40r92YpG/fIghP/Nn0L9RBAe/KL+zv3/TIj/Blwu/CX+Lv/FZkz7BCQo+nI7bv7tw079WW6C/EvR1vqVng7+Jm7O/728GPjqUsL/X1ww/EPaGPzhiAj7Y/Ne/acSjP1qAgT+4NzW+ZDWlP0DAMz0Y8pS/60ZFP7XqIz8R1sy+h1x7P7GUt7/SpNK/mKpKP54x+j7RDiU+4fZEvVchKT9e9HC/81qiP1iJOD5UVCs/LumFP70Oor4vg2Q+QbWUvwp0KD9bDJu/kfrcPcRsFb+n9bG/9RqOP12Jbj+vKs+/PECoPm/J1b8CTIu/uiEVP5ihXz97ON+9ONChP7BHkb+reMK/jVW5v7ncRT60+wI/rDKFP+bdLD9zZQU+XZOlP6jmAT/Tl48/gYXCPgf8Or9S3x+/EfWnPtbFvr9bvak/YxyMP+A5tr8zA0W/4946P84Snz/Mx6k/XFxHv9U9Zz+aWy0+mcaWPjpgMj/sIVo/uNG5PQEgnL+uQFK+EAY7P0HD2D6WFa4/I3mFv7qmjz8wMAc/z/8CP5eolb/j/o0+rHqiP+GI2L8Xsru/4jC3vyulLj8XsE4+FqDWv3Bu0r/T8D4//nOmP5phLz8YUSS/OSVvP8UCOT60yYY/koOWv7dvgb7iKKG/ZcDav+rHnL+Upjm/MDL6PtubMj9wMqI/UzGdPxK+j7+TKsO/dGmCP22a3D5l7K8/vEbav5LTkb/oB7M/6DLSvdX0Kz8t5y2+5YTCPmkIJ74qevk+2vivv59fdz8Aqrq/PGsLP0y2f7/36R8/qtaSP0OPIj4JX7I+ewFbPznKOr9WOII/pmcFPzeFlD/oJbK/PmIVvfGNmj9EtI4/bx69v1l9rL2G7Z0/goiCvx+EsT53HI8/7+q4v5gIHD8KCcC/eQqwP/p2Qr/lMcu/su7JPuRJvL9PxOS9JChFP45FuD+kWAE+vvJ7PkT6fz6tTkg/KJrIvxMJxr83e10+t6STvr4fHL3NY8C/Pkjdv9osgD9G2UA+tZKuv4RFbL+Fchk/yi6nv+9YQT8RHyw/PnWcv615lL/VwWy/2q7Pvx8IrD9VxMG/zj+Bv2qITz4i/t4+FtmdP0SSJ751q66/CX+ZP/34rT9/7Zg/1ucoPndbHz/mIKe/CHekP9O+Dz7JYaE/56XNPYHJPL8zgps+tbH2PpgIHD8A46M/BkOxvzacqb9g/k4/6FisPkvCHz8IrqE/1b4Ev1e9Fr/uoc6/ZGwPvgu5nr9CEi++wSaPP+PjST5kkKC+ghR7PzMLjL+w2qY/OjeVP8G/lT98yys/kOdHP8DHaD83vuQ+c2UFPrPvGr4CK54//g2tv8paOT5Xo7u/TJGRvcHCOz/A19m/juUNP0Opgj+QFrA/goE6vm3tKT7QwdS93Vajv4EsxDyCr74+6mtKP9aBm79LuJA/dMiIPRWExjwqlak/RiuqvyyKj7/xF9S/teAmPhHsqj9i2o4+/cZ7P8AEHb8MCqU/XFSsv4Wajj8bob+/WqKAPo0Xoj3MUqU/L+iav/8wSD8RGWo/lzGfPzvwkz+vFI0+Wv6vPzXZwL842ri/0OakPwNSub966Ki/ZRbIvwehND9wbBw+MnC9v2lO0b8Ta9a/GXqiPjiN0b7Spsu/1vubPyiZKz/aAKC/FLj3PV5/Ub+vWn6/jLU8P1kWn79evEi/ZWTTPoZctT+j/hE/Mo1RPl6QgT8h8Ks+QDy9v3CnST8sTi+/vnWDP2OSpj8bCRK/1IiaP9SDnT75bKw/31TOv1PO9D6Orcc9Lm5ZP4qfND9YMBe/zAOsP4LSHz8NDKG/Ml3gPdKsu78Pft+9bl5NPv2ClL+X+aO/Tr3Tv+eCET++osC/q9eWvpw2Mr7YJJ892w8TPlWEG7+0SzU/ojYHv/wWqT4bb32/UCapP5CzYD/naFQ/EypIP14cLT4KtJ0/cqqkPyL+3j5Pers/jEMKPwJcsT8LF7u/E/2gP+R2I7/9SdW/GNNKPoIiMD83+eU93zOSv8NToT+QjSs+LCGhv6Fkmb8ITTU/CdGKv+VHpr+JKmA/cOsmPzGwmz+1LZM/lf+svyexyL9DVD4/AWumPwKkBD+b45o/b9Ohv9H0UL/eDik+TiyhvU960b9xUtK/a4qov2k3ob+F0gE/0p1/v5Ve17/pjv0+J2OtP0nscb2rJCY/fuEuP7/cSD91aqg/lSedP02nhj8HoTQ/5WQavQCEoT/suJc/u/tUP7s/Fz8qHzw/CDu8P/brJj/5UlA/Jq22v81wPj/FXky94IADPl4SPz4kfqI/dlhOPx+DtL97jwi+QFmqPyWunT9ZeCQ/DKioP5297Tz9Q7y/gdIfP0zlrj/d+/I+2O6WPzhZfj4ZSkc+uIzTv8dwID/cedq/0/5dP3N93L9qiE8+suwxPpQ3CT8AVvs+MiKJPybgjb+vfUA+zetyPrGjr7/5UlA/bhkYP2vJr7/F0Uk/PREzPj6UoT/CPKg/u/wNPg==\",\"dtype\":\"float32\",\"shape\":[1000]},\"y\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[1000]}},\"selected\":{\"id\":\"a908113a-9c56-4bc2-be4d-fad314280f33\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"635b7ffc-a2e5-4884-81cf-dc092b34f651\",\"type\":\"UnionRenderers\"}},\"id\":\"8a198caa-58bc-45ab-96fb-d01f90102f0e\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"635b7ffc-a2e5-4884-81cf-dc092b34f651\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"4ed1bbe1-4561-4703-a2ea-36bb9deac212\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"45a86b85-6cc2-4c63-976d-7a0d5dcb6380\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"renderers\":\"auto\",\"tooltips\":[[\"token\",\"@token\"]]},\"id\":\"af617af7-779f-44c1-9de9-76bd1f48b139\",\"type\":\"HoverTool\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"7d605346-e8f9-4c1c-957a-f99b3c6c4815\",\"type\":\"Title\"},{\"attributes\":{\"below\":[{\"id\":\"cbefe07d-800c-4a3b-adec-38a316f8749f\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"02d4e978-b6d0-4df5-b27f-711c73b87b36\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"renderers\":[{\"id\":\"cbefe07d-800c-4a3b-adec-38a316f8749f\",\"type\":\"LinearAxis\"},{\"id\":\"7e564180-22a7-4bfa-963b-0731cc96fdcc\",\"type\":\"Grid\"},{\"id\":\"02d4e978-b6d0-4df5-b27f-711c73b87b36\",\"type\":\"LinearAxis\"},{\"id\":\"d444d535-3009-4c3d-acd8-222a7d59b12f\",\"type\":\"Grid\"},{\"id\":\"37020d50-e2be-4d1a-aa77-77a33fce1e64\",\"type\":\"BoxAnnotation\"},{\"id\":\"406372d1-e0bb-4db2-966d-d2f374cd86a3\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"7d605346-e8f9-4c1c-957a-f99b3c6c4815\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"62289067-c412-40ce-abe9-90674652c4eb\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"c2d38120-7f85-4360-8ab4-4b549ce77154\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"6beb4060-0812-47c5-b0fe-3b1a79962897\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"b5bb11f8-fac6-4c7e-ab6e-4d7fdf6eb7e6\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"45a86b85-6cc2-4c63-976d-7a0d5dcb6380\",\"type\":\"LinearScale\"}},\"id\":\"ab9055d0-2edf-4c57-a68c-b3b64f7e6a80\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null},\"id\":\"b5bb11f8-fac6-4c7e-ab6e-4d7fdf6eb7e6\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"077a8f47-8def-401a-9075-4a8eae796d5d\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null},\"id\":\"c2d38120-7f85-4360-8ab4-4b549ce77154\",\"type\":\"DataRange1d\"},{\"attributes\":{\"overlay\":{\"id\":\"37020d50-e2be-4d1a-aa77-77a33fce1e64\",\"type\":\"BoxAnnotation\"}},\"id\":\"c13a61a4-8b68-4eac-84a1-9bf47a8ded76\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"feea1738-3611-42af-bc86-1f132ea31047\",\"type\":\"SaveTool\"}],\"root_ids\":[\"ab9055d0-2edf-4c57-a68c-b3b64f7e6a80\"]},\"title\":\"Bokeh Application\",\"version\":\"0.13.0\"}};\n",
              "  var render_items = [{\"docid\":\"f6e473b1-c45b-4c97-ac52-2032e282ef8a\",\"roots\":{\"ab9055d0-2edf-4c57-a68c-b3b64f7e6a80\":\"88bca0f8-d975-4259-ab29-879554d2fca0\"}}];\n",
              "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        embed_document(root);\n",
              "        clearInterval(timer);\n",
              "      }\n",
              "      attempts++;\n",
              "      if (attempts > 100) {\n",
              "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
              "        clearInterval(timer);\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "tags": [],
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "ab9055d0-2edf-4c57-a68c-b3b64f7e6a80"
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "UnQMDFTgKCY0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Визуализация работы сети\n",
        "\n",
        "На каждом шаге RNN выдает какой-то вектор. Полносвязный слой применяется только к последнему выходу. Но можно же посмотреть и на промежуточные состояния - как менялось мнение сети о том, к чему относится это слово.\n",
        "\n",
        "**Задание** Напишите свой визуализатор."
      ]
    },
    {
      "metadata": {
        "id": "12rQahnMD5oJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Тут тоже стал писать для `RNN`, а не для `LSTM` - поэтому пришлось переписать ее немного, чтобы можно было доставать $h_{t}$. В `LSTM` просто хватило бы `output` (первое возвращаемоее значение `nn.LSTM`).*"
      ]
    },
    {
      "metadata": {
        "id": "fum6kh9OsW_2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SimpleRNNVizualizer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Here will store h_{t}\n",
        "        self.hidden_states = []\n",
        "        \n",
        "        self._hidden_size = hidden_size\n",
        "        self._hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        # Flush, because we need it only for eval\n",
        "        self.hidden_states = []\n",
        "        seq_len, batch_size = inputs.shape[:2]\n",
        "        \n",
        "        if hidden is None:\n",
        "            hidden = inputs.new_zeros(batch_size, self._hidden_size)\n",
        "         \n",
        "        for i in range(len(inputs)):\n",
        "            hidden = torch.tanh(self._hidden(torch.cat((hidden, inputs[i]), -1)))\n",
        "            self.hidden_states.append(hidden)\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ao8ZUS7ZrlFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnamesClassifierVizualizer(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, lstm_hidden_dim, classes_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Here will store outputs for each h_{t}\n",
        "        self.output_history = []\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._rnn = SimpleRNNVizualizer(emb_dim, lstm_hidden_dim)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, classes_count)\n",
        "            \n",
        "    def forward(self, inputs):\n",
        "        'embed(inputs) -> prediction'\n",
        "        last, history = self.embed(inputs)\n",
        "        \n",
        "        # Flush, because we need it only for eval\n",
        "        self.output_history = []\n",
        "        for entity in history:\n",
        "            self.output_history.append(self._out_layer(entity))\n",
        "        \n",
        "        return self._out_layer(last)\n",
        "    \n",
        "    def embed(self, inputs):\n",
        "        'inputs -> word embedding'\n",
        "        return self._rnn(self._emb(inputs)), self._rnn.hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fC2lP1eos1IC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2a6e44f2-06fd-48d8-ca3b-91cd8bf431d7"
      },
      "cell_type": "code",
      "source": [
        "model = SurnamesClassifierVizualizer(vocab_size=len(char2ind), emb_dim=16, lstm_hidden_dim=64, classes_count=len(lang2ind)).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, epochs_count=10, batch_size=128, train_data=(data_train, labels_train),\n",
        "    val_data=(data_test, labels_test), val_batch_size=512)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 10, Epoch Time = 3.21s: Train Loss = 1.9655, Val Loss = 1.8525\n",
            "Epoch 2 / 10, Epoch Time = 3.01s: Train Loss = 1.8487, Val Loss = 1.8498\n",
            "Epoch 3 / 10, Epoch Time = 3.06s: Train Loss = 1.8480, Val Loss = 1.8470\n",
            "Epoch 4 / 10, Epoch Time = 2.97s: Train Loss = 1.8347, Val Loss = 1.8127\n",
            "Epoch 5 / 10, Epoch Time = 3.04s: Train Loss = 1.6998, Val Loss = 1.7294\n",
            "Epoch 6 / 10, Epoch Time = 2.90s: Train Loss = 1.5597, Val Loss = 1.5707\n",
            "Epoch 7 / 10, Epoch Time = 2.93s: Train Loss = 1.4918, Val Loss = 1.6643\n",
            "Epoch 8 / 10, Epoch Time = 2.87s: Train Loss = 1.4672, Val Loss = 1.4975\n",
            "Epoch 9 / 10, Epoch Time = 2.89s: Train Loss = 1.4163, Val Loss = 1.3957\n",
            "Epoch 10 / 10, Epoch Time = 2.99s: Train Loss = 1.3737, Val Loss = 1.3579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8VaUd4LWtyd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_indices = np.random.choice(np.arange(len(data_test)), 1, replace=False)\n",
        "words = [data_test[ind] for ind in word_indices]\n",
        "word_labels = [labels_test[ind] for ind in word_indices]\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_batch, y_batch = next(iterate_batches(words, word_labels, batch_size=1))\n",
        "    logits = model(LongTensor(X_batch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kiEBC1uvwzDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vizualize():\n",
        "    true_language = 'ELVEN'\n",
        "    for lang in lang2ind:\n",
        "        if lang2ind[lang] == y_batch:\n",
        "            true_language = lang\n",
        "    print('SURNAME: {}\\tLANGUAGE: {}\\n'.format(words[0], true_language))\n",
        "    \n",
        "    results = [torch.softmax(model.output_history[i], 1).argmax().item() for i in range(len(model.output_history))]\n",
        "    \n",
        "    for idx, result in enumerate(results):\n",
        "        for lang in lang2ind:\n",
        "            if lang2ind[lang] == result:\n",
        "                pred_language = lang\n",
        "        print('SURNAME PREFIX: {}\\tPREDICTED LANGUAGE: {}'.format(words[0][:idx+1], pred_language))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_zir25uoxSi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5abd327c-fabd-420f-cfd6-f265be8dd185"
      },
      "cell_type": "code",
      "source": [
        "vizualize()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SURNAME: Djisev\tLANGUAGE: Russian\n",
            "\n",
            "SURNAME PREFIX: D\tPREDICTED LANGUAGE: Greek\n",
            "SURNAME PREFIX: Dj\tPREDICTED LANGUAGE: English\n",
            "SURNAME PREFIX: Dji\tPREDICTED LANGUAGE: English\n",
            "SURNAME PREFIX: Djis\tPREDICTED LANGUAGE: English\n",
            "SURNAME PREFIX: Djise\tPREDICTED LANGUAGE: English\n",
            "SURNAME PREFIX: Djisev\tPREDICTED LANGUAGE: Russian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DickfCNwJZFT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Улучшение сети\n",
        "\n",
        "**Задание** Замените SimpleRNN на LSTM. Сравните качества."
      ]
    },
    {
      "metadata": {
        "id": "fwK8yu756aFw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMSurnamesClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, lstm_hidden_dim, classes_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._lstm = nn.LSTM(emb_dim, lstm_hidden_dim)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, classes_count)\n",
        "            \n",
        "    def forward(self, inputs):\n",
        "        'embed(inputs) -> prediction'\n",
        "        return self._out_layer(self.embed(inputs))\n",
        "    \n",
        "    def embed(self, inputs):\n",
        "        'inputs -> word embedding'\n",
        "        output, (h_n, c_n) = self._lstm(self._emb(inputs))\n",
        "        return h_n[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9Bcg-VWPZRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "25904104-56ad-49ac-b31b-50f93b1408c4"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMSurnamesClassifier(vocab_size=len(char2ind), emb_dim=16, lstm_hidden_dim=64, classes_count=len(lang2ind)).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, epochs_count=50, batch_size=128, train_data=(data_train, labels_train),\n",
        "    val_data=(data_test, labels_test), val_batch_size=512)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 50, Epoch Time = 1.35s: Train Loss = 1.9547, Val Loss = 1.7609\n",
            "Epoch 2 / 50, Epoch Time = 1.42s: Train Loss = 1.6031, Val Loss = 1.4812\n",
            "Epoch 3 / 50, Epoch Time = 1.37s: Train Loss = 1.4621, Val Loss = 1.4059\n",
            "Epoch 4 / 50, Epoch Time = 1.37s: Train Loss = 1.3639, Val Loss = 1.3486\n",
            "Epoch 5 / 50, Epoch Time = 1.46s: Train Loss = 1.2916, Val Loss = 1.2719\n",
            "Epoch 6 / 50, Epoch Time = 1.38s: Train Loss = 1.2210, Val Loss = 1.2403\n",
            "Epoch 7 / 50, Epoch Time = 1.41s: Train Loss = 1.1632, Val Loss = 1.1942\n",
            "Epoch 8 / 50, Epoch Time = 1.34s: Train Loss = 1.1130, Val Loss = 1.1376\n",
            "Epoch 9 / 50, Epoch Time = 1.36s: Train Loss = 1.0659, Val Loss = 1.0877\n",
            "Epoch 10 / 50, Epoch Time = 1.40s: Train Loss = 1.0145, Val Loss = 1.0665\n",
            "Epoch 11 / 50, Epoch Time = 1.35s: Train Loss = 0.9885, Val Loss = 1.0530\n",
            "Epoch 12 / 50, Epoch Time = 1.38s: Train Loss = 0.9506, Val Loss = 0.9917\n",
            "Epoch 13 / 50, Epoch Time = 1.35s: Train Loss = 0.9162, Val Loss = 0.9670\n",
            "Epoch 14 / 50, Epoch Time = 1.34s: Train Loss = 0.8855, Val Loss = 0.9315\n",
            "Epoch 15 / 50, Epoch Time = 1.41s: Train Loss = 0.8536, Val Loss = 0.9447\n",
            "Epoch 16 / 50, Epoch Time = 1.34s: Train Loss = 0.8339, Val Loss = 0.9322\n",
            "Epoch 17 / 50, Epoch Time = 1.38s: Train Loss = 0.8205, Val Loss = 0.9099\n",
            "Epoch 18 / 50, Epoch Time = 1.33s: Train Loss = 0.7942, Val Loss = 0.9087\n",
            "Epoch 19 / 50, Epoch Time = 1.43s: Train Loss = 0.7779, Val Loss = 0.8809\n",
            "Epoch 20 / 50, Epoch Time = 1.33s: Train Loss = 0.7549, Val Loss = 0.8771\n",
            "Epoch 21 / 50, Epoch Time = 1.31s: Train Loss = 0.7305, Val Loss = 0.8460\n",
            "Epoch 22 / 50, Epoch Time = 1.35s: Train Loss = 0.7099, Val Loss = 0.8172\n",
            "Epoch 23 / 50, Epoch Time = 1.34s: Train Loss = 0.6993, Val Loss = 0.8011\n",
            "Epoch 24 / 50, Epoch Time = 1.38s: Train Loss = 0.6771, Val Loss = 0.8090\n",
            "Epoch 25 / 50, Epoch Time = 1.28s: Train Loss = 0.6620, Val Loss = 0.8149\n",
            "Epoch 26 / 50, Epoch Time = 1.27s: Train Loss = 0.6442, Val Loss = 0.8118\n",
            "Epoch 27 / 50, Epoch Time = 1.26s: Train Loss = 0.6335, Val Loss = 0.7950\n",
            "Epoch 28 / 50, Epoch Time = 1.27s: Train Loss = 0.6187, Val Loss = 0.7698\n",
            "Epoch 29 / 50, Epoch Time = 1.36s: Train Loss = 0.6090, Val Loss = 0.7775\n",
            "Epoch 30 / 50, Epoch Time = 1.26s: Train Loss = 0.5965, Val Loss = 0.7640\n",
            "Epoch 31 / 50, Epoch Time = 1.25s: Train Loss = 0.5819, Val Loss = 0.8094\n",
            "Epoch 32 / 50, Epoch Time = 1.25s: Train Loss = 0.5751, Val Loss = 0.7687\n",
            "Epoch 33 / 50, Epoch Time = 1.29s: Train Loss = 0.5629, Val Loss = 0.7631\n",
            "Epoch 34 / 50, Epoch Time = 1.32s: Train Loss = 0.5483, Val Loss = 0.7993\n",
            "Epoch 35 / 50, Epoch Time = 1.26s: Train Loss = 0.5478, Val Loss = 0.7513\n",
            "Epoch 36 / 50, Epoch Time = 1.25s: Train Loss = 0.5301, Val Loss = 0.7610\n",
            "Epoch 37 / 50, Epoch Time = 1.27s: Train Loss = 0.5253, Val Loss = 0.7617\n",
            "Epoch 38 / 50, Epoch Time = 1.33s: Train Loss = 0.5092, Val Loss = 0.7722\n",
            "Epoch 39 / 50, Epoch Time = 1.27s: Train Loss = 0.5072, Val Loss = 0.7751\n",
            "Epoch 40 / 50, Epoch Time = 1.28s: Train Loss = 0.4892, Val Loss = 0.7603\n",
            "Epoch 41 / 50, Epoch Time = 1.32s: Train Loss = 0.4807, Val Loss = 0.7825\n",
            "Epoch 42 / 50, Epoch Time = 1.31s: Train Loss = 0.4741, Val Loss = 0.7665\n",
            "Epoch 43 / 50, Epoch Time = 1.36s: Train Loss = 0.4688, Val Loss = 0.7805\n",
            "Epoch 44 / 50, Epoch Time = 1.29s: Train Loss = 0.4615, Val Loss = 0.7581\n",
            "Epoch 45 / 50, Epoch Time = 1.29s: Train Loss = 0.4538, Val Loss = 0.7494\n",
            "Epoch 46 / 50, Epoch Time = 1.30s: Train Loss = 0.4441, Val Loss = 0.7854\n",
            "Epoch 47 / 50, Epoch Time = 1.29s: Train Loss = 0.4430, Val Loss = 0.7932\n",
            "Epoch 48 / 50, Epoch Time = 1.35s: Train Loss = 0.4323, Val Loss = 0.8000\n",
            "Epoch 49 / 50, Epoch Time = 1.29s: Train Loss = 0.4215, Val Loss = 0.8143\n",
            "Epoch 50 / 50, Epoch Time = 1.28s: Train Loss = 0.4155, Val Loss = 0.7712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "btJgd0EcPcmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "8236d68a-f0d7-4930-ae39-d254ca42a64f"
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "X_test, y_test = next(iterate_batches(data_test, labels_test, batch_size=len(data_test)))\n",
        "X_test = LongTensor(X_test)\n",
        "y_pred = torch.softmax(model(X_test), 1).argmax(1)\n",
        "\n",
        "print('Accuracy = {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test, y_pred, \n",
        "                            target_names=[lang for lang, _ in sorted(lang2ind.items(), key=lambda x: x[1])]))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 78.88%\n",
            "Classification report:\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    Chinese       0.40      0.69      0.51        80\n",
            "     Arabic       0.94      1.00      0.97       600\n",
            "      Dutch       0.68      0.17      0.27        89\n",
            "     German       0.39      0.45      0.42       217\n",
            "    Russian       0.91      0.92      0.92      2823\n",
            "      Irish       0.58      0.16      0.25        70\n",
            "    Spanish       0.30      0.35      0.32        89\n",
            "    Italian       0.62      0.66      0.64       213\n",
            " Portuguese       0.00      0.00      0.00        22\n",
            "      Greek       0.70      0.46      0.55        61\n",
            " Vietnamese       0.10      0.05      0.06        22\n",
            "     French       0.56      0.12      0.20        83\n",
            "   Japanese       0.77      0.82      0.79       297\n",
            "     Polish       0.57      0.40      0.47        42\n",
            "     Korean       0.29      0.07      0.11        28\n",
            "    English       0.69      0.79      0.74      1101\n",
            "      Czech       0.31      0.21      0.25       156\n",
            "   Scottish       0.00      0.00      0.00        30\n",
            "\n",
            "avg / total       0.78      0.79      0.77      6023\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "KaTGhLm9PgXn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Добавьте Dropout до LSTM (а можно и после). Адекватным будет значение порядка 0.3."
      ]
    },
    {
      "metadata": {
        "id": "6nXzmlGHPiZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DropoutLSTMSurnamesClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, lstm_hidden_dim, classes_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._dropout = nn.Dropout(0.3)\n",
        "        self._lstm = nn.LSTM(emb_dim, lstm_hidden_dim)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, classes_count)\n",
        "            \n",
        "    def forward(self, inputs):\n",
        "        'embed(inputs) -> prediction'\n",
        "        return self._out_layer(self.embed(inputs))\n",
        "    \n",
        "    def embed(self, inputs):\n",
        "        'inputs -> word embedding'\n",
        "        output, (h_n, c_n) = self._lstm(self._dropout(self._emb(inputs)))\n",
        "        return h_n[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRpOiz_bPlP8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "624fe400-297d-464f-a9a6-34f265321400"
      },
      "cell_type": "code",
      "source": [
        "model = DropoutLSTMSurnamesClassifier(vocab_size=len(char2ind), emb_dim=16, lstm_hidden_dim=64, classes_count=len(lang2ind)).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, epochs_count=100, batch_size=128, train_data=(data_train, labels_train),\n",
        "    val_data=(data_test, labels_test), val_batch_size=512)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 100, Epoch Time = 1.33s: Train Loss = 1.9672, Val Loss = 1.6947\n",
            "Epoch 2 / 100, Epoch Time = 1.45s: Train Loss = 1.6278, Val Loss = 1.5232\n",
            "Epoch 3 / 100, Epoch Time = 1.42s: Train Loss = 1.5240, Val Loss = 1.4576\n",
            "Epoch 4 / 100, Epoch Time = 1.38s: Train Loss = 1.4460, Val Loss = 1.3303\n",
            "Epoch 5 / 100, Epoch Time = 1.37s: Train Loss = 1.3778, Val Loss = 1.3173\n",
            "Epoch 6 / 100, Epoch Time = 1.35s: Train Loss = 1.3204, Val Loss = 1.2310\n",
            "Epoch 7 / 100, Epoch Time = 1.40s: Train Loss = 1.2729, Val Loss = 1.2340\n",
            "Epoch 8 / 100, Epoch Time = 1.49s: Train Loss = 1.2411, Val Loss = 1.1323\n",
            "Epoch 9 / 100, Epoch Time = 1.46s: Train Loss = 1.1864, Val Loss = 1.1030\n",
            "Epoch 10 / 100, Epoch Time = 1.41s: Train Loss = 1.1526, Val Loss = 1.0672\n",
            "Epoch 11 / 100, Epoch Time = 1.83s: Train Loss = 1.1158, Val Loss = 1.0331\n",
            "Epoch 12 / 100, Epoch Time = 1.61s: Train Loss = 1.0938, Val Loss = 1.0106\n",
            "Epoch 13 / 100, Epoch Time = 1.60s: Train Loss = 1.0644, Val Loss = 0.9750\n",
            "Epoch 14 / 100, Epoch Time = 1.45s: Train Loss = 1.0500, Val Loss = 0.9590\n",
            "Epoch 15 / 100, Epoch Time = 1.56s: Train Loss = 1.0140, Val Loss = 0.9449\n",
            "Epoch 16 / 100, Epoch Time = 1.56s: Train Loss = 1.0065, Val Loss = 0.9236\n",
            "Epoch 17 / 100, Epoch Time = 1.49s: Train Loss = 0.9807, Val Loss = 0.8995\n",
            "Epoch 18 / 100, Epoch Time = 1.60s: Train Loss = 0.9637, Val Loss = 0.8993\n",
            "Epoch 19 / 100, Epoch Time = 1.42s: Train Loss = 0.9431, Val Loss = 0.8557\n",
            "Epoch 20 / 100, Epoch Time = 1.51s: Train Loss = 0.9222, Val Loss = 0.8618\n",
            "Epoch 21 / 100, Epoch Time = 1.56s: Train Loss = 0.9188, Val Loss = 0.8913\n",
            "Epoch 22 / 100, Epoch Time = 1.58s: Train Loss = 0.8986, Val Loss = 0.8433\n",
            "Epoch 23 / 100, Epoch Time = 1.68s: Train Loss = 0.8913, Val Loss = 0.8266\n",
            "Epoch 24 / 100, Epoch Time = 1.56s: Train Loss = 0.8796, Val Loss = 0.8168\n",
            "Epoch 25 / 100, Epoch Time = 1.62s: Train Loss = 0.8567, Val Loss = 0.8312\n",
            "Epoch 26 / 100, Epoch Time = 1.50s: Train Loss = 0.8553, Val Loss = 0.7985\n",
            "Epoch 27 / 100, Epoch Time = 1.63s: Train Loss = 0.8429, Val Loss = 0.8010\n",
            "Epoch 28 / 100, Epoch Time = 1.38s: Train Loss = 0.8304, Val Loss = 0.7802\n",
            "Epoch 29 / 100, Epoch Time = 1.42s: Train Loss = 0.8214, Val Loss = 0.7767\n",
            "Epoch 30 / 100, Epoch Time = 1.35s: Train Loss = 0.8106, Val Loss = 0.7572\n",
            "Epoch 31 / 100, Epoch Time = 1.41s: Train Loss = 0.8022, Val Loss = 0.7710\n",
            "Epoch 32 / 100, Epoch Time = 1.57s: Train Loss = 0.7941, Val Loss = 0.7548\n",
            "Epoch 33 / 100, Epoch Time = 1.46s: Train Loss = 0.7845, Val Loss = 0.7470\n",
            "Epoch 34 / 100, Epoch Time = 1.49s: Train Loss = 0.7806, Val Loss = 0.7230\n",
            "Epoch 35 / 100, Epoch Time = 1.52s: Train Loss = 0.7646, Val Loss = 0.7326\n",
            "Epoch 36 / 100, Epoch Time = 1.53s: Train Loss = 0.7649, Val Loss = 0.7482\n",
            "Epoch 37 / 100, Epoch Time = 1.58s: Train Loss = 0.7545, Val Loss = 0.7227\n",
            "Epoch 38 / 100, Epoch Time = 1.44s: Train Loss = 0.7575, Val Loss = 0.7083\n",
            "Epoch 39 / 100, Epoch Time = 1.50s: Train Loss = 0.7384, Val Loss = 0.7101\n",
            "Epoch 40 / 100, Epoch Time = 1.40s: Train Loss = 0.7418, Val Loss = 0.7134\n",
            "Epoch 41 / 100, Epoch Time = 1.40s: Train Loss = 0.7358, Val Loss = 0.7013\n",
            "Epoch 42 / 100, Epoch Time = 1.47s: Train Loss = 0.7307, Val Loss = 0.6947\n",
            "Epoch 43 / 100, Epoch Time = 1.45s: Train Loss = 0.7184, Val Loss = 0.7120\n",
            "Epoch 44 / 100, Epoch Time = 1.38s: Train Loss = 0.7191, Val Loss = 0.7068\n",
            "Epoch 45 / 100, Epoch Time = 1.41s: Train Loss = 0.7103, Val Loss = 0.7054\n",
            "Epoch 46 / 100, Epoch Time = 1.45s: Train Loss = 0.7060, Val Loss = 0.6896\n",
            "Epoch 47 / 100, Epoch Time = 1.44s: Train Loss = 0.7013, Val Loss = 0.6717\n",
            "Epoch 48 / 100, Epoch Time = 1.52s: Train Loss = 0.6951, Val Loss = 0.6745\n",
            "Epoch 49 / 100, Epoch Time = 1.48s: Train Loss = 0.6959, Val Loss = 0.6957\n",
            "Epoch 50 / 100, Epoch Time = 1.47s: Train Loss = 0.6877, Val Loss = 0.6855\n",
            "Epoch 51 / 100, Epoch Time = 1.56s: Train Loss = 0.6858, Val Loss = 0.6743\n",
            "Epoch 52 / 100, Epoch Time = 1.47s: Train Loss = 0.6813, Val Loss = 0.6818\n",
            "Epoch 53 / 100, Epoch Time = 1.49s: Train Loss = 0.6753, Val Loss = 0.6646\n",
            "Epoch 54 / 100, Epoch Time = 1.42s: Train Loss = 0.6594, Val Loss = 0.6640\n",
            "Epoch 55 / 100, Epoch Time = 1.41s: Train Loss = 0.6602, Val Loss = 0.6708\n",
            "Epoch 56 / 100, Epoch Time = 1.46s: Train Loss = 0.6649, Val Loss = 0.6700\n",
            "Epoch 57 / 100, Epoch Time = 1.43s: Train Loss = 0.6543, Val Loss = 0.6882\n",
            "Epoch 58 / 100, Epoch Time = 1.45s: Train Loss = 0.6609, Val Loss = 0.6574\n",
            "Epoch 59 / 100, Epoch Time = 1.37s: Train Loss = 0.6523, Val Loss = 0.6632\n",
            "Epoch 60 / 100, Epoch Time = 1.38s: Train Loss = 0.6490, Val Loss = 0.6823\n",
            "Epoch 61 / 100, Epoch Time = 1.47s: Train Loss = 0.6425, Val Loss = 0.6534\n",
            "Epoch 62 / 100, Epoch Time = 1.40s: Train Loss = 0.6504, Val Loss = 0.6526\n",
            "Epoch 63 / 100, Epoch Time = 1.46s: Train Loss = 0.6396, Val Loss = 0.6696\n",
            "Epoch 64 / 100, Epoch Time = 1.45s: Train Loss = 0.6395, Val Loss = 0.6624\n",
            "Epoch 65 / 100, Epoch Time = 1.66s: Train Loss = 0.6308, Val Loss = 0.6582\n",
            "Epoch 66 / 100, Epoch Time = 1.49s: Train Loss = 0.6314, Val Loss = 0.6594\n",
            "Epoch 67 / 100, Epoch Time = 1.46s: Train Loss = 0.6261, Val Loss = 0.6496\n",
            "Epoch 68 / 100, Epoch Time = 1.45s: Train Loss = 0.6271, Val Loss = 0.6582\n",
            "Epoch 69 / 100, Epoch Time = 1.45s: Train Loss = 0.6176, Val Loss = 0.6756\n",
            "Epoch 70 / 100, Epoch Time = 1.59s: Train Loss = 0.6222, Val Loss = 0.6716\n",
            "Epoch 71 / 100, Epoch Time = 1.39s: Train Loss = 0.6192, Val Loss = 0.6573\n",
            "Epoch 72 / 100, Epoch Time = 1.41s: Train Loss = 0.6055, Val Loss = 0.6495\n",
            "Epoch 73 / 100, Epoch Time = 1.38s: Train Loss = 0.6186, Val Loss = 0.6473\n",
            "Epoch 74 / 100, Epoch Time = 1.42s: Train Loss = 0.6084, Val Loss = 0.6355\n",
            "Epoch 75 / 100, Epoch Time = 1.45s: Train Loss = 0.6096, Val Loss = 0.6360\n",
            "Epoch 76 / 100, Epoch Time = 1.41s: Train Loss = 0.6038, Val Loss = 0.6383\n",
            "Epoch 77 / 100, Epoch Time = 1.38s: Train Loss = 0.6036, Val Loss = 0.6534\n",
            "Epoch 78 / 100, Epoch Time = 1.39s: Train Loss = 0.5963, Val Loss = 0.6301\n",
            "Epoch 79 / 100, Epoch Time = 1.40s: Train Loss = 0.6017, Val Loss = 0.6199\n",
            "Epoch 80 / 100, Epoch Time = 1.55s: Train Loss = 0.5870, Val Loss = 0.6253\n",
            "Epoch 81 / 100, Epoch Time = 1.58s: Train Loss = 0.5996, Val Loss = 0.6272\n",
            "Epoch 82 / 100, Epoch Time = 1.57s: Train Loss = 0.5861, Val Loss = 0.6333\n",
            "Epoch 83 / 100, Epoch Time = 1.55s: Train Loss = 0.5836, Val Loss = 0.6307\n",
            "Epoch 84 / 100, Epoch Time = 1.66s: Train Loss = 0.5890, Val Loss = 0.6357\n",
            "Epoch 85 / 100, Epoch Time = 1.60s: Train Loss = 0.5840, Val Loss = 0.6324\n",
            "Epoch 86 / 100, Epoch Time = 1.60s: Train Loss = 0.5810, Val Loss = 0.6445\n",
            "Epoch 87 / 100, Epoch Time = 1.48s: Train Loss = 0.5734, Val Loss = 0.6332\n",
            "Epoch 88 / 100, Epoch Time = 1.51s: Train Loss = 0.5812, Val Loss = 0.6336\n",
            "Epoch 89 / 100, Epoch Time = 1.54s: Train Loss = 0.5793, Val Loss = 0.6403\n",
            "Epoch 90 / 100, Epoch Time = 1.41s: Train Loss = 0.5771, Val Loss = 0.6422\n",
            "Epoch 91 / 100, Epoch Time = 1.39s: Train Loss = 0.5712, Val Loss = 0.6389\n",
            "Epoch 92 / 100, Epoch Time = 1.38s: Train Loss = 0.5579, Val Loss = 0.6313\n",
            "Epoch 93 / 100, Epoch Time = 1.41s: Train Loss = 0.5688, Val Loss = 0.6308\n",
            "Epoch 94 / 100, Epoch Time = 1.51s: Train Loss = 0.5675, Val Loss = 0.6260\n",
            "Epoch 95 / 100, Epoch Time = 1.36s: Train Loss = 0.5601, Val Loss = 0.6174\n",
            "Epoch 96 / 100, Epoch Time = 1.41s: Train Loss = 0.5613, Val Loss = 0.6286\n",
            "Epoch 97 / 100, Epoch Time = 1.37s: Train Loss = 0.5589, Val Loss = 0.6211\n",
            "Epoch 98 / 100, Epoch Time = 1.44s: Train Loss = 0.5570, Val Loss = 0.6319\n",
            "Epoch 99 / 100, Epoch Time = 1.57s: Train Loss = 0.5666, Val Loss = 0.6406\n",
            "Epoch 100 / 100, Epoch Time = 1.60s: Train Loss = 0.5496, Val Loss = 0.6264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vgBB6VWWQXcX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "9a6b9355-c463-4676-b758-62eddd2ad18c"
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "X_test, y_test = next(iterate_batches(data_test, labels_test, batch_size=len(data_test)))\n",
        "X_test = LongTensor(X_test)\n",
        "y_pred = torch.softmax(model(X_test), 1).argmax(1)\n",
        "\n",
        "print('Accuracy = {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test, y_pred, \n",
        "                            target_names=[lang for lang, _ in sorted(lang2ind.items(), key=lambda x: x[1])]))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 81.07%\n",
            "Classification report:\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    Chinese       0.48      0.76      0.59        80\n",
            "     Arabic       0.90      1.00      0.95       600\n",
            "      Dutch       0.46      0.20      0.28        89\n",
            "     German       0.51      0.40      0.45       217\n",
            "    Russian       0.92      0.93      0.93      2823\n",
            "      Irish       0.48      0.17      0.25        70\n",
            "    Spanish       0.41      0.25      0.31        89\n",
            "    Italian       0.65      0.77      0.71       213\n",
            " Portuguese       1.00      0.05      0.09        22\n",
            "      Greek       0.67      0.72      0.69        61\n",
            " Vietnamese       0.00      0.00      0.00        22\n",
            "     French       0.41      0.16      0.23        83\n",
            "   Japanese       0.81      0.89      0.85       297\n",
            "     Polish       0.52      0.33      0.41        42\n",
            "     Korean       0.20      0.11      0.14        28\n",
            "    English       0.71      0.82      0.76      1101\n",
            "      Czech       0.55      0.24      0.34       156\n",
            "   Scottish       0.00      0.00      0.00        30\n",
            "\n",
            "avg / total       0.79      0.81      0.79      6023\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Sys5NQ9PikJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Важным видом RNN является Bidirectional RNN. По сути это две RNN, одна обходит последовательность слева направо, вторая - наоборот. \n",
        "\n",
        "В результате для каждого момента времени у нас есть вектор $h_t = [f_t; b_t]$ - конкатенация (или какая-то ещё функция от $f_t$ и $b_t$) состояний $f_t$ и $b_t$ - прямого и обратного прохода последовательности. В сумме они покрывают весь контекст.\n",
        "\n",
        "В нашей задаче Bidirectional вариант может помочь тем, что сеть будет меньше забывать, с чего начиналась последовательность. То есть нам нужно будет взять $f_N$ и $b_N$ состояния: первое - последнее состояние в проходе слева направо, т.е. выход от последнего символа. Второе - последнее состояние при обратно проходе, т.е. выход для первого символа.\n",
        "\n",
        "Реализуйте Bidirectional классификатор. Для этого в `LSTM` есть параметр `bidirectional`."
      ]
    },
    {
      "metadata": {
        "id": "Qp30bXiLSsn_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiDropoutLSTMSurnamesClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, lstm_hidden_dim, classes_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._dropout = nn.Dropout(0.4)\n",
        "        self._lstm = nn.LSTM(emb_dim, lstm_hidden_dim, bidirectional=True)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, classes_count)\n",
        "            \n",
        "    def forward(self, inputs):\n",
        "        'embed(inputs) -> prediction'\n",
        "        return self._out_layer(self.embed(inputs))\n",
        "    \n",
        "    def embed(self, inputs):\n",
        "        'inputs -> word embedding'\n",
        "        output, (h_n, c_n) = self._lstm(self._dropout(self._emb(inputs)))\n",
        "        return h_n[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2gUeJYoSwRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "867710a7-fd24-4085-c059-5de944788a52"
      },
      "cell_type": "code",
      "source": [
        "model = BiDropoutLSTMSurnamesClassifier(vocab_size=len(char2ind), emb_dim=128, lstm_hidden_dim=256, classes_count=len(lang2ind)).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, epochs_count=15, batch_size=128, train_data=(data_train, labels_train),\n",
        "    val_data=(data_test, labels_test), val_batch_size=512)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 15, Epoch Time = 2.30s: Train Loss = 1.6363, Val Loss = 1.3234\n",
            "Epoch 2 / 15, Epoch Time = 2.17s: Train Loss = 1.2146, Val Loss = 1.0210\n",
            "Epoch 3 / 15, Epoch Time = 2.33s: Train Loss = 0.9676, Val Loss = 0.8398\n",
            "Epoch 4 / 15, Epoch Time = 2.19s: Train Loss = 0.8183, Val Loss = 0.7760\n",
            "Epoch 5 / 15, Epoch Time = 2.19s: Train Loss = 0.7229, Val Loss = 0.6761\n",
            "Epoch 6 / 15, Epoch Time = 2.20s: Train Loss = 0.6575, Val Loss = 0.6734\n",
            "Epoch 7 / 15, Epoch Time = 2.04s: Train Loss = 0.5930, Val Loss = 0.6106\n",
            "Epoch 8 / 15, Epoch Time = 2.03s: Train Loss = 0.5494, Val Loss = 0.6013\n",
            "Epoch 9 / 15, Epoch Time = 1.97s: Train Loss = 0.5168, Val Loss = 0.5813\n",
            "Epoch 10 / 15, Epoch Time = 1.95s: Train Loss = 0.4825, Val Loss = 0.5637\n",
            "Epoch 11 / 15, Epoch Time = 2.00s: Train Loss = 0.4477, Val Loss = 0.5803\n",
            "Epoch 12 / 15, Epoch Time = 2.05s: Train Loss = 0.4230, Val Loss = 0.5733\n",
            "Epoch 13 / 15, Epoch Time = 2.15s: Train Loss = 0.3947, Val Loss = 0.5896\n",
            "Epoch 14 / 15, Epoch Time = 2.07s: Train Loss = 0.3812, Val Loss = 0.5572\n",
            "Epoch 15 / 15, Epoch Time = 2.07s: Train Loss = 0.3554, Val Loss = 0.5940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fOk_jjmmSy5O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "ab7f5a7c-883f-4dde-a826-7b7bc831d4c7"
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "X_test, y_test = next(iterate_batches(data_test, labels_test, batch_size=len(data_test)))\n",
        "X_test = LongTensor(X_test)\n",
        "y_pred = torch.softmax(model(X_test), 1).argmax(1)\n",
        "\n",
        "print('Accuracy = {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test, y_pred, \n",
        "                            target_names=[lang for lang, _ in sorted(lang2ind.items(), key=lambda x: x[1])]))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 82.95%\n",
            "Classification report:\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    Chinese       0.56      0.68      0.61        80\n",
            "     Arabic       0.93      1.00      0.96       600\n",
            "      Dutch       0.54      0.42      0.47        89\n",
            "     German       0.51      0.57      0.54       217\n",
            "    Russian       0.94      0.94      0.94      2823\n",
            "      Irish       0.55      0.43      0.48        70\n",
            "    Spanish       0.41      0.28      0.33        89\n",
            "    Italian       0.68      0.75      0.71       213\n",
            " Portuguese       0.00      0.00      0.00        22\n",
            "      Greek       0.69      0.67      0.68        61\n",
            " Vietnamese       0.50      0.18      0.27        22\n",
            "     French       0.52      0.16      0.24        83\n",
            "   Japanese       0.86      0.90      0.88       297\n",
            "     Polish       0.58      0.33      0.42        42\n",
            "     Korean       0.36      0.14      0.21        28\n",
            "    English       0.73      0.83      0.78      1101\n",
            "      Czech       0.57      0.33      0.42       156\n",
            "   Scottish       0.00      0.00      0.00        30\n",
            "\n",
            "avg / total       0.81      0.83      0.82      6023\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ES8F8nISfuL7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiDropoutLSTMSurnamesClassifierStar(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, lstm_hidden_dim, classes_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._dropout = nn.Dropout(0.5)\n",
        "        self._lstm = nn.LSTM(emb_dim, lstm_hidden_dim, bidirectional=True)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, classes_count)\n",
        "            \n",
        "    def forward(self, inputs):\n",
        "        'embed(inputs) -> prediction'\n",
        "        return self._out_layer(self.embed(inputs))\n",
        "    \n",
        "    def embed(self, inputs):\n",
        "        'inputs -> word embedding'\n",
        "        output, (h_n, c_n) = self._lstm(self._dropout(self._emb(inputs)))\n",
        "        return h_n[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hHqMLO3vfuyj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "e46c44d7-fe2b-44d7-d6fc-fc2cc536d69f"
      },
      "cell_type": "code",
      "source": [
        "model = BiDropoutLSTMSurnamesClassifierStar(vocab_size=len(char2ind), emb_dim=128, lstm_hidden_dim=256, classes_count=len(lang2ind)).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, epochs_count=15, batch_size=128, train_data=(data_train, labels_train),\n",
        "    val_data=(data_test, labels_test), val_batch_size=512)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 / 15, Epoch Time = 2.57s: Train Loss = 1.6845, Val Loss = 1.3675\n",
            "Epoch 2 / 15, Epoch Time = 2.56s: Train Loss = 1.2469, Val Loss = 1.0375\n",
            "Epoch 3 / 15, Epoch Time = 2.67s: Train Loss = 1.0030, Val Loss = 0.8737\n",
            "Epoch 4 / 15, Epoch Time = 2.47s: Train Loss = 0.8603, Val Loss = 0.7834\n",
            "Epoch 5 / 15, Epoch Time = 2.61s: Train Loss = 0.7698, Val Loss = 0.7310\n",
            "Epoch 6 / 15, Epoch Time = 2.65s: Train Loss = 0.6933, Val Loss = 0.6919\n",
            "Epoch 7 / 15, Epoch Time = 2.58s: Train Loss = 0.6361, Val Loss = 0.6397\n",
            "Epoch 8 / 15, Epoch Time = 2.72s: Train Loss = 0.5989, Val Loss = 0.6114\n",
            "Epoch 9 / 15, Epoch Time = 2.59s: Train Loss = 0.5552, Val Loss = 0.6033\n",
            "Epoch 10 / 15, Epoch Time = 2.56s: Train Loss = 0.5189, Val Loss = 0.5899\n",
            "Epoch 11 / 15, Epoch Time = 2.54s: Train Loss = 0.4941, Val Loss = 0.5860\n",
            "Epoch 12 / 15, Epoch Time = 2.35s: Train Loss = 0.4675, Val Loss = 0.5821\n",
            "Epoch 13 / 15, Epoch Time = 2.42s: Train Loss = 0.4415, Val Loss = 0.5846\n",
            "Epoch 14 / 15, Epoch Time = 2.32s: Train Loss = 0.4254, Val Loss = 0.5690\n",
            "Epoch 15 / 15, Epoch Time = 2.36s: Train Loss = 0.3963, Val Loss = 0.5576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Af0k_2OffwMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "6e0d177a-80fb-43f8-992f-5b94de8343bc"
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "X_test, y_test = next(iterate_batches(data_test, labels_test, batch_size=len(data_test)))\n",
        "X_test = LongTensor(X_test)\n",
        "y_pred = torch.softmax(model(X_test), 1).argmax(1)\n",
        "\n",
        "print('Accuracy = {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test, y_pred, \n",
        "                            target_names=[lang for lang, _ in sorted(lang2ind.items(), key=lambda x: x[1])]))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 83.53%\n",
            "Classification report:\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "    Chinese       0.47      0.93      0.62        80\n",
            "     Arabic       0.96      1.00      0.98       600\n",
            "      Dutch       0.56      0.38      0.45        89\n",
            "     German       0.59      0.55      0.57       217\n",
            "    Russian       0.95      0.94      0.94      2823\n",
            "      Irish       0.62      0.44      0.52        70\n",
            "    Spanish       0.38      0.35      0.36        89\n",
            "    Italian       0.72      0.73      0.72       213\n",
            " Portuguese       0.14      0.05      0.07        22\n",
            "      Greek       0.79      0.69      0.74        61\n",
            " Vietnamese       0.20      0.05      0.07        22\n",
            "     French       0.66      0.23      0.34        83\n",
            "   Japanese       0.88      0.88      0.88       297\n",
            "     Polish       0.57      0.31      0.40        42\n",
            "     Korean       1.00      0.04      0.07        28\n",
            "    English       0.74      0.84      0.79      1101\n",
            "      Czech       0.45      0.42      0.43       156\n",
            "   Scottish       0.00      0.00      0.00        30\n",
            "\n",
            "avg / total       0.83      0.84      0.83      6023\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LgiTUH8MfKFt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Перебрав все параметры, которые только было можно, я смог получить модель `BiDropoutLSTMSurnamesClassifierStar`, точность которой на 0.08 выше, чем у `LogisticRegression`.*"
      ]
    },
    {
      "metadata": {
        "id": "gdMatMdGKq9X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача задания\n",
        "\n",
        "[Опрос](https://goo.gl/forms/6d04Bkk36mVpBYt32)"
      ]
    }
  ]
}