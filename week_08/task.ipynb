{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week 08 - Language Models (Part 2).ipynb","version":"0.3.2","provenance":[{"file_id":"1lUlBsdvAYJc5rLHwkOICyFhvns5Ssp1X","timestamp":1540884619516},{"file_id":"1W5uaNpKFoaq1gV9N9FpIAEDyrsGGRBBi","timestamp":1540140192448}],"collapsed_sections":["w8V0KAz_CNf0","Vwb5e5hPQebd"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"OE7fXh-OSJYF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":65},"outputId":"bac08db9-3a83-4e33-c065-b6fb32bc87f0","executionInfo":{"status":"ok","timestamp":1542206650309,"user_tz":-180,"elapsed":20281,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["!pip3 -qq install torch==0.4.1\n","!pip -qq install torchtext==0.3.1\n","!pip -qq install gensim==3.6.0\n","!pip -qq install pyldavis==2.1.2\n","!pip -qq install attrs==18.2.0\n","!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1OIU9ICMebvZXJ0Grc2SLlMep3x9EkZtz' -O perashki.txt\n","!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1v66uAEKL3KunyylYitNKggdl2gCeYgZZ' -O poroshki.txt\n","!git clone https://github.com/UniversalDependencies/UD_Russian-SynTagRus.git\n","!wget -qq https://raw.githubusercontent.com/DanAnastasyev/neuromorphy/master/neuromorphy/train/corpus_iterator.py"],"execution_count":28,"outputs":[{"output_type":"stream","text":["fatal: destination path 'UD_Russian-SynTagRus' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"metadata":{"id":"uhvfH55PUJ8K","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","if torch.cuda.is_available():\n","    from torch.cuda import FloatTensor, LongTensor\n","    DEVICE = torch.device('cuda')\n","else:\n","    from torch import FloatTensor, LongTensor\n","    DEVICE = torch.device('cpu')\n","\n","np.random.seed(42)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"txWqIO_74A4s","colab_type":"text"},"cell_type":"markdown","source":["# Word-Level Text Generation"]},{"metadata":{"id":"KOD_3I7d4oDV","colab_type":"text"},"cell_type":"markdown","source":["Сегодня занимаемся, в основном, тем, что генерируем *пирожки* и *порошки*.\n","\n","*(Данные без спросу скачаны с сайта http://poetory.ru)*\n","\n","Пирожки - это вот:"]},{"metadata":{"id":"d2vMrlrRQpuJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"outputId":"ef964efc-62eb-40c8-b19b-a3fc4cd337c3","executionInfo":{"status":"ok","timestamp":1542206653416,"user_tz":-180,"elapsed":2324,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["!head perashki.txt"],"execution_count":30,"outputs":[{"output_type":"stream","text":["старик вытягивает сети\r\n","они пусты и лишь в конце\r\n","записка рыба недоступна\r\n","или вне действия сети\r\n","\r\n","олег адепт шизофрении\r\n","шагает бодро из окна\r\n","и жызнь летит перед глазами\r\n","да не одна а сразу две\r\n","\r\n"],"name":"stdout"}]},{"metadata":{"id":"0Lm0-PeG5Dh9","colab_type":"text"},"cell_type":"markdown","source":["Порошки вот:"]},{"metadata":{"id":"2-Jf88bxVTGj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"outputId":"6437400f-b9e6-4ca4-a12f-6b02657a8780","executionInfo":{"status":"ok","timestamp":1542206655567,"user_tz":-180,"elapsed":2104,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["!head poroshki.txt"],"execution_count":31,"outputs":[{"output_type":"stream","text":["кто любит цоя кто покушать\r\n","кто на рассвете пенье птиц\r\n","а я люблю в коротких платьях\r\n","физлиц\r\n","\r\n","твой монолог так гениален\r\n","что я чуть не открыла дверь\r\n","но станиславский тихо сверху\r\n","не верь\r\n","\r\n"],"name":"stdout"}]},{"metadata":{"id":"AgYh4FNP5FyX","colab_type":"text"},"cell_type":"markdown","source":["Не перепутайте!\n","\n","Вообще, пирожок - это четверостишие, написанное четырехстопным ямбом по схеме 9-8-9-8. У порошка схема 9-8-9-2."]},{"metadata":{"id":"bSBpLFRgGaXS","colab_type":"code","colab":{}},"cell_type":"code","source":["vowels = 'ёуеыаоэяию'\n","\n","odd_pattern = '-+-+-+-+-'\n","even_pattern = '-+-+-+-+'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Hl9BFoug519c","colab_type":"text"},"cell_type":"markdown","source":["Считываем данные:"]},{"metadata":{"id":"O3aFzzOQKLlD","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_poem(path):\n","    poem = []\n","    with open(path, encoding='utf8') as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if len(line) == 0:\n","                yield poem\n","                poem = []\n","                continue\n","            \n","            poem.extend(line.split() + ['\\\\n'])\n","            \n","perashki = list(read_poem('perashki.txt'))\n","poroshki = list(read_poem('poroshki.txt'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xiRq1vbf55qN","colab_type":"text"},"cell_type":"markdown","source":["Построим датасет для порошков:"]},{"metadata":{"id":"ZOBgLAgVTrk1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d6d947f3-e15c-43b0-8df9-f5761d40bd70","executionInfo":{"status":"ok","timestamp":1542182397633,"user_tz":-180,"elapsed":1215,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["from torchtext.data import Field, Example, Dataset, BucketIterator\n","\n","text_field = Field(init_token='<s>', eos_token='</s>')\n","        \n","fields = [('text', text_field)]\n","examples = [Example.fromlist([poem], fields) for poem in poroshki]\n","dataset = Dataset(examples, fields)\n","\n","text_field.build_vocab(dataset, min_freq=7)\n","\n","print('Vocab size =', len(text_field.vocab))\n","train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n","\n","train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n","                                              shuffle=True, device=DEVICE, sort=False)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Vocab size = 6298\n"],"name":"stdout"}]},{"metadata":{"id":"8FYJe2CA8GcY","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Напишите класс языковой модели."]},{"metadata":{"id":"x8ndCRZLl4ZZ","colab_type":"code","colab":{}},"cell_type":"code","source":["class LMModel(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=256, lstm_hidden_dim=256, num_layers=1):\n","        super().__init__()\n","\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n","        \n","        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n","        \n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._emb.weight.data.uniform_(-init_range, init_range)\n","        self._out_layer.bias.data.zero_()\n","        self._out_layer.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, inputs, hidden=None):\n","        embs = self._emb(inputs)\n","        output, hidden = self._rnn(embs, hidden)\n","        output = self._out_layer(output)\n","        return output, hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ySJ4tUAqvFvB","colab_type":"code","colab":{}},"cell_type":"code","source":["batch = next(iter(train_iter))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5_qVuSL8QJg4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1343},"outputId":"4ca38a6f-9eba-4993-e17d-b55c94c46a2f","executionInfo":{"status":"ok","timestamp":1542182403268,"user_tz":-180,"elapsed":891,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","model(batch.text)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[ 0.0251, -0.0173,  0.0144,  ...,  0.0027, -0.0017,  0.0269],\n","          [ 0.0251, -0.0173,  0.0144,  ...,  0.0027, -0.0017,  0.0269],\n","          [ 0.0251, -0.0173,  0.0144,  ...,  0.0027, -0.0017,  0.0269],\n","          ...,\n","          [ 0.0251, -0.0173,  0.0144,  ...,  0.0027, -0.0017,  0.0269],\n","          [ 0.0251, -0.0173,  0.0144,  ...,  0.0027, -0.0017,  0.0269],\n","          [ 0.0251, -0.0173,  0.0144,  ...,  0.0027, -0.0017,  0.0269]],\n"," \n","         [[ 0.0423, -0.0151,  0.0136,  ...,  0.0002, -0.0152,  0.0473],\n","          [ 0.0144, -0.0198,  0.0121,  ...,  0.0039, -0.0109,  0.0292],\n","          [ 0.0181, -0.0206,  0.0079,  ..., -0.0143, -0.0073,  0.0349],\n","          ...,\n","          [ 0.0201, -0.0149,  0.0221,  ...,  0.0041, -0.0131,  0.0272],\n","          [ 0.0144, -0.0198,  0.0121,  ...,  0.0039, -0.0109,  0.0292],\n","          [ 0.0164, -0.0234,  0.0136,  ...,  0.0071, -0.0102,  0.0373]],\n"," \n","         [[ 0.0378, -0.0156,  0.0098,  ...,  0.0131, -0.0087,  0.0434],\n","          [ 0.0105, -0.0214,  0.0112,  ...,  0.0065, -0.0147,  0.0297],\n","          [ 0.0272, -0.0087,  0.0162,  ..., -0.0115, -0.0094,  0.0422],\n","          ...,\n","          [ 0.0271, -0.0222,  0.0095,  ...,  0.0050,  0.0049,  0.0295],\n","          [ 0.0205, -0.0221,  0.0206,  ...,  0.0057, -0.0261,  0.0271],\n","          [ 0.0201, -0.0144,  0.0105,  ...,  0.0165, -0.0078,  0.0387]],\n"," \n","         ...,\n"," \n","         [[ 0.0163, -0.0105,  0.0246,  ..., -0.0018, -0.0227,  0.0249],\n","          [ 0.0162, -0.0108,  0.0246,  ..., -0.0024, -0.0229,  0.0244],\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0025, -0.0230,  0.0243],\n","          ...,\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0026, -0.0230,  0.0243],\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0026, -0.0230,  0.0243],\n","          [ 0.0165, -0.0105,  0.0249,  ..., -0.0014, -0.0225,  0.0257]],\n"," \n","         [[ 0.0162, -0.0107,  0.0246,  ..., -0.0021, -0.0228,  0.0246],\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0025, -0.0229,  0.0243],\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0026, -0.0230,  0.0243],\n","          ...,\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0027, -0.0230,  0.0243],\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0026, -0.0230,  0.0243],\n","          [ 0.0163, -0.0106,  0.0247,  ..., -0.0018, -0.0227,  0.0250]],\n"," \n","         [[ 0.0162, -0.0108,  0.0245,  ..., -0.0023, -0.0229,  0.0244],\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0026, -0.0230,  0.0243],\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0026, -0.0230,  0.0243],\n","          ...,\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0027, -0.0229,  0.0243],\n","          [ 0.0162, -0.0109,  0.0246,  ..., -0.0027, -0.0230,  0.0243],\n","          [ 0.0162, -0.0107,  0.0246,  ..., -0.0021, -0.0228,  0.0247]]],\n","        device='cuda:0', grad_fn=<ThAddBackward>),\n"," (tensor([[[ 2.4030e-03,  8.3352e-03,  6.2213e-03,  ...,  6.7888e-02,\n","            -1.7733e-02,  2.0485e-02],\n","           [ 2.6845e-03,  8.6018e-03,  6.1663e-03,  ...,  6.7885e-02,\n","            -1.7646e-02,  2.0332e-02],\n","           [ 2.7454e-03,  8.6503e-03,  6.1405e-03,  ...,  6.7878e-02,\n","            -1.7615e-02,  2.0333e-02],\n","           ...,\n","           [ 2.8152e-03,  8.7032e-03,  6.1344e-03,  ...,  6.7871e-02,\n","            -1.7608e-02,  2.0318e-02],\n","           [ 2.7999e-03,  8.6925e-03,  6.1374e-03,  ...,  6.7872e-02,\n","            -1.7610e-02,  2.0320e-02],\n","           [ 2.1891e-03,  8.0057e-03,  6.1273e-03,  ...,  6.7814e-02,\n","            -1.7715e-02,  2.0615e-02]]],\n","         device='cuda:0', grad_fn=<CudnnRnnBackward>),\n","  tensor([[[ 4.9986e-03,  1.6198e-02,  1.2827e-02,  ...,  1.3552e-01,\n","            -3.6214e-02,  4.0166e-02],\n","           [ 5.5843e-03,  1.6714e-02,  1.2713e-02,  ...,  1.3550e-01,\n","            -3.6038e-02,  3.9869e-02],\n","           [ 5.7110e-03,  1.6808e-02,  1.2659e-02,  ...,  1.3549e-01,\n","            -3.5975e-02,  3.9872e-02],\n","           ...,\n","           [ 5.8562e-03,  1.6911e-02,  1.2647e-02,  ...,  1.3547e-01,\n","            -3.5960e-02,  3.9844e-02],\n","           [ 5.8245e-03,  1.6890e-02,  1.2653e-02,  ...,  1.3548e-01,\n","            -3.5965e-02,  3.9847e-02],\n","           [ 4.5538e-03,  1.5559e-02,  1.2633e-02,  ...,  1.3538e-01,\n","            -3.6179e-02,  4.0419e-02]]],\n","         device='cuda:0', grad_fn=<CudnnRnnBackward>)))"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"Rsh3_eR08PqQ","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Добавьте подсчет потерей с маскингом паддингов."]},{"metadata":{"id":"_E2JxfRuphch","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):                \n","                logits, _ = model(batch.text)\n","\n","                targets = torch.cat((batch.text[1:], batch.text.new_ones((1, batch.text.shape[1]))))\n","\n","                loss = criterion(logits.view(-1, logits.shape[-1]), targets.view(-1))\n","                \n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n","            \n","            if best_val_loss and val_loss > best_val_loss:\n","                optimizer.param_groups[0]['lr'] /= 4.\n","                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n","            else:\n","                best_val_loss = val_loss\n","        print()\n","        generate(model)\n","        print()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ufpoSwQ-8bcN","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Напишите функцию-генератор для модели."]},{"metadata":{"colab_type":"code","id":"BYoHY1se2bhB","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"1ab56b04-1a47-4739-be90-99d0dd02580a","executionInfo":{"status":"ok","timestamp":1542182404972,"user_tz":-180,"elapsed":995,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["def sample(probs, temp):\n","    probs = F.log_softmax(probs.squeeze(), dim=0)\n","    probs = (probs / temp).exp()\n","    probs /= probs.sum()\n","    probs = probs.cpu().numpy()\n","\n","    return np.random.choice(np.arange(len(probs)), p=probs)\n","\n","\n","def generate(model, temp=0.6):\n","    model.eval()\n","    with torch.no_grad():        \n","        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n","        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n","        \n","        hidden = None\n","        for _ in range(150):\n","            probs, hidden = model(LongTensor([[prev_token]]), hidden)\n","            prev_token = sample(probs, temp)\n","            \n","            if prev_token == end_token:\n","                return\n","            \n","            if train_iter.dataset.fields['text'].vocab.itos[prev_token] == '\\\\n':\n","                print()\n","            else:\n","                print(train_iter.dataset.fields['text'].vocab.itos[prev_token], end=' ')\n","                \n","generate(model)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["изольда разбег тоскливо девчонки пошёл пошёл счастья влезает жалко палату стал сталин сусанин бес одним сергей навсегда толпою часов стены лучах ад страшный бюст надеюсь мин полно поцелуй визг своей кобзон ван жене пункт смены понты намного ай ищешь выбираю суть зачат прошу лягу торт будьте толпа слышен икры уста спустился ключи половиной кабинета далеко несите земля дно слова мяса сидела пришлось состоит растёт искать дыханье лень пинок довольно фома кабинет пей был пушистый отверг страстно искры сердца спорить вроде вернее останки речь тому страха морковь сук связала жёлтый список кричат порей доска подарили исаев жён строчки спас год ногами конечно рухнул эс осколки лупит забыв кляня грека узнал народ сиськи пух отправил пониже простить георгий пломбир выйти изменила грусти получился капкан голов борща ставит спальни ремня букетик кто пирог одеяло дверью кричат бок похожа кухню сантехник ножки яйцо стопарь скажыте ничей кактус змея окон лежит кружок менять пальто дрожа "],"name":"stdout"}]},{"metadata":{"id":"5X2kYDU_rCjP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4525},"outputId":"8b04748f-fb89-42dd-b4ac-8b57be7e5819","executionInfo":{"status":"ok","timestamp":1542182892379,"user_tz":-180,"elapsed":478231,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[1 / 30] Train: Loss = 5.21460, PPX = 183.94: 100%|██████████| 677/677 [00:15<00:00, 42.59it/s]\n","[1 / 30]   Val: Loss = 4.58895, PPX = 98.39: 100%|██████████| 19/19 [00:00<00:00, 44.11it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","<unk> <unk> <unk> <unk> \n","<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> "],"name":"stdout"},{"output_type":"stream","text":["[2 / 30] Train: Loss = 4.49331, PPX = 89.42:   0%|          | 3/677 [00:00<00:22, 29.99it/s] "],"name":"stderr"},{"output_type":"stream","text":["<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n","<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n"],"name":"stdout"},{"output_type":"stream","text":["[2 / 30] Train: Loss = 4.45844, PPX = 86.35: 100%|██████████| 677/677 [00:15<00:00, 43.18it/s]\n","[2 / 30]   Val: Loss = 4.03546, PPX = 56.57: 100%|██████████| 19/19 [00:00<00:00, 46.45it/s]\n","[3 / 30] Train: Loss = 4.81527, PPX = 123.38:   1%|          | 4/677 [00:00<00:19, 35.15it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","<unk> <unk> \n","<unk> <unk> в <unk> \n","<unk> <unk> <unk> <unk> <unk> \n","<unk> <unk> <unk> <unk> \n","<unk> <unk> \n","<unk> <unk> в <unk> \n","<unk> <unk> \n","<unk> <unk> <unk> \n","<unk> <unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[3 / 30] Train: Loss = 4.19409, PPX = 66.29: 100%|██████████| 677/677 [00:15<00:00, 43.85it/s]\n","[3 / 30]   Val: Loss = 4.22738, PPX = 68.54: 100%|██████████| 19/19 [00:00<00:00, 47.65it/s]\n","[4 / 30] Train: Loss = 3.79708, PPX = 44.57:   0%|          | 2/677 [00:00<00:25, 26.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5\n","\n","<unk> <unk> <unk> <unk> <unk> <unk> <unk> \n","<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n","<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n","<unk> <unk> <unk> <unk> <unk> <unk> \n","<unk> <unk> <unk> <unk> <unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[4 / 30] Train: Loss = 3.71661, PPX = 41.12: 100%|██████████| 677/677 [00:15<00:00, 43.90it/s]\n","[4 / 30]   Val: Loss = 3.80366, PPX = 44.87: 100%|██████████| 19/19 [00:00<00:00, 45.49it/s]\n","[5 / 30] Train: Loss = 3.56812, PPX = 35.45:   1%|          | 5/677 [00:00<00:22, 29.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","<unk> <unk> \n","<unk> не <unk> \n","а он <unk> не <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[5 / 30] Train: Loss = 3.66925, PPX = 39.22: 100%|██████████| 677/677 [00:15<00:00, 43.80it/s]\n","[5 / 30]   Val: Loss = 3.78065, PPX = 43.84: 100%|██████████| 19/19 [00:00<00:00, 47.51it/s]\n","[6 / 30] Train: Loss = 3.59885, PPX = 36.56:   1%|          | 5/677 [00:00<00:22, 30.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","<unk> <unk> <unk> \n","что у меня <unk> \n","<unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[6 / 30] Train: Loss = 3.62904, PPX = 37.68: 100%|██████████| 677/677 [00:15<00:00, 43.58it/s]\n","[6 / 30]   Val: Loss = 3.75809, PPX = 42.87: 100%|██████████| 19/19 [00:00<00:00, 47.99it/s]\n","[7 / 30] Train: Loss = 3.61509, PPX = 37.15:   1%|          | 5/677 [00:00<00:19, 35.21it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","<unk> <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> <unk> \n","<unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[7 / 30] Train: Loss = 3.58984, PPX = 36.23: 100%|██████████| 677/677 [00:15<00:00, 43.72it/s]\n","[7 / 30]   Val: Loss = 3.74104, PPX = 42.14: 100%|██████████| 19/19 [00:00<00:00, 48.59it/s]\n","[8 / 30] Train: Loss = 3.44260, PPX = 31.27:   1%|          | 5/677 [00:00<00:20, 32.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","<unk> <unk> на <unk> \n","и в смысле <unk> <unk> \n","а я <unk> в <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[8 / 30] Train: Loss = 3.55185, PPX = 34.88: 100%|██████████| 677/677 [00:15<00:00, 44.08it/s]\n","[8 / 30]   Val: Loss = 3.72916, PPX = 41.64: 100%|██████████| 19/19 [00:00<00:00, 44.51it/s]\n","[9 / 30] Train: Loss = 3.43469, PPX = 31.02:   1%|          | 5/677 [00:00<00:19, 34.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","<unk> <unk> \n","<unk> <unk> на <unk> \n","я <unk> до <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 30] Train: Loss = 3.51418, PPX = 33.59: 100%|██████████| 677/677 [00:15<00:00, 44.23it/s]\n","[9 / 30]   Val: Loss = 3.71864, PPX = 41.21: 100%|██████████| 19/19 [00:00<00:00, 43.43it/s]\n","[10 / 30] Train: Loss = 3.54215, PPX = 34.54:   1%|          | 5/677 [00:00<00:23, 28.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","одно <unk> <unk> \n","<unk> <unk> \n","<unk> <unk> \n","в <unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 30] Train: Loss = 3.47794, PPX = 32.39: 100%|██████████| 677/677 [00:15<00:00, 44.06it/s]\n","[10 / 30]   Val: Loss = 3.70823, PPX = 40.78: 100%|██████████| 19/19 [00:00<00:00, 44.95it/s]\n","[11 / 30] Train: Loss = 3.32011, PPX = 27.66:   1%|          | 6/677 [00:00<00:19, 34.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","а мы <unk> <unk> \n","<unk> <unk> <unk> \n","мы тут <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[11 / 30] Train: Loss = 3.44173, PPX = 31.24: 100%|██████████| 677/677 [00:15<00:00, 43.92it/s]\n","[11 / 30]   Val: Loss = 3.71000, PPX = 40.85: 100%|██████████| 19/19 [00:00<00:00, 46.48it/s]\n","[12 / 30] Train: Loss = 3.52668, PPX = 34.01:   1%|          | 5/677 [00:00<00:20, 33.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.25\n","\n","ты <unk> меня <unk> \n","и даже в <unk> <unk> \n","но вместо <unk> <unk> \n","с трудом \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 30] Train: Loss = 3.36882, PPX = 29.04: 100%|██████████| 677/677 [00:15<00:00, 44.04it/s]\n","[12 / 30]   Val: Loss = 3.69346, PPX = 40.18: 100%|██████████| 19/19 [00:00<00:00, 46.88it/s]\n","[13 / 30] Train: Loss = 3.39592, PPX = 29.84:   1%|          | 5/677 [00:00<00:23, 28.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","<unk> <unk> <unk> \n","и достаёт под <unk> \n","но по <unk> <unk> \n","в <unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 30] Train: Loss = 3.35306, PPX = 28.59: 100%|██████████| 677/677 [00:15<00:00, 43.79it/s]\n","[13 / 30]   Val: Loss = 3.69282, PPX = 40.16: 100%|██████████| 19/19 [00:00<00:00, 44.96it/s]\n","[14 / 30] Train: Loss = 3.32121, PPX = 27.69:   1%|          | 5/677 [00:00<00:22, 29.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в <unk> <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 30] Train: Loss = 3.34052, PPX = 28.23: 100%|██████████| 677/677 [00:15<00:00, 44.05it/s]\n","[14 / 30]   Val: Loss = 3.69428, PPX = 40.22: 100%|██████████| 19/19 [00:00<00:00, 45.42it/s]\n","[15 / 30] Train: Loss = 3.36791, PPX = 29.02:   1%|          | 5/677 [00:00<00:23, 29.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.3125\n","\n","на <unk> <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 30] Train: Loss = 3.31699, PPX = 27.58: 100%|██████████| 677/677 [00:15<00:00, 43.81it/s]\n","[15 / 30]   Val: Loss = 3.69503, PPX = 40.25: 100%|██████████| 19/19 [00:00<00:00, 45.51it/s]\n","[16 / 30] Train: Loss = 3.40814, PPX = 30.21:   1%|          | 5/677 [00:00<00:23, 28.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.078125\n","\n","<unk> в <unk> <unk> \n","олег <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[16 / 30] Train: Loss = 3.30954, PPX = 27.37: 100%|██████████| 677/677 [00:15<00:00, 43.94it/s]\n","[16 / 30]   Val: Loss = 3.69800, PPX = 40.37: 100%|██████████| 19/19 [00:00<00:00, 45.67it/s]\n","[17 / 30] Train: Loss = 3.16679, PPX = 23.73:   1%|          | 5/677 [00:00<00:19, 34.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0195312\n","\n","<unk> <unk> \n","<unk> <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[17 / 30] Train: Loss = 3.30755, PPX = 27.32: 100%|██████████| 677/677 [00:15<00:00, 44.12it/s]\n","[17 / 30]   Val: Loss = 3.69530, PPX = 40.26: 100%|██████████| 19/19 [00:00<00:00, 44.79it/s]\n","[18 / 30] Train: Loss = 3.27022, PPX = 26.32:   1%|          | 5/677 [00:00<00:22, 29.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.00488281\n","\n","<unk> <unk> <unk> \n","<unk> <unk> <unk> \n","а там <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[18 / 30] Train: Loss = 3.30708, PPX = 27.31: 100%|██████████| 677/677 [00:15<00:00, 43.82it/s]\n","[18 / 30]   Val: Loss = 3.69638, PPX = 40.30: 100%|██████████| 19/19 [00:00<00:00, 45.04it/s]\n","[19 / 30] Train: Loss = 3.38978, PPX = 29.66:   1%|          | 5/677 [00:00<00:23, 28.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0012207\n","\n","<unk> <unk> <unk> \n","<unk> <unk> <unk> \n","и <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[19 / 30] Train: Loss = 3.30694, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 43.69it/s]\n","[19 / 30]   Val: Loss = 3.69634, PPX = 40.30: 100%|██████████| 19/19 [00:00<00:00, 45.23it/s]\n","[20 / 30] Train: Loss = 3.32234, PPX = 27.73:   1%|          | 6/677 [00:00<00:19, 34.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.000305176\n","\n","<unk> <unk> <unk> <unk> \n","<unk> <unk> <unk> \n","и <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[20 / 30] Train: Loss = 3.30680, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 43.90it/s]\n","[20 / 30]   Val: Loss = 3.69659, PPX = 40.31: 100%|██████████| 19/19 [00:00<00:00, 46.45it/s]\n","[21 / 30] Train: Loss = 3.16153, PPX = 23.61:   1%|          | 5/677 [00:00<00:20, 32.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.62939e-05\n","\n","я <unk> <unk> \n","<unk> <unk> \n","и чтоб <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[21 / 30] Train: Loss = 3.30700, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 44.15it/s]\n","[21 / 30]   Val: Loss = 3.69492, PPX = 40.24: 100%|██████████| 19/19 [00:00<00:00, 47.70it/s]\n","[22 / 30] Train: Loss = 3.16914, PPX = 23.79:   1%|          | 5/677 [00:00<00:22, 29.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.90735e-05\n","\n","<unk> <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> <unk> в <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[22 / 30] Train: Loss = 3.30697, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 43.66it/s]\n","[22 / 30]   Val: Loss = 3.69609, PPX = 40.29: 100%|██████████| 19/19 [00:00<00:00, 46.83it/s]\n","[23 / 30] Train: Loss = 3.02718, PPX = 20.64:   1%|          | 5/677 [00:00<00:23, 28.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.76837e-06\n","\n","вы <unk> <unk> \n","на <unk> <unk> \n","и <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[23 / 30] Train: Loss = 3.30688, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 44.03it/s]\n","[23 / 30]   Val: Loss = 3.69563, PPX = 40.27: 100%|██████████| 19/19 [00:00<00:00, 45.21it/s]\n","[24 / 30] Train: Loss = 3.33497, PPX = 28.08:   1%|          | 5/677 [00:00<00:23, 28.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.19209e-06\n","\n","<unk> <unk> <unk> \n","<unk> <unk> за <unk> \n","<unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[24 / 30] Train: Loss = 3.30679, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 43.52it/s]\n","[24 / 30]   Val: Loss = 3.69622, PPX = 40.29: 100%|██████████| 19/19 [00:00<00:00, 47.54it/s]\n","[25 / 30] Train: Loss = 3.36646, PPX = 28.98:   1%|          | 5/677 [00:00<00:22, 29.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.98023e-07\n","\n","<unk> в <unk> и <unk> \n","а <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[25 / 30] Train: Loss = 3.30681, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 43.42it/s]\n","[25 / 30]   Val: Loss = 3.69582, PPX = 40.28: 100%|██████████| 19/19 [00:00<00:00, 45.69it/s]\n","[26 / 30] Train: Loss = 3.37615, PPX = 29.26:   1%|          | 5/677 [00:00<00:23, 29.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.45058e-08\n","\n","я <unk> <unk> \n","в <unk> <unk> <unk> \n","и <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[26 / 30] Train: Loss = 3.30702, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 43.73it/s]\n","[26 / 30]   Val: Loss = 3.69590, PPX = 40.28: 100%|██████████| 19/19 [00:00<00:00, 46.36it/s]\n","[27 / 30] Train: Loss = 3.40423, PPX = 30.09:   1%|          | 4/677 [00:00<00:24, 27.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.86265e-08\n","\n","<unk> <unk> <unk> \n","я <unk> <unk> \n","а ты <unk> не <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[27 / 30] Train: Loss = 3.30676, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 44.72it/s]\n","[27 / 30]   Val: Loss = 3.69646, PPX = 40.30: 100%|██████████| 19/19 [00:00<00:00, 47.40it/s]\n","[28 / 30] Train: Loss = 3.59946, PPX = 36.58:   1%|          | 5/677 [00:00<00:23, 28.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.65661e-09\n","\n","<unk> <unk> \n","в <unk> <unk> <unk> \n","<unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[28 / 30] Train: Loss = 3.30682, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 43.81it/s]\n","[28 / 30]   Val: Loss = 3.69561, PPX = 40.27: 100%|██████████| 19/19 [00:00<00:00, 46.37it/s]\n","[29 / 30] Train: Loss = 3.39091, PPX = 29.69:   1%|          | 5/677 [00:00<00:23, 29.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.16415e-09\n","\n","я <unk> <unk> \n","<unk> <unk> <unk> \n","и <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[29 / 30] Train: Loss = 3.30683, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 44.08it/s]\n","[29 / 30]   Val: Loss = 3.69630, PPX = 40.30: 100%|██████████| 19/19 [00:00<00:00, 49.32it/s]\n","[30 / 30] Train: Loss = 3.28454, PPX = 26.70:   1%|          | 5/677 [00:00<00:22, 30.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.91038e-10\n","\n","<unk> <unk> \n","на <unk> <unk> \n","и <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[30 / 30] Train: Loss = 3.30686, PPX = 27.30: 100%|██████████| 677/677 [00:15<00:00, 43.96it/s]\n","[30 / 30]   Val: Loss = 3.69627, PPX = 40.30: 100%|██████████| 19/19 [00:00<00:00, 47.33it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.27596e-11\n","\n","когда <unk> <unk> \n","в <unk> <unk> \n","<unk> <unk> <unk> \n","<unk> \n","\n"],"name":"stdout"}]},{"metadata":{"id":"r_YtM4ms8v--","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Добавьте маскинг `<unk>` токенов при тренировке модели."]},{"metadata":{"id":"IRx6j48-9LVu","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):                \n","                logits, _ = model(batch.text)\n","\n","                targets = torch.cat((batch.text[1:], batch.text.new_ones((1, batch.text.shape[1])))).view(-1)\n","\n","                loss = criterion(logits.view(-1, logits.shape[-1]), targets)\n","                \n","                mask = (1 - ((targets == unk_idx) + (targets == pad_idx))).float()\n","                \n","                loss = (loss * mask).sum() / mask.sum()\n","                \n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n","            \n","            if best_val_loss and val_loss > best_val_loss:\n","                optimizer.param_groups[0]['lr'] /= 4.\n","                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n","            else:\n","                best_val_loss = val_loss\n","        print()\n","        generate(model)\n","        print()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FqYDdA2n84xp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4505},"outputId":"070d8680-11a5-4bf4-c044-e81eaa91b3fc","executionInfo":{"status":"ok","timestamp":1542183380662,"user_tz":-180,"elapsed":487354,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = LMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[1 / 30] Train: Loss = 4.85370, PPX = 128.21: 100%|██████████| 677/677 [00:15<00:00, 43.29it/s]\n","[1 / 30]   Val: Loss = 4.43599, PPX = 84.44: 100%|██████████| 19/19 [00:00<00:00, 48.75it/s]\n","[2 / 30] Train: Loss = 4.32521, PPX = 75.58:   1%|          | 4/677 [00:00<00:20, 33.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я из день четыре с тобою \n","я не могу и не с собой \n","я так с себе что не думать \n","в них и \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[2 / 30] Train: Loss = 4.43800, PPX = 84.61: 100%|██████████| 677/677 [00:15<00:00, 43.03it/s]\n","[2 / 30]   Val: Loss = 4.30951, PPX = 74.40: 100%|██████████| 19/19 [00:00<00:00, 43.54it/s]\n","[3 / 30] Train: Loss = 3.98990, PPX = 54.05:   1%|          | 4/677 [00:00<00:24, 27.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в связи я своей не может \n","не потому что то я \n","а я как не по эту \n","и ты \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[3 / 30] Train: Loss = 4.14876, PPX = 63.36: 100%|██████████| 677/677 [00:15<00:00, 42.96it/s]\n","[3 / 30]   Val: Loss = 4.19565, PPX = 66.40: 100%|██████████| 19/19 [00:00<00:00, 46.68it/s]\n","[4 / 30] Train: Loss = 4.24416, PPX = 69.70:   1%|          | 4/677 [00:00<00:22, 29.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в лесу у женщины в конце рожденья \n","и в детстве не могу не быть \n","что только снова на диване \n","и хлеб \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[4 / 30] Train: Loss = 3.99983, PPX = 54.59: 100%|██████████| 677/677 [00:15<00:00, 43.18it/s]\n","[4 / 30]   Val: Loss = 4.14375, PPX = 63.04: 100%|██████████| 19/19 [00:00<00:00, 44.35it/s]\n","[5 / 30] Train: Loss = 3.93169, PPX = 50.99:   1%|          | 4/677 [00:00<00:22, 29.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я в год из тех кто без слов и \n","и не затем и всё не то \n","но не могла бы сразу видно \n","и я \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[5 / 30] Train: Loss = 3.88084, PPX = 48.46: 100%|██████████| 677/677 [00:15<00:00, 43.14it/s]\n","[5 / 30]   Val: Loss = 4.13189, PPX = 62.30: 100%|██████████| 19/19 [00:00<00:00, 45.31it/s]\n","[6 / 30] Train: Loss = 3.77827, PPX = 43.74:   1%|          | 4/677 [00:00<00:24, 26.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я не могу пошли по паре \n","и сердце в голову но всё ж \n","я не могу чего же боле \n","как ты \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[6 / 30] Train: Loss = 3.94383, PPX = 51.62: 100%|██████████| 677/677 [00:15<00:00, 43.18it/s]\n","[6 / 30]   Val: Loss = 4.18844, PPX = 65.92: 100%|██████████| 19/19 [00:00<00:00, 47.91it/s]\n","[7 / 30] Train: Loss = 3.72199, PPX = 41.35:   1%|          | 4/677 [00:00<00:20, 33.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5\n","\n","в твоей любви не получилось \n","но не осталось ни на зла \n","а мы же знал что не на ужин \n","и в лес \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[7 / 30] Train: Loss = 3.59169, PPX = 36.30: 100%|██████████| 677/677 [00:15<00:00, 43.34it/s]\n","[7 / 30]   Val: Loss = 4.11363, PPX = 61.17: 100%|██████████| 19/19 [00:00<00:00, 45.93it/s]\n","[8 / 30] Train: Loss = 3.54808, PPX = 34.75:   1%|          | 4/677 [00:00<00:22, 29.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в тот день когда ты мне приснился \n","то я бы дед но я несу \n","ну а теперь я тоже \n","когда \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[8 / 30] Train: Loss = 3.50355, PPX = 33.23: 100%|██████████| 677/677 [00:15<00:00, 43.13it/s]\n","[8 / 30]   Val: Loss = 4.12421, PPX = 61.82: 100%|██████████| 19/19 [00:00<00:00, 43.27it/s]\n","[9 / 30] Train: Loss = 3.49224, PPX = 32.86:   1%|          | 4/677 [00:00<00:24, 27.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.25\n","\n","олег в метро не попадает \n","и не желает на столе \n","и не хватает чем попало \n","в снегу \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 30] Train: Loss = 3.38837, PPX = 29.62: 100%|██████████| 677/677 [00:15<00:00, 43.20it/s]\n","[9 / 30]   Val: Loss = 4.12657, PPX = 61.97: 100%|██████████| 19/19 [00:00<00:00, 44.34it/s]\n","[10 / 30] Train: Loss = 3.34024, PPX = 28.23:   1%|          | 4/677 [00:00<00:22, 29.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.3125\n","\n","ну что ж ты так и так же плохо \n","я только лишь от дел до дна \n","и вас и даже не приду он \n","не смог \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 30] Train: Loss = 3.35021, PPX = 28.51: 100%|██████████| 677/677 [00:15<00:00, 43.32it/s]\n","[10 / 30]   Val: Loss = 4.12754, PPX = 62.03: 100%|██████████| 19/19 [00:00<00:00, 42.83it/s]\n","[11 / 30] Train: Loss = 3.37825, PPX = 29.32:   1%|          | 4/677 [00:00<00:23, 28.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.078125\n","\n","как мало шпаг для нас в париже \n","не так давно и не без дня \n","а я хочу лишь новый месяц \n","не знал \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[11 / 30] Train: Loss = 3.33957, PPX = 28.21: 100%|██████████| 677/677 [00:15<00:00, 42.87it/s]\n","[11 / 30]   Val: Loss = 4.13032, PPX = 62.20: 100%|██████████| 19/19 [00:00<00:00, 44.26it/s]\n","[12 / 30] Train: Loss = 3.11216, PPX = 22.47:   1%|          | 4/677 [00:00<00:24, 27.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0195312\n","\n","а я играю на болоте \n","на ужин и на дне а в рай \n","хотя давай еще анастасия \n","в воде \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 30] Train: Loss = 3.33674, PPX = 28.13: 100%|██████████| 677/677 [00:15<00:00, 43.11it/s]\n","[12 / 30]   Val: Loss = 4.12985, PPX = 62.17: 100%|██████████| 19/19 [00:00<00:00, 47.14it/s]\n","[13 / 30] Train: Loss = 3.32842, PPX = 27.89:   1%|          | 4/677 [00:00<00:20, 33.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.00488281\n","\n","вы не могли бы вы не быть мы \n","в чом хуже новый год о ту \n","а я на кухне так же всё же \n","ну да \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 30] Train: Loss = 3.33618, PPX = 28.11: 100%|██████████| 677/677 [00:15<00:00, 42.93it/s]\n","[13 / 30]   Val: Loss = 4.12970, PPX = 62.16: 100%|██████████| 19/19 [00:00<00:00, 45.28it/s]\n","[14 / 30] Train: Loss = 3.19333, PPX = 24.37:   1%|          | 5/677 [00:00<00:20, 33.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0012207\n","\n","вы не сумел меня не сложно \n","а вы тут у меня есть тот \n","мне не желаю чем на свете \n","не ной \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 30] Train: Loss = 3.33599, PPX = 28.11: 100%|██████████| 677/677 [00:15<00:00, 42.59it/s]\n","[14 / 30]   Val: Loss = 4.13014, PPX = 62.19: 100%|██████████| 19/19 [00:00<00:00, 43.42it/s]\n","[15 / 30] Train: Loss = 3.43050, PPX = 30.89:   1%|          | 4/677 [00:00<00:23, 28.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.000305176\n","\n","ну что и всё же говорите \n","на самом деле было в час \n","ну что ж сейчас не происходит \n","в чём соль \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 30] Train: Loss = 3.33604, PPX = 28.11: 100%|██████████| 677/677 [00:16<00:00, 41.92it/s]\n","[15 / 30]   Val: Loss = 4.12873, PPX = 62.10: 100%|██████████| 19/19 [00:00<00:00, 45.01it/s]\n","[16 / 30] Train: Loss = 3.18683, PPX = 24.21:   1%|          | 4/677 [00:00<00:24, 27.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.62939e-05\n","\n","я так и знал прошу остаться \n","и все на вас и всё не то \n","я полчаса тебе всё это \n","не так \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[16 / 30] Train: Loss = 3.33589, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 43.09it/s]\n","[16 / 30]   Val: Loss = 4.12970, PPX = 62.16: 100%|██████████| 19/19 [00:00<00:00, 43.74it/s]\n","[17 / 30] Train: Loss = 3.51730, PPX = 33.69:   1%|          | 4/677 [00:00<00:23, 28.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.90735e-05\n","\n","ты не поверите я не знала \n","что я не видела а бог \n","не я же не желаю ж буду \n","сам брат \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[17 / 30] Train: Loss = 3.33588, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 42.90it/s]\n","[17 / 30]   Val: Loss = 4.13004, PPX = 62.18: 100%|██████████| 19/19 [00:00<00:00, 47.08it/s]\n","[18 / 30] Train: Loss = 3.23616, PPX = 25.44:   1%|          | 4/677 [00:00<00:19, 33.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.76837e-06\n","\n","вот вы в любви сказала ольга \n","а я и рад не так уж \n","а я могу ещё на это \n","не дам \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[18 / 30] Train: Loss = 3.33586, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 42.95it/s]\n","[18 / 30]   Val: Loss = 4.13003, PPX = 62.18: 100%|██████████| 19/19 [00:00<00:00, 44.27it/s]\n","[19 / 30] Train: Loss = 3.26975, PPX = 26.30:   0%|          | 3/677 [00:00<00:22, 29.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.19209e-06\n","\n","у нас опять проблемы хьюстон \n","и думаешь что вы не ной \n","и что то в нём не все что в этих \n","ни в чём \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[19 / 30] Train: Loss = 3.33584, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 42.94it/s]\n","[19 / 30]   Val: Loss = 4.12981, PPX = 62.17: 100%|██████████| 19/19 [00:00<00:00, 43.66it/s]\n","[20 / 30] Train: Loss = 3.40821, PPX = 30.21:   1%|          | 4/677 [00:00<00:23, 28.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.98023e-07\n","\n","у вас жена и не играли \n","что мы не полный он должна \n","за что ты сам уже не может \n","не пей \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[20 / 30] Train: Loss = 3.33590, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 43.05it/s]\n","[20 / 30]   Val: Loss = 4.12993, PPX = 62.17: 100%|██████████| 19/19 [00:00<00:00, 49.09it/s]\n","[21 / 30] Train: Loss = 3.36954, PPX = 29.07:   1%|          | 5/677 [00:00<00:19, 33.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.45058e-08\n","\n","я не могу твоя родная \n","как ты вторых вопрос такой \n","но не на лбу а просто значит \n","не раз \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[21 / 30] Train: Loss = 3.33584, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 42.98it/s]\n","[21 / 30]   Val: Loss = 4.13073, PPX = 62.22: 100%|██████████| 19/19 [00:00<00:00, 44.50it/s]\n","[22 / 30] Train: Loss = 3.27750, PPX = 26.51:   1%|          | 4/677 [00:00<00:22, 29.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.86265e-08\n","\n","в борьбе с утра к нему приходит \n","не то чтоб спит не ест и ест \n","а те не надо сделать мне б \n","не пей \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[22 / 30] Train: Loss = 3.33597, PPX = 28.11: 100%|██████████| 677/677 [00:15<00:00, 42.87it/s]\n","[22 / 30]   Val: Loss = 4.12933, PPX = 62.14: 100%|██████████| 19/19 [00:00<00:00, 43.90it/s]\n","[23 / 30] Train: Loss = 3.28794, PPX = 26.79:   1%|          | 4/677 [00:00<00:22, 29.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.65661e-09\n","\n","я так хочу чтоб анатолий \n","в ответ как в океан \n","и в этот раз в углу да не \n","вот так \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[23 / 30] Train: Loss = 3.33587, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 43.31it/s]\n","[23 / 30]   Val: Loss = 4.12914, PPX = 62.12: 100%|██████████| 19/19 [00:00<00:00, 49.08it/s]\n","[24 / 30] Train: Loss = 3.48537, PPX = 32.63:   1%|          | 4/677 [00:00<00:20, 33.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.16415e-09\n","\n","мы с вами встретились случайно \n","по жизни с вами не знаком \n","и не могу понять что это \n","не бро \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[24 / 30] Train: Loss = 3.33591, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 43.47it/s]\n","[24 / 30]   Val: Loss = 4.13005, PPX = 62.18: 100%|██████████| 19/19 [00:00<00:00, 46.30it/s]\n","[25 / 30] Train: Loss = 3.34016, PPX = 28.22:   1%|          | 4/677 [00:00<00:24, 27.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.91038e-10\n","\n","уж лучше б я не понимаю \n","не в смысле что в стране ка \n","но я ему в постели с кухни \n","не быть \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[25 / 30] Train: Loss = 3.33569, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 43.25it/s]\n","[25 / 30]   Val: Loss = 4.12898, PPX = 62.11: 100%|██████████| 19/19 [00:00<00:00, 45.87it/s]\n","[26 / 30] Train: Loss = 3.45577, PPX = 31.68:   1%|          | 4/677 [00:00<00:23, 29.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.27596e-11\n","\n","в кружок у женщины в желудке \n","все мы с ужасом в плаще \n","ну вот и всё же так же ветер \n","как можно \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[26 / 30] Train: Loss = 3.33588, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 43.29it/s]\n","[26 / 30]   Val: Loss = 4.12987, PPX = 62.17: 100%|██████████| 19/19 [00:00<00:00, 47.48it/s]\n","[27 / 30] Train: Loss = 3.46973, PPX = 32.13:   1%|          | 4/677 [00:00<00:19, 33.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.81899e-11\n","\n","в окне в конце концов не видит \n","ни в чём бы в чом моя жена \n","я буду буду я в тебя не \n","с ума \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[27 / 30] Train: Loss = 3.33603, PPX = 28.11: 100%|██████████| 677/677 [00:15<00:00, 43.07it/s]\n","[27 / 30]   Val: Loss = 4.13051, PPX = 62.21: 100%|██████████| 19/19 [00:00<00:00, 44.59it/s]\n","[28 / 30] Train: Loss = 3.50725, PPX = 33.36:   1%|          | 4/677 [00:00<00:20, 33.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.54747e-12\n","\n","когда б имел то не в субботу \n","не говорили в сказке \n","и в этом раз он очень плохо \n","а нет \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[28 / 30] Train: Loss = 3.33606, PPX = 28.11: 100%|██████████| 677/677 [00:15<00:00, 42.97it/s]\n","[28 / 30]   Val: Loss = 4.12848, PPX = 62.08: 100%|██████████| 19/19 [00:00<00:00, 46.57it/s]\n","[29 / 30] Train: Loss = 3.23333, PPX = 25.36:   1%|          | 4/677 [00:00<00:24, 27.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.13687e-12\n","\n","в лесу ему как будто ночью \n","что не умел как бог бог \n","но каждый раз уже оксану \n","и ест \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[29 / 30] Train: Loss = 3.33576, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 43.30it/s]\n","[29 / 30]   Val: Loss = 4.12959, PPX = 62.15: 100%|██████████| 19/19 [00:00<00:00, 48.21it/s]\n","[30 / 30] Train: Loss = 3.36206, PPX = 28.85:   1%|          | 4/677 [00:00<00:20, 33.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.84217e-13\n","\n","чтоб не могу понять собою \n","и что на самом деле нет \n","а вы не с ним с кем мы в нем больше \n","помочь \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[30 / 30] Train: Loss = 3.33576, PPX = 28.10: 100%|██████████| 677/677 [00:15<00:00, 43.18it/s]\n","[30 / 30]   Val: Loss = 4.12915, PPX = 62.13: 100%|██████████| 19/19 [00:00<00:00, 45.96it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.10543e-14\n","\n","я не могу понять деревня \n","и как ни больше не а тем \n","я с ней в кровать с любовью и не видно \n","ни в чём \n","\n"],"name":"stdout"}]},{"metadata":{"id":"LzGwmgVf9Dkg","colab_type":"text"},"cell_type":"markdown","source":["## Улучшаем модель"]},{"metadata":{"id":"BHneb8br9WXh","colab_type":"text"},"cell_type":"markdown","source":["### Tying input and output embeddings\n","\n","В модели есть два эмбеддинга - входной и выходной. Красивая и полезная в жизни идея - учить только одну матрицу, расшаренную между ними: [Using the Output Embedding to Improve Language Models](http://www.aclweb.org/anthology/E17-2025).\n","\n","От идеи одни плюсы: получается намного меньше обучаемых параметров и при этом достаточно заметно более высокое качество.\n","\n","**Задание** Реализуйте это. Достаточно написать что-то типа этого в конструкторе:\n","\n","`self._out_layer.weight = self._emb.weight`"]},{"metadata":{"id":"44_vGUn9BPy3","colab_type":"code","colab":{}},"cell_type":"code","source":["class InputOutputLMModel(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=512, lstm_hidden_dim=512, num_layers=1):\n","        super().__init__()\n","\n","        self._dropout = nn.Dropout(0.4)\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers)\n","        \n","        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n","        self._out_layer.weight = self._emb.weight\n","        \n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._emb.weight.data.uniform_(-init_range, init_range)\n","        self._out_layer.bias.data.zero_()\n","        self._out_layer.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, inputs, hidden=None):\n","        embs = self._emb(inputs)\n","        output, hidden = self._rnn(embs, hidden)\n","        return self._out_layer(self._dropout(output)), hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_HcgeY41B3lq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4454},"outputId":"9c6aa78c-846b-45a8-e782-adfabe2c6cbf","executionInfo":{"status":"ok","timestamp":1542184031522,"user_tz":-180,"elapsed":650157,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = InputOutputLMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[1 / 30] Train: Loss = 5.10353, PPX = 164.60: 100%|██████████| 677/677 [00:21<00:00, 32.17it/s]\n","[1 / 30]   Val: Loss = 4.45067, PPX = 85.68: 100%|██████████| 19/19 [00:00<00:00, 33.89it/s]\n","[2 / 30] Train: Loss = 4.47596, PPX = 87.88:   0%|          | 3/677 [00:00<00:28, 23.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","как по виде то в мире \n","и все в него и не сказал \n","а вот уже не под \n","ничо \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[2 / 30] Train: Loss = 4.52891, PPX = 92.66: 100%|██████████| 677/677 [00:21<00:00, 32.10it/s]\n","[2 / 30]   Val: Loss = 4.49696, PPX = 89.74: 100%|██████████| 19/19 [00:00<00:00, 33.31it/s]\n","[3 / 30] Train: Loss = 4.32368, PPX = 75.47:   0%|          | 2/677 [00:00<00:33, 20.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5\n","\n","олег на рынке и очень \n","у вас на все на всех на это \n","а я не раз на ней о том да \n","не ссы \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[3 / 30] Train: Loss = 4.18552, PPX = 65.73: 100%|██████████| 677/677 [00:20<00:00, 32.24it/s]\n","[3 / 30]   Val: Loss = 4.19196, PPX = 66.15: 100%|██████████| 19/19 [00:00<00:00, 31.98it/s]\n","[4 / 30] Train: Loss = 4.14442, PPX = 63.08:   0%|          | 3/677 [00:00<00:29, 23.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я был давно на днях в лицо \n","и от него своих людей \n","и даже стало что с детства \n","не те \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[4 / 30] Train: Loss = 4.07410, PPX = 58.80: 100%|██████████| 677/677 [00:20<00:00, 32.27it/s]\n","[4 / 30]   Val: Loss = 4.14553, PPX = 63.15: 100%|██████████| 19/19 [00:00<00:00, 31.92it/s]\n","[5 / 30] Train: Loss = 3.99191, PPX = 54.16:   0%|          | 3/677 [00:00<00:27, 24.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в себя на пятом деле крокодил \n","и я не то как в кино \n","и не идут а не любили \n","а сзади \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[5 / 30] Train: Loss = 3.99779, PPX = 54.48: 100%|██████████| 677/677 [00:20<00:00, 32.35it/s]\n","[5 / 30]   Val: Loss = 4.10272, PPX = 60.50: 100%|██████████| 19/19 [00:00<00:00, 31.27it/s]\n","[6 / 30] Train: Loss = 4.12197, PPX = 61.68:   0%|          | 2/677 [00:00<00:34, 19.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в лесу не любит я не даром \n","я в сорок раз не в силах \n","и я не против всё простите \n","не те \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[6 / 30] Train: Loss = 3.93003, PPX = 50.91: 100%|██████████| 677/677 [00:20<00:00, 32.32it/s]\n","[6 / 30]   Val: Loss = 4.08144, PPX = 59.23: 100%|██████████| 19/19 [00:00<00:00, 33.08it/s]\n","[7 / 30] Train: Loss = 3.84952, PPX = 46.97:   0%|          | 3/677 [00:00<00:28, 23.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","к нам в дверь и в домике у соседей \n","и я не глядя в конце \n","и все же в этом мире жизни \n","дурак \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[7 / 30] Train: Loss = 3.86822, PPX = 47.86: 100%|██████████| 677/677 [00:20<00:00, 32.38it/s]\n","[7 / 30]   Val: Loss = 4.06067, PPX = 58.01: 100%|██████████| 19/19 [00:00<00:00, 31.98it/s]\n","[8 / 30] Train: Loss = 3.88694, PPX = 48.76:   0%|          | 3/677 [00:00<00:35, 18.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","из глеба вышел у оксаны \n","в глазах у радости на юг \n","и на четвёртый раз с тобой \n","на мне \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[8 / 30] Train: Loss = 3.80925, PPX = 45.12: 100%|██████████| 677/677 [00:21<00:00, 31.96it/s]\n","[8 / 30]   Val: Loss = 4.04973, PPX = 57.38: 100%|██████████| 19/19 [00:00<00:00, 33.00it/s]\n","[9 / 30] Train: Loss = 3.66412, PPX = 39.02:   0%|          | 3/677 [00:00<00:28, 23.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","сейчас мы с вами не согласен \n","не то но как бы не мне \n","стал не скажу мне не до смерти \n","из рук \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 30] Train: Loss = 3.75014, PPX = 42.53: 100%|██████████| 677/677 [00:20<00:00, 32.43it/s]\n","[9 / 30]   Val: Loss = 4.04515, PPX = 57.12: 100%|██████████| 19/19 [00:00<00:00, 33.54it/s]\n","[10 / 30] Train: Loss = 3.57695, PPX = 35.76:   0%|          | 3/677 [00:00<00:28, 23.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","твой мозг с огромным на планете \n","все ждут и так всю ночь на лоб \n","а сам не так и даже если \n","такой \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 30] Train: Loss = 3.69086, PPX = 40.08: 100%|██████████| 677/677 [00:20<00:00, 32.48it/s]\n","[10 / 30]   Val: Loss = 4.05086, PPX = 57.45: 100%|██████████| 19/19 [00:00<00:00, 31.28it/s]\n","[11 / 30] Train: Loss = 3.45914, PPX = 31.79:   0%|          | 3/677 [00:00<00:28, 23.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.25\n","\n","нет я не рад но за привычке \n","не надо ждать себя в ответ \n","а вот и всё таки я сразу \n","не ты \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[11 / 30] Train: Loss = 3.54748, PPX = 34.73: 100%|██████████| 677/677 [00:20<00:00, 32.33it/s]\n","[11 / 30]   Val: Loss = 4.03344, PPX = 56.45: 100%|██████████| 19/19 [00:00<00:00, 32.59it/s]\n","[12 / 30] Train: Loss = 3.43833, PPX = 31.13:   0%|          | 3/677 [00:00<00:29, 23.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","меня не станет и не в силах \n","готов в чём было всё в порядке \n","и я теперь я в этой жизни \n","не трожь \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 30] Train: Loss = 3.51018, PPX = 33.45: 100%|██████████| 677/677 [00:20<00:00, 32.24it/s]\n","[12 / 30]   Val: Loss = 4.04180, PPX = 56.93: 100%|██████████| 19/19 [00:00<00:00, 33.32it/s]\n","[13 / 30] Train: Loss = 3.35621, PPX = 28.68:   0%|          | 3/677 [00:00<00:29, 23.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.3125\n","\n","в глазах у деда солнце \n","не зная что меня в воде \n","а я хочу и не против \n","и у \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 30] Train: Loss = 3.46036, PPX = 31.83: 100%|██████████| 677/677 [00:20<00:00, 32.34it/s]\n","[13 / 30]   Val: Loss = 4.03955, PPX = 56.80: 100%|██████████| 19/19 [00:00<00:00, 32.46it/s]\n","[14 / 30] Train: Loss = 3.41643, PPX = 30.46:   0%|          | 3/677 [00:00<00:28, 23.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.078125\n","\n","а где у вас тут нет в чём то \n","что вы не знаю что не та \n","и как всегда странным выход \n","не та \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 30] Train: Loss = 3.44280, PPX = 31.27: 100%|██████████| 677/677 [00:20<00:00, 32.38it/s]\n","[14 / 30]   Val: Loss = 4.04233, PPX = 56.96: 100%|██████████| 19/19 [00:00<00:00, 31.51it/s]\n","[15 / 30] Train: Loss = 3.41618, PPX = 30.45:   0%|          | 3/677 [00:00<00:27, 24.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0195312\n","\n","я буду свет в конце тоннеля \n","и в мир холодный и жена \n","мы не могли бы быть не против \n","не с тем \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 30] Train: Loss = 3.43756, PPX = 31.11: 100%|██████████| 677/677 [00:20<00:00, 32.58it/s]\n","[15 / 30]   Val: Loss = 4.04270, PPX = 56.98: 100%|██████████| 19/19 [00:00<00:00, 32.06it/s]\n","[16 / 30] Train: Loss = 3.39643, PPX = 29.86:   0%|          | 3/677 [00:00<00:27, 24.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.00488281\n","\n","я был при жизни был не верю \n","ни в чём то в той палате \n","не потому что он в постели \n","и нет \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[16 / 30] Train: Loss = 3.43846, PPX = 31.14: 100%|██████████| 677/677 [00:21<00:00, 32.16it/s]\n","[16 / 30]   Val: Loss = 4.04262, PPX = 56.98: 100%|██████████| 19/19 [00:00<00:00, 32.34it/s]\n","[17 / 30] Train: Loss = 3.38114, PPX = 29.40:   0%|          | 3/677 [00:00<00:34, 19.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0012207\n","\n","я не боюсь не понимаю \n","что можно в этом было сто \n","а то с тобой я слышу видел \n","и в рот \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[17 / 30] Train: Loss = 3.43704, PPX = 31.09: 100%|██████████| 677/677 [00:20<00:00, 32.39it/s]\n","[17 / 30]   Val: Loss = 4.04306, PPX = 57.00: 100%|██████████| 19/19 [00:00<00:00, 32.51it/s]\n","[18 / 30] Train: Loss = 3.46605, PPX = 32.01:   0%|          | 3/677 [00:00<00:29, 22.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.000305176\n","\n","я не хочу кричал геннадий \n","а ты в ответ не так уж плох \n","а я уже не так не старый \n","и рад \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[18 / 30] Train: Loss = 3.43826, PPX = 31.13: 100%|██████████| 677/677 [00:20<00:00, 32.52it/s]\n","[18 / 30]   Val: Loss = 4.04228, PPX = 56.96: 100%|██████████| 19/19 [00:00<00:00, 33.61it/s]\n","[19 / 30] Train: Loss = 3.36514, PPX = 28.94:   0%|          | 3/677 [00:00<00:28, 23.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.62939e-05\n","\n","не надо мною что на свете \n","сказал мне с ужасом иван \n","и я внезапно сам не буду \n","меня \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[19 / 30] Train: Loss = 3.43646, PPX = 31.08: 100%|██████████| 677/677 [00:20<00:00, 32.36it/s]\n","[19 / 30]   Val: Loss = 4.04192, PPX = 56.94: 100%|██████████| 19/19 [00:00<00:00, 31.04it/s]\n","[20 / 30] Train: Loss = 3.49750, PPX = 33.03:   0%|          | 3/677 [00:00<00:28, 23.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.90735e-05\n","\n","прошу вас не за странный денег \n","вы не могли бы не пойму \n","я сам всегда не догадался \n","в ответ \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[20 / 30] Train: Loss = 3.43731, PPX = 31.10: 100%|██████████| 677/677 [00:20<00:00, 32.32it/s]\n","[20 / 30]   Val: Loss = 4.04332, PPX = 57.02: 100%|██████████| 19/19 [00:00<00:00, 31.40it/s]\n","[21 / 30] Train: Loss = 3.41063, PPX = 30.28:   0%|          | 3/677 [00:00<00:33, 19.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.76837e-06\n","\n","не надо бы в театре смерти \n","сказала ты сказала да \n","и тут же в наш колхоз приехал \n","из вас \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[21 / 30] Train: Loss = 3.43690, PPX = 31.09: 100%|██████████| 677/677 [00:20<00:00, 32.45it/s]\n","[21 / 30]   Val: Loss = 4.04206, PPX = 56.94: 100%|██████████| 19/19 [00:00<00:00, 32.02it/s]\n","[22 / 30] Train: Loss = 3.40624, PPX = 30.15:   0%|          | 3/677 [00:00<00:29, 22.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.19209e-06\n","\n","олег проснулся на работу \n","в кафе с улыбкой в башке \n","и там во всех сегодня этот \n","как бог \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[22 / 30] Train: Loss = 3.43646, PPX = 31.08: 100%|██████████| 677/677 [00:21<00:00, 32.04it/s]\n","[22 / 30]   Val: Loss = 4.04183, PPX = 56.93: 100%|██████████| 19/19 [00:00<00:00, 31.83it/s]\n","[23 / 30] Train: Loss = 3.58439, PPX = 36.03:   0%|          | 3/677 [00:00<00:27, 24.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.98023e-07\n","\n","я знаю что такое осень \n","что я не был и не могу \n","а то что с ней не в силах \n","нельзя \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[23 / 30] Train: Loss = 3.43727, PPX = 31.10: 100%|██████████| 677/677 [00:21<00:00, 32.18it/s]\n","[23 / 30]   Val: Loss = 4.04294, PPX = 56.99: 100%|██████████| 19/19 [00:00<00:00, 32.90it/s]\n","[24 / 30] Train: Loss = 3.45270, PPX = 31.59:   0%|          | 3/677 [00:00<00:28, 23.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.45058e-08\n","\n","когда ты мне приснилось чудо \n","и я всё чаще хорошо \n","про то что не заметил замуж \n","но как \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[24 / 30] Train: Loss = 3.43601, PPX = 31.06: 100%|██████████| 677/677 [00:20<00:00, 32.38it/s]\n","[24 / 30]   Val: Loss = 4.04116, PPX = 56.89: 100%|██████████| 19/19 [00:00<00:00, 33.21it/s]\n","[25 / 30] Train: Loss = 3.40065, PPX = 29.98:   0%|          | 3/677 [00:00<00:27, 24.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.86265e-08\n","\n","когда в экран то не хватало \n","то в доме то в конце концов \n","а то с утра меня в капусте \n","то без \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[25 / 30] Train: Loss = 3.43608, PPX = 31.06: 100%|██████████| 677/677 [00:21<00:00, 32.14it/s]\n","[25 / 30]   Val: Loss = 4.04244, PPX = 56.97: 100%|██████████| 19/19 [00:00<00:00, 32.43it/s]\n","[26 / 30] Train: Loss = 3.48315, PPX = 32.56:   0%|          | 3/677 [00:00<00:29, 23.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.65661e-09\n","\n","в окно вселенной в виду боя \n","не в силах даже в ночь ко мне \n","но мы с тобой в бою по локоть \n","не та \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[26 / 30] Train: Loss = 3.43622, PPX = 31.07: 100%|██████████| 677/677 [00:20<00:00, 32.37it/s]\n","[26 / 30]   Val: Loss = 4.04264, PPX = 56.98: 100%|██████████| 19/19 [00:00<00:00, 33.25it/s]\n","[27 / 30] Train: Loss = 3.36616, PPX = 28.97:   0%|          | 3/677 [00:00<00:29, 23.15it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.16415e-09\n","\n","у нас в конце концов не сложно \n","а мы с тобой в вполне женой \n","и в этом тапки по привычке \n","на ты \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[27 / 30] Train: Loss = 3.43652, PPX = 31.08: 100%|██████████| 677/677 [00:21<00:00, 32.22it/s]\n","[27 / 30]   Val: Loss = 4.04224, PPX = 56.95: 100%|██████████| 19/19 [00:00<00:00, 33.03it/s]\n","[28 / 30] Train: Loss = 3.55847, PPX = 35.11:   0%|          | 3/677 [00:00<00:34, 19.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.91038e-10\n","\n","и в сердце в этом мире люди \n","на этот бой и в носу \n","я так сейчас не понимаю \n","в кровать \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[28 / 30] Train: Loss = 3.43795, PPX = 31.12: 100%|██████████| 677/677 [00:21<00:00, 32.24it/s]\n","[28 / 30]   Val: Loss = 4.04245, PPX = 56.97: 100%|██████████| 19/19 [00:00<00:00, 31.56it/s]\n","[29 / 30] Train: Loss = 3.43239, PPX = 30.95:   0%|          | 3/677 [00:00<00:29, 22.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.27596e-11\n","\n","я видел двух тебя как будто \n","но в самом деле всё в стране \n","и я не знаю что не видел \n","в аду \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[29 / 30] Train: Loss = 3.43725, PPX = 31.10: 100%|██████████| 677/677 [00:21<00:00, 32.05it/s]\n","[29 / 30]   Val: Loss = 4.04271, PPX = 56.98: 100%|██████████| 19/19 [00:00<00:00, 31.67it/s]\n","[30 / 30] Train: Loss = 3.42859, PPX = 30.83:   0%|          | 3/677 [00:00<00:29, 23.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.81899e-11\n","\n","мне не нужна на этом свете \n","уже три года на год \n","и в этом мире он не в силах \n","не спит \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[30 / 30] Train: Loss = 3.43688, PPX = 31.09: 100%|██████████| 677/677 [00:21<00:00, 31.86it/s]\n","[30 / 30]   Val: Loss = 4.04218, PPX = 56.95: 100%|██████████| 19/19 [00:00<00:00, 32.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.54747e-12\n","\n","из всех сторон и я влюбилась \n","что был один со мной как ты \n","а у меня там на краю \n","жену \n","\n"],"name":"stdout"}]},{"metadata":{"id":"N8I3QC4a_a8q","colab_type":"text"},"cell_type":"markdown","source":["### Добавление информации в выборку\n","\n","Сейчас у нас каждое слово предствляется одним индексом. Модели очень сложно узнать, сколько в нем слогов - а значит, сложно генерировать корректное стихотворение.\n","\n","На самом деле к каждому слову можно приписать кусочек из метрического шаблона:\n","\n","![](https://hsto.org/web/59a/b39/bd0/59ab39bd020c49a78a12cbab62c80181.png =x200)\n","\n","**Задание** Обновите функцию `read_poem`, пусть она генерирует два списка - список слов и список кусков шаблона.  \n","Добавьте в модель вход - последовательности шаблонов, конкатенируйте их эмбеддинги со словами.  \n","Дополнительная идея - заставьте модель угадывать, какой шаблон должен идти следующим (где-то половина будет подходящими, остальные - нет). Добавьте дополнительные потери от угадывания шаблона."]},{"metadata":{"id":"v-RTm5xyxVgg","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_poem(path):\n","    poem, patterns = [], []\n","    with open(path, encoding='utf8') as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if len(line) == 0:\n","                yield poem, patterns\n","                poem, patterns = [], []\n","                continue\n","\n","            pattern = []\n","            template = ''\n","            sign = '-'\n","            \n","            for word in line.split():\n","                for char in word:\n","                    if char in vowels:\n","                        template += sign\n","                        sign = '+'if sign == '-' else '-'\n","                pattern.append(template)\n","                template = ''\n","\n","            poem.extend(line.split() + ['\\\\n'])\n","            patterns.extend(pattern + ['\\\\n'])\n","\n","poroshki = np.array(list(read_poem('poroshki.txt')))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X8hes6hPfkaf","colab_type":"code","colab":{}},"cell_type":"code","source":["poroshki, poroshki_patterns = poroshki[:, 0], poroshki[:, 1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WoUDWTVZzFer","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b7f2bd77-b82d-4b95-9ed4-4f2b8208de61","executionInfo":{"status":"ok","timestamp":1542184435053,"user_tz":-180,"elapsed":1346,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["from torchtext.data import Field, Example, Dataset, BucketIterator\n","\n","text_field = Field(init_token='<s>', eos_token='</s>')\n","pattern_field = Field(init_token='<s>', eos_token='</s>')\n","        \n","fields = [('text', text_field), ('pattern', pattern_field)]\n","examples = [Example.fromlist([poem, pattern], fields) for poem, pattern in zip(poroshki, poroshki_patterns)]\n","dataset = Dataset(examples, fields)\n","\n","text_field.build_vocab(dataset, min_freq=7)\n","pattern_field.build_vocab(dataset)\n","\n","print('Vocab size =', len(text_field.vocab))\n","train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n","\n","train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n","                                              shuffle=True, device=DEVICE, sort=False)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Vocab size = 6298\n"],"name":"stdout"}]},{"metadata":{"id":"di6dw3b0z9wG","colab_type":"code","colab":{}},"cell_type":"code","source":["class PatternModel(nn.Module):\n","    def __init__(self, vocab_size, pattern_size, emb_dim=512, lstm_hidden_dim=512, num_layers=1):\n","        super().__init__()\n","\n","        self._dropout = nn.Dropout(0.4)\n","        self._text_emb = nn.Embedding(vocab_size, emb_dim)\n","        self._pattern_emb = torch.eye(pattern_size).cuda()\n","        self._rnn = nn.LSTM(input_size=emb_dim+pattern_size, hidden_size=lstm_hidden_dim, num_layers=num_layers)\n","        \n","        self._text_out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n","        self._text_out_layer.weight = self._text_emb.weight\n","        \n","        self._pattern_out_layer = nn.Linear(lstm_hidden_dim, pattern_size)\n","        \n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._text_emb.weight.data.uniform_(-init_range, init_range)\n","        self._text_out_layer.bias.data.zero_()\n","        self._text_out_layer.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, inputs, patterns, hidden=None):\n","        text_embs = self._text_emb(inputs)\n","        \n","        pattern_embs = self._pattern_emb[patterns]\n","        \n","        embs = torch.cat((text_embs, pattern_embs), -1)\n","        \n","        output, hidden = self._rnn(embs, hidden)\n","        \n","        text_output = self._text_out_layer(self._dropout(output))\n","        pattern_output = self._pattern_out_layer(self._dropout(output))\n","        \n","        return text_output, pattern_output, hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Nn9If6DI4zkP","colab_type":"code","colab":{}},"cell_type":"code","source":["model = PatternModel(vocab_size=len(train_iter.dataset.fields['text'].vocab),\n","                     pattern_size=len(train_iter.dataset.fields['pattern'].vocab)).to(DEVICE)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T4_JNivC4yfV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":94},"outputId":"ad2901e9-9295-4a5c-bfe9-d8e2c5127c39","executionInfo":{"status":"ok","timestamp":1542184438685,"user_tz":-180,"elapsed":1207,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["def sample(probs, temp):\n","    probs = F.log_softmax(probs.squeeze(), dim=0)\n","    probs = (probs / temp).exp()\n","    probs /= probs.sum()\n","    probs = probs.cpu().numpy()\n","\n","    return np.random.choice(np.arange(len(probs)), p=probs)\n","\n","def generate(model, temp=0.6):\n","    model.eval()\n","    with torch.no_grad():        \n","        prev_text = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n","        prev_pattern = train_iter.dataset.fields['pattern'].vocab.stoi['<s>']\n","        end_text = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n","        end_pattern = train_iter.dataset.fields['pattern'].vocab.stoi['</s>']\n","        \n","        hidden = None\n","        sign = '-'\n","        for _ in range(150):\n","            text_probs, _, hidden = model(LongTensor([[prev_text]]), LongTensor([[prev_pattern]]), hidden)\n","            prev_text = sample(text_probs, temp)\n","            \n","            pattern = ''\n","            for char in train_iter.dataset.fields['text'].vocab.itos[prev_text]:\n","                if char in vowels:\n","                    pattern += sign\n","                    sign = '+' if sign == '-' else '-'\n","            prev_pattern = train_iter.dataset.fields['pattern'].vocab.stoi[pattern]\n","            \n","            if prev_text == end_text:\n","                return\n","            \n","            if train_iter.dataset.fields['text'].vocab.itos[prev_text] == '\\\\n':\n","                sign = '-'\n","                print()\n","            else:\n","                print(train_iter.dataset.fields['text'].vocab.itos[prev_text], end=' ')\n","                \n","generate(model)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["год столу рифма вырос назад иной сцену дега игру ан ребёнок хвосты молчанье коллеги какого вашем пора настал сложив поймать солнце четыре сдох краю друг грим обнимку рыбка кормит горы роняя штирлиц зайка закричал смотря уроды пытаюсь седьмой скрипит превратился вслух догадался дала меняет догадался достиг съезде выход камней ужас блин увы дают покоя собакой одетый крыш украл ум дык пера вылез времён шаги держали опытный режим необходимо такого фу семён теребя дурака слезу сложив любые светится бывают высоты навстречу небрит крики обычным шаинский гуляш прах будешь полна тычет ага вертолёт богатство коту плывёт вине достиг пыл плоть ева суёт вин тыгдым жива снилось жрал слове рояля сидел трудом байкал взор шага изпод летит древних каждого первых песнь обязан глеб тыкать козлом седин лез ежа добавить судья побыть дождя старушка рыдая сказал эркюль даче сознанья холодильник оксана чехов сюрприз юности таким выпь зелёных баню зажал чуковский машине ильич вова стакан "],"name":"stdout"}]},{"metadata":{"id":"f1dOsbn02Gly","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):\n","                text_logits, pattern_logits, _ = model(batch.text, batch.pattern)\n","\n","                text_targets = torch.cat((batch.text[1:], batch.text.new_ones((1, batch.text.shape[1])))).view(-1)\n","                \n","                additional = None\n","                if optimizer:\n","                    pattern_targets = torch.cat((batch.pattern[1:], batch.pattern.new_ones((1, batch.pattern.shape[1])))).view(-1)\n","                    pattern_loss = criterion(pattern_logits.view(-1, pattern_logits.shape[-1]), pattern_targets)\n","                    pattern_mask = (1 - (pattern_targets == pad_idx)).float()\n","                    additional = (pattern_loss * pattern_mask).sum() / pattern_mask.sum()\n","\n","                text_loss = criterion(text_logits.view(-1, text_logits.shape[-1]), text_targets)\n","                \n","                text_mask = (1 - ((text_targets == unk_idx) + (text_targets == pad_idx))).float()\n","                \n","                loss = (text_loss * text_mask).sum() / text_mask.sum()\n","                if additional:\n","                    loss += additional\n","                \n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n","            \n","            if best_val_loss and val_loss > best_val_loss:\n","                optimizer.param_groups[0]['lr'] /= 4.\n","                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n","            else:\n","                best_val_loss = val_loss\n","        print()\n","        generate(model)\n","        print()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Sk_ej9rD1zIW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4386},"outputId":"da2b1b89-8730-4f36-c84c-27d6571dd6c1","executionInfo":{"status":"ok","timestamp":1542185198983,"user_tz":-180,"elapsed":759565,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = PatternModel(vocab_size=len(train_iter.dataset.fields['text'].vocab),\n","                     pattern_size=len(train_iter.dataset.fields['pattern'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[1 / 30] Train: Loss = 7.58461, PPX = 1967.69: 100%|██████████| 677/677 [00:24<00:00, 27.55it/s]\n","[1 / 30]   Val: Loss = 4.60099, PPX = 99.58: 100%|██████████| 19/19 [00:00<00:00, 29.26it/s]\n","[2 / 30] Train: Loss = 6.74021, PPX = 845.74:   0%|          | 2/677 [00:00<00:35, 19.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","на день и не не знаю было \n","о вдруг в вас не тебя я \n","и не \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[2 / 30] Train: Loss = 6.29870, PPX = 543.86: 100%|██████████| 677/677 [00:24<00:00, 27.44it/s]\n","[2 / 30]   Val: Loss = 4.51373, PPX = 91.26: 100%|██████████| 19/19 [00:00<00:00, 28.96it/s]\n","[3 / 30] Train: Loss = 6.45594, PPX = 636.47:   0%|          | 2/677 [00:00<00:36, 18.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","на самом деле на работу \n","и не могу тогда не друг \n","и жизнь не все не буду \n","не те \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[3 / 30] Train: Loss = 6.10329, PPX = 447.33: 100%|██████████| 677/677 [00:24<00:00, 27.74it/s]\n","[3 / 30]   Val: Loss = 4.23814, PPX = 69.28: 100%|██████████| 19/19 [00:00<00:00, 29.14it/s]\n","[4 / 30] Train: Loss = 5.49055, PPX = 242.39:   0%|          | 2/677 [00:00<00:35, 19.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","когда ты ты не понимаю \n","а я не стал и по весне \n","я в них то и по как то \n","в себя \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[4 / 30] Train: Loss = 5.91760, PPX = 371.52: 100%|██████████| 677/677 [00:24<00:00, 27.46it/s]\n","[4 / 30]   Val: Loss = 4.25632, PPX = 70.55: 100%|██████████| 19/19 [00:00<00:00, 28.45it/s]\n","[5 / 30] Train: Loss = 5.41340, PPX = 224.39:   0%|          | 2/677 [00:00<00:35, 18.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5\n","\n","олег в тумане к нам всё время \n","и с той с тобою я \n","и вот бы вместе с нею просто \n","и те \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[5 / 30] Train: Loss = 5.02392, PPX = 152.01: 100%|██████████| 677/677 [00:24<00:00, 28.04it/s]\n","[5 / 30]   Val: Loss = 4.01836, PPX = 55.61: 100%|██████████| 19/19 [00:00<00:00, 30.51it/s]\n","[6 / 30] Train: Loss = 4.60896, PPX = 100.38:   0%|          | 2/677 [00:00<00:35, 19.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я написал но я не буду \n","в себя из них у нас в аду \n","но я же из тебя не стало \n","в сети \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[6 / 30] Train: Loss = 4.76177, PPX = 116.95: 100%|██████████| 677/677 [00:24<00:00, 27.84it/s]\n","[6 / 30]   Val: Loss = 3.96373, PPX = 52.65: 100%|██████████| 19/19 [00:00<00:00, 27.54it/s]\n","[7 / 30] Train: Loss = 4.74284, PPX = 114.76:   0%|          | 1/677 [00:00<00:49, 13.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","не больно я не по квартире \n","и тут в окно и в этом мире \n","и я с вам не уверен что то \n","но нет \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[7 / 30] Train: Loss = 4.68568, PPX = 108.38: 100%|██████████| 677/677 [00:24<00:00, 27.33it/s]\n","[7 / 30]   Val: Loss = 3.94100, PPX = 51.47: 100%|██████████| 19/19 [00:00<00:00, 28.97it/s]\n","[8 / 30] Train: Loss = 4.57261, PPX = 96.80:   0%|          | 2/677 [00:00<00:36, 18.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","не в силах нет в своей постели \n","как ни в любви и не хожу \n","но в зоопарке вдруг со мною \n","и в глаз \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[8 / 30] Train: Loss = 4.63027, PPX = 102.54: 100%|██████████| 677/677 [00:24<00:00, 26.71it/s]\n","[8 / 30]   Val: Loss = 3.91826, PPX = 50.31: 100%|██████████| 19/19 [00:00<00:00, 27.41it/s]\n","[9 / 30] Train: Loss = 4.37675, PPX = 79.58:   0%|          | 2/677 [00:00<00:36, 18.53it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","мы с ним княжна своей недели \n","и вслед и не из под ноги \n","а мне не слишком просто понял \n","в руках \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 30] Train: Loss = 4.58004, PPX = 97.52: 100%|██████████| 677/677 [00:24<00:00, 27.78it/s]\n","[9 / 30]   Val: Loss = 3.90952, PPX = 49.87: 100%|██████████| 19/19 [00:00<00:00, 28.77it/s]\n","[10 / 30] Train: Loss = 4.42890, PPX = 83.84:   0%|          | 2/677 [00:00<00:35, 19.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я рад но не из за начала \n","и говорит я не могу \n","и я хочу чтоб по привычке \n","и ты \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 30] Train: Loss = 4.53302, PPX = 93.04: 100%|██████████| 677/677 [00:24<00:00, 27.63it/s]\n","[10 / 30]   Val: Loss = 3.89319, PPX = 49.07: 100%|██████████| 19/19 [00:00<00:00, 29.36it/s]\n","[11 / 30] Train: Loss = 4.49840, PPX = 89.87:   0%|          | 2/677 [00:00<00:36, 18.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","когда мужчина у подъезда \n","её в объятьях лист а эс \n","и что то в этом месте было \n","есть лось \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[11 / 30] Train: Loss = 4.48608, PPX = 88.77: 100%|██████████| 677/677 [00:24<00:00, 27.44it/s]\n","[11 / 30]   Val: Loss = 3.88598, PPX = 48.71: 100%|██████████| 19/19 [00:00<00:00, 30.20it/s]\n","[12 / 30] Train: Loss = 4.32757, PPX = 75.76:   0%|          | 1/677 [00:00<00:47, 14.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","пришёл от мира на планете \n","и вот я с миром не в меня \n","когда на ухо в поле боя \n","и вот \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 30] Train: Loss = 4.44210, PPX = 84.95: 100%|██████████| 677/677 [00:24<00:00, 27.40it/s]\n","[12 / 30]   Val: Loss = 3.87870, PPX = 48.36: 100%|██████████| 19/19 [00:00<00:00, 28.99it/s]\n","[13 / 30] Train: Loss = 4.28874, PPX = 72.87:   0%|          | 2/677 [00:00<00:34, 19.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я не люблю тебя родная \n","и вот как только так и сяк \n","а вот и все мы будем в этом \n","не те \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 30] Train: Loss = 4.39447, PPX = 81.00: 100%|██████████| 677/677 [00:24<00:00, 26.71it/s]\n","[13 / 30]   Val: Loss = 3.87908, PPX = 48.38: 100%|██████████| 19/19 [00:00<00:00, 29.42it/s]\n","[14 / 30] Train: Loss = 4.28453, PPX = 72.57:   0%|          | 2/677 [00:00<00:35, 19.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.25\n","\n","в окно заходит в ресторане \n","не знаю что меня не зря \n","во сне я понял что не надо \n","и ты \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 30] Train: Loss = 4.26545, PPX = 71.20: 100%|██████████| 677/677 [00:24<00:00, 27.08it/s]\n","[14 / 30]   Val: Loss = 3.86846, PPX = 47.87: 100%|██████████| 19/19 [00:00<00:00, 29.13it/s]\n","[15 / 30] Train: Loss = 4.30388, PPX = 73.99:   0%|          | 1/677 [00:00<00:47, 14.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в тот день когда ты мне приснился \n","я буду чем то грудь и тут \n","и не нашёл и с ней и дама \n","в окно \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 30] Train: Loss = 4.23520, PPX = 69.08: 100%|██████████| 677/677 [00:24<00:00, 27.70it/s]\n","[15 / 30]   Val: Loss = 3.86923, PPX = 47.91: 100%|██████████| 19/19 [00:00<00:00, 29.26it/s]\n","[16 / 30] Train: Loss = 4.09961, PPX = 60.32:   0%|          | 2/677 [00:00<00:34, 19.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.3125\n","\n","так хочется в твои объятья \n","в большой любви и даже в нос \n","но оказалось это просто \n","не я \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[16 / 30] Train: Loss = 4.19493, PPX = 66.35: 100%|██████████| 677/677 [00:24<00:00, 27.61it/s]\n","[16 / 30]   Val: Loss = 3.87260, PPX = 48.07: 100%|██████████| 19/19 [00:00<00:00, 29.40it/s]\n","[17 / 30] Train: Loss = 4.32387, PPX = 75.48:   0%|          | 2/677 [00:00<00:36, 18.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.078125\n","\n","не слишком много слов и этих \n","и вот уже в глазах трусы \n","кто б мы с тобой уже не против \n","и те \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[17 / 30] Train: Loss = 4.17919, PPX = 65.31: 100%|██████████| 677/677 [00:24<00:00, 27.80it/s]\n","[17 / 30]   Val: Loss = 3.86953, PPX = 47.92: 100%|██████████| 19/19 [00:00<00:00, 28.98it/s]\n","[18 / 30] Train: Loss = 4.15818, PPX = 63.96:   0%|          | 2/677 [00:00<00:35, 19.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0195312\n","\n","я ставлю долгих лет в семнадцать \n","и на пол несколько минут \n","и вдруг в итоге ставит в поле \n","к утру \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[18 / 30] Train: Loss = 4.17602, PPX = 65.11: 100%|██████████| 677/677 [00:24<00:00, 27.65it/s]\n","[18 / 30]   Val: Loss = 3.87071, PPX = 47.98: 100%|██████████| 19/19 [00:00<00:00, 30.29it/s]\n","[19 / 30] Train: Loss = 4.23028, PPX = 68.74:   0%|          | 2/677 [00:00<00:34, 19.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.00488281\n","\n","я так хотел тебя родная \n","я не могу вас не люблю \n","но тут у нас же есть последний \n","с утра \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[19 / 30] Train: Loss = 4.17617, PPX = 65.12: 100%|██████████| 677/677 [00:24<00:00, 28.26it/s]\n","[19 / 30]   Val: Loss = 3.87019, PPX = 47.95: 100%|██████████| 19/19 [00:00<00:00, 28.99it/s]\n","[20 / 30] Train: Loss = 4.04913, PPX = 57.35:   0%|          | 2/677 [00:00<00:36, 18.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0012207\n","\n","я не могу вас за любимой \n","но на работу не могу \n","а здесь у нас тут не вернулся \n","в окно \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[20 / 30] Train: Loss = 4.17483, PPX = 65.03: 100%|██████████| 677/677 [00:24<00:00, 27.31it/s]\n","[20 / 30]   Val: Loss = 3.87082, PPX = 47.98: 100%|██████████| 19/19 [00:00<00:00, 29.44it/s]\n","[21 / 30] Train: Loss = 4.09033, PPX = 59.76:   0%|          | 1/677 [00:00<00:49, 13.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.000305176\n","\n","скажи что ты не понимая \n","а вы мне не было ли в том \n","что я не бью не тот не курит \n","и вот \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[21 / 30] Train: Loss = 4.17510, PPX = 65.05: 100%|██████████| 677/677 [00:24<00:00, 27.65it/s]\n","[21 / 30]   Val: Loss = 3.86961, PPX = 47.92: 100%|██████████| 19/19 [00:00<00:00, 28.30it/s]\n","[22 / 30] Train: Loss = 4.23237, PPX = 68.88:   0%|          | 1/677 [00:00<00:54, 12.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.62939e-05\n","\n","я в ресторане снова видел \n","что я не помню вы просил \n","и этот взгляд теперь не помню \n","как так \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[22 / 30] Train: Loss = 4.17565, PPX = 65.08: 100%|██████████| 677/677 [00:24<00:00, 27.94it/s]\n","[22 / 30]   Val: Loss = 3.87034, PPX = 47.96: 100%|██████████| 19/19 [00:00<00:00, 29.30it/s]\n","[23 / 30] Train: Loss = 4.16677, PPX = 64.51:   0%|          | 1/677 [00:00<00:47, 14.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.90735e-05\n","\n","я не пойму вас за тобою \n","жить в день с утра не тру и быть \n","но в лес но он не видел так и \n","у вас \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[23 / 30] Train: Loss = 4.17475, PPX = 65.02: 100%|██████████| 677/677 [00:24<00:00, 26.57it/s]\n","[23 / 30]   Val: Loss = 3.86996, PPX = 47.94: 100%|██████████| 19/19 [00:00<00:00, 29.46it/s]\n","[24 / 30] Train: Loss = 4.10267, PPX = 60.50:   0%|          | 2/677 [00:00<00:36, 18.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.76837e-06\n","\n","я не хочу чтоб вы не стали \n","пока я так из темноты \n","и так вот только он в тумане \n","не дам \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[24 / 30] Train: Loss = 4.17534, PPX = 65.06: 100%|██████████| 677/677 [00:24<00:00, 27.78it/s]\n","[24 / 30]   Val: Loss = 3.87084, PPX = 47.98: 100%|██████████| 19/19 [00:00<00:00, 30.88it/s]\n","[25 / 30] Train: Loss = 4.20125, PPX = 66.77:   0%|          | 2/677 [00:00<00:35, 19.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.19209e-06\n","\n","олег сказал что он не надо \n","и я по прежнему в петлю \n","а в ней по жизни не находит \n","да ну \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[25 / 30] Train: Loss = 4.17546, PPX = 65.07: 100%|██████████| 677/677 [00:24<00:00, 27.57it/s]\n","[25 / 30]   Val: Loss = 3.87007, PPX = 47.95: 100%|██████████| 19/19 [00:00<00:00, 28.30it/s]\n","[26 / 30] Train: Loss = 4.22384, PPX = 68.29:   0%|          | 2/677 [00:00<00:35, 18.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.98023e-07\n","\n","не то что не было всё ясно \n","и я не знаю что не мог \n","но это всё таки такого \n","не тот \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[26 / 30] Train: Loss = 4.17620, PPX = 65.12: 100%|██████████| 677/677 [00:24<00:00, 27.66it/s]\n","[26 / 30]   Val: Loss = 3.86994, PPX = 47.94: 100%|██████████| 19/19 [00:00<00:00, 29.45it/s]\n","[27 / 30] Train: Loss = 4.04210, PPX = 56.95:   0%|          | 2/677 [00:00<00:36, 18.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.45058e-08\n","\n","в конце концов мы не писали \n","мне кажется что я не рад \n","не надо мне в лицо и вынул \n","в окно \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[27 / 30] Train: Loss = 4.17566, PPX = 65.08: 100%|██████████| 677/677 [00:24<00:00, 27.56it/s]\n","[27 / 30]   Val: Loss = 3.87014, PPX = 47.95: 100%|██████████| 19/19 [00:00<00:00, 30.36it/s]\n","[28 / 30] Train: Loss = 4.16175, PPX = 64.18:   0%|          | 2/677 [00:00<00:35, 18.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.86265e-08\n","\n","не знала что я не такая \n","что я в тот день когда лицом \n","а с ним уже не зная люди \n","в стихах \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[28 / 30] Train: Loss = 4.17572, PPX = 65.09: 100%|██████████| 677/677 [00:24<00:00, 27.84it/s]\n","[28 / 30]   Val: Loss = 3.87033, PPX = 47.96: 100%|██████████| 19/19 [00:00<00:00, 31.26it/s]\n","[29 / 30] Train: Loss = 4.23183, PPX = 68.84:   0%|          | 1/677 [00:00<00:47, 14.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.65661e-09\n","\n","в одном из нас в деревне нету \n","лежит в глазах и в шалаше \n","а ты всё мной в меня сегодня \n","как раз \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[29 / 30] Train: Loss = 4.17691, PPX = 65.16: 100%|██████████| 677/677 [00:24<00:00, 27.88it/s]\n","[29 / 30]   Val: Loss = 3.87030, PPX = 47.96: 100%|██████████| 19/19 [00:00<00:00, 29.36it/s]\n","[30 / 30] Train: Loss = 4.35838, PPX = 78.13:   0%|          | 1/677 [00:00<00:51, 13.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.16415e-09\n","\n","я вас люблю тебя родная \n","я в этот раз свою весь день \n","и что поделать с ними было \n","как все \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[30 / 30] Train: Loss = 4.17517, PPX = 65.05: 100%|██████████| 677/677 [00:24<00:00, 27.76it/s]\n","[30 / 30]   Val: Loss = 3.87039, PPX = 47.96: 100%|██████████| 19/19 [00:00<00:00, 30.78it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.91038e-10\n","\n","я не люблю тебя до бабы \n","что к нам и в рай не в шалаше \n","а не могли бы вы мне стали \n","в раю \n","\n"],"name":"stdout"}]},{"metadata":{"id":"FFI5dOezDxvj","colab_type":"text"},"cell_type":"markdown","source":["### Увеличиваем выборку\n","\n","У нас есть выборка для пирожков, которая заметно больше.\n","\n","**Задание** Обучитесь на ней."]},{"metadata":{"id":"gdavwIn9fQOX","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_poem(path):\n","    poem = []\n","    with open(path, encoding='utf8') as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if len(line) == 0:\n","                yield poem\n","                poem = []\n","                continue\n","            \n","            poem.extend(line.split() + ['\\\\n'])\n","            \n","perashki = list(read_poem('perashki.txt'))\n","poroshki = list(read_poem('poroshki.txt'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kcr4CIuTDygF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"03fc855c-284c-4011-aa29-72ad415fabeb","executionInfo":{"status":"ok","timestamp":1542194451278,"user_tz":-180,"elapsed":1996,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["from torchtext.data import Field, Example, Dataset, BucketIterator\n","\n","text_field = Field(init_token='<s>', eos_token='</s>')\n","        \n","fields = [('text', text_field)]\n","examples = [Example.fromlist([poem], fields) for poem in perashki]\n","dataset = Dataset(examples, fields)\n","\n","text_field.build_vocab(dataset, min_freq=7)\n","\n","print('Vocab size =', len(text_field.vocab))\n","train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n","\n","train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n","                                              shuffle=True, device=DEVICE, sort=False)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Vocab size = 13070\n"],"name":"stdout"}]},{"metadata":{"id":"eYRkgrz4X6rE","colab_type":"code","colab":{}},"cell_type":"code","source":["def sample(probs, temp):\n","    probs = F.log_softmax(probs.squeeze(), dim=0)\n","    probs = (probs / temp).exp()\n","    probs /= probs.sum()\n","    probs = probs.cpu().numpy()\n","\n","    return np.random.choice(np.arange(len(probs)), p=probs)\n","\n","\n","def generate(model, temp=0.6):\n","    model.eval()\n","    with torch.no_grad():        \n","        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n","        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n","        \n","        hidden = None\n","        for _ in range(150):\n","            probs, hidden = model(LongTensor([[prev_token]]), hidden)\n","            prev_token = sample(probs, temp)\n","            \n","            if prev_token == end_token:\n","                return\n","            \n","            if train_iter.dataset.fields['text'].vocab.itos[prev_token] == '\\\\n':\n","                print()\n","            else:\n","                print(train_iter.dataset.fields['text'].vocab.itos[prev_token], end=' ')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"347fwUM9YAhE","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):                \n","                logits, _ = model(batch.text)\n","\n","                targets = torch.cat((batch.text[1:], batch.text.new_ones((1, batch.text.shape[1])))).view(-1)\n","\n","                loss = criterion(logits.view(-1, logits.shape[-1]), targets)\n","                \n","                mask = (1 - ((targets == unk_idx) + (targets == pad_idx))).float()\n","                \n","                loss = (loss * mask).sum() / mask.sum()\n","                \n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n","            \n","            if best_val_loss and val_loss > best_val_loss:\n","                optimizer.param_groups[0]['lr'] /= 4.\n","                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n","            else:\n","                best_val_loss = val_loss\n","        print()\n","        generate(model)\n","        print()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0lcCSLh1YFKD","colab_type":"code","colab":{}},"cell_type":"code","source":["class InputOutputLMModel(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=512, lstm_hidden_dim=512, num_layers=1):\n","        super().__init__()\n","\n","        self._dropout = nn.Dropout(0.4)\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers)\n","        \n","        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n","        self._out_layer.weight = self._emb.weight\n","        \n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._emb.weight.data.uniform_(-init_range, init_range)\n","        self._out_layer.bias.data.zero_()\n","        self._out_layer.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, inputs, hidden=None):\n","        embs = self._emb(inputs)\n","        output, hidden = self._rnn(embs, hidden)\n","        return self._out_layer(self._dropout(output)), hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7hToWnTJD-Qo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2125},"outputId":"54847501-fc42-402f-8a74-f37d5f16231f","executionInfo":{"status":"ok","timestamp":1542195442996,"user_tz":-180,"elapsed":989483,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = InputOutputLMModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=15, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[1 / 15] Train: Loss = 5.44821, PPX = 232.34: 100%|██████████| 1386/1386 [01:03<00:00, 22.24it/s]\n","[1 / 15]   Val: Loss = 4.88946, PPX = 132.88: 100%|██████████| 39/39 [00:01<00:00, 20.02it/s]\n","  0%|          | 0/1386 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я помню но не так и не серёжа \n","а в толпе ни в них не так \n","а я не так и не было \n","и не один и не в меня \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[2 / 15] Train: Loss = 4.79604, PPX = 121.03: 100%|██████████| 1386/1386 [01:03<00:00, 21.76it/s]\n","[2 / 15]   Val: Loss = 4.61948, PPX = 101.44: 100%|██████████| 39/39 [00:01<00:00, 19.22it/s]\n","  0%|          | 1/1386 [00:00<02:48,  8.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","олег не с николаем в бога \n","и утром в понедельник и \n","на ночь висит на стенах месте \n","а в жизни в жизни не смотреть \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[3 / 15] Train: Loss = 4.54293, PPX = 93.97: 100%|██████████| 1386/1386 [01:03<00:00, 22.08it/s]\n","[3 / 15]   Val: Loss = 4.47807, PPX = 88.06: 100%|██████████| 39/39 [00:02<00:00, 19.38it/s]\n","  0%|          | 0/1386 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я не могу пишу я плачу \n","и в детстве с мамой я не знал \n","но я не знаю что не знаю \n","вот вот и это вот и вот \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[4 / 15] Train: Loss = 4.37721, PPX = 79.62: 100%|██████████| 1386/1386 [01:03<00:00, 21.68it/s]\n","[4 / 15]   Val: Loss = 4.40277, PPX = 81.68: 100%|██████████| 39/39 [00:01<00:00, 19.65it/s]\n","  0%|          | 0/1386 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","а в тот же миг когда все фото \n","я сразу понял что вы мне \n","но всё равно она не понимаешь \n","как будто я не был в субботу \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[5 / 15] Train: Loss = 4.25575, PPX = 70.51: 100%|██████████| 1386/1386 [01:03<00:00, 21.84it/s]\n","[5 / 15]   Val: Loss = 4.34896, PPX = 77.40: 100%|██████████| 39/39 [00:02<00:00, 19.40it/s]\n","[6 / 15] Train: Loss = 4.05658, PPX = 57.78:   0%|          | 1/1386 [00:00<02:38,  8.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я не люблю я вас умею \n","я тоже только потому \n","что ты не знаешь я не верю \n","я буду вас не говорю \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[6 / 15] Train: Loss = 4.11665, PPX = 61.35: 100%|██████████| 1386/1386 [01:03<00:00, 22.06it/s]\n","[6 / 15]   Val: Loss = 4.31348, PPX = 74.70: 100%|██████████| 39/39 [00:02<00:00, 19.59it/s]\n","  0%|          | 1/1386 [00:00<02:40,  8.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","олег с утра кричит оксана \n","я не люблю тебя люблю \n","и ты мне в рот и так и так же \n","и не и ни при чем не знал \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[7 / 15] Train: Loss = 4.02270, PPX = 55.85: 100%|██████████| 1386/1386 [01:04<00:00, 21.55it/s]\n","[7 / 15]   Val: Loss = 4.31897, PPX = 75.11: 100%|██████████| 39/39 [00:01<00:00, 20.55it/s]\n","[8 / 15] Train: Loss = 3.79874, PPX = 44.64:   0%|          | 1/1386 [00:00<02:44,  8.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5\n","\n","я не могу заснуть за маму \n","сказал директор и орет \n","на нас понять что в этом доме \n","не хочет ли тебя люблю \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[8 / 15] Train: Loss = 3.66047, PPX = 38.88: 100%|██████████| 1386/1386 [01:03<00:00, 21.68it/s]\n","[8 / 15]   Val: Loss = 4.18785, PPX = 65.88: 100%|██████████| 39/39 [00:02<00:00, 19.32it/s]\n","[9 / 15] Train: Loss = 3.33657, PPX = 28.12:   0%|          | 1/1386 [00:00<02:46,  8.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я умер умер умер умер \n","и я не знаю почему \n","мне нужно срочно дать по делу \n","я все в порядке с шаурмой \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 15] Train: Loss = 3.52019, PPX = 33.79: 100%|██████████| 1386/1386 [01:04<00:00, 21.43it/s]\n","[9 / 15]   Val: Loss = 4.17448, PPX = 65.01: 100%|██████████| 39/39 [00:01<00:00, 19.57it/s]\n","[10 / 15] Train: Loss = 3.31210, PPX = 27.44:   0%|          | 1/1386 [00:00<02:48,  8.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я в общий доступ покупаю \n","сказал мне старый капитан \n","и я представил что у нас их \n","и что то тихо как всегда \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 15] Train: Loss = 3.43337, PPX = 30.98: 100%|██████████| 1386/1386 [01:04<00:00, 21.61it/s]\n","[10 / 15]   Val: Loss = 4.17296, PPX = 64.91: 100%|██████████| 39/39 [00:01<00:00, 19.60it/s]\n","[11 / 15] Train: Loss = 3.35736, PPX = 28.71:   0%|          | 1/1386 [00:00<02:47,  8.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","когда то ты была в постели \n","но мне не нравится меня \n","и я в ответ ему на карте \n","в котором он меня узнал \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[11 / 15] Train: Loss = 3.35757, PPX = 28.72: 100%|██████████| 1386/1386 [01:03<00:00, 21.88it/s]\n","[11 / 15]   Val: Loss = 4.16397, PPX = 64.33: 100%|██████████| 39/39 [00:01<00:00, 19.74it/s]\n","  0%|          | 0/1386 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","у нас в деревне столько снега \n","когда у нас случился смерть \n","и в целом как то что то в сердце \n","мы стали в камере играть \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 15] Train: Loss = 3.28752, PPX = 26.78: 100%|██████████| 1386/1386 [01:03<00:00, 21.92it/s]\n","[12 / 15]   Val: Loss = 4.16468, PPX = 64.37: 100%|██████████| 39/39 [00:01<00:00, 19.73it/s]\n","  0%|          | 0/1386 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.25\n","\n","я не люблю тебя я в жизни \n","то сразу понял что она \n","не только с кем то в этой жизни \n","что мы не стали бы и все \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 15] Train: Loss = 3.11293, PPX = 22.49: 100%|██████████| 1386/1386 [01:03<00:00, 21.95it/s]\n","[13 / 15]   Val: Loss = 4.15302, PPX = 63.63: 100%|██████████| 39/39 [00:01<00:00, 19.72it/s]\n","  0%|          | 1/1386 [00:00<02:48,  8.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я так люблю тебя родная \n","что даже к смерти и в метро \n","и в ванной в зеркало да в небо \n","а ты как будто бы не так \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 15] Train: Loss = 3.06449, PPX = 21.42: 100%|██████████| 1386/1386 [01:03<00:00, 21.91it/s]\n","[14 / 15]   Val: Loss = 4.15577, PPX = 63.80: 100%|██████████| 39/39 [00:01<00:00, 20.02it/s]\n","[15 / 15] Train: Loss = 3.15733, PPX = 23.51:   0%|          | 1/1386 [00:00<02:42,  8.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.3125\n","\n","я умер а пока что умер \n","а я уже не осознал \n","как ты и знаешь что не любишь \n","и как меня не узнаю \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 15] Train: Loss = 3.00624, PPX = 20.21: 100%|██████████| 1386/1386 [01:02<00:00, 22.10it/s]\n","[15 / 15]   Val: Loss = 4.16203, PPX = 64.20: 100%|██████████| 39/39 [00:01<00:00, 20.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.078125\n","\n","когда мы с пушкиным бежали \n","в столице на меня смотреть \n","я в этот день не представляю \n","а в принципе я не хочу \n","\n"],"name":"stdout"}]},{"metadata":{"id":"MBX4NjzZ-0Hc","colab_type":"text"},"cell_type":"markdown","source":["### Transfer learning\n","\n","Простой и приятный способ улучшения модели - сделать перенос обученной на большом корпусе модели на меньшего объема датасет.\n","\n","Популярен этот способ больше в компьютерном зрении: [Transfer learning, cs231n](http://cs231n.github.io/transfer-learning/) - там есть огромный ImageNet, на котором предобучают модель, чтобы потом заморозить нижние слои и заменить выходные. В итоге модель использует универсальные представления данных, выученные на большом корпусе, но для предсказания совсем других меток - и качество очень здорово растет.\n","\n","Нам такие извращения пока не нужны (хотя потом пригодятся, ключевые слова: ULMFiT, ELMo и компания). Просто возьмем обученную на большем корпусе модель и поучим ее на меньшем корпусе. Ей всего-то нужно новый матрический шаблон последней строки выучить.\n","\n","**Задание** Обученную в прошлом пункте модель дообучите на порошки."]},{"metadata":{"id":"6WkcuiT7nqWm","colab_type":"code","colab":{}},"cell_type":"code","source":["from torchtext.data import Field, Example, Dataset, BucketIterator\n","        \n","fields = [('text', text_field)]\n","examples = [Example.fromlist([poem], fields) for poem in poroshki]\n","dataset = Dataset(examples, fields)\n","\n","train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n","\n","train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n","                                              shuffle=True, device=DEVICE, sort=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wfsk7-W-j0u5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4097},"outputId":"11088510-225f-46e3-afc7-a6607e075b05","executionInfo":{"status":"ok","timestamp":1542196777687,"user_tz":-180,"elapsed":896308,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[1 / 30] Train: Loss = 4.74858, PPX = 115.42: 100%|██████████| 677/677 [00:29<00:00, 23.02it/s]\n","[1 / 30]   Val: Loss = 4.36804, PPX = 78.89: 100%|██████████| 19/19 [00:00<00:00, 21.69it/s]\n","[2 / 30] Train: Loss = 4.59046, PPX = 98.54:   0%|          | 2/677 [00:00<00:38, 17.40it/s] "],"name":"stderr"},{"output_type":"stream","text":["\n","олег не умер это умер \n","но не к тебе а в смысле я \n","а то ли на людей всё то же \n","теперь \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[2 / 30] Train: Loss = 4.55488, PPX = 95.10: 100%|██████████| 677/677 [00:29<00:00, 23.32it/s]\n","[2 / 30]   Val: Loss = 4.32254, PPX = 75.38: 100%|██████████| 19/19 [00:00<00:00, 21.16it/s]\n","[3 / 30] Train: Loss = 4.47910, PPX = 88.16:   0%|          | 2/677 [00:00<00:40, 16.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я вел себя на жабры с пивом \n","и с каждым годом все с пятном \n","и с каждой строчкой люди ближе \n","и ходят голые стволы \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[3 / 30] Train: Loss = 4.49813, PPX = 89.85: 100%|██████████| 677/677 [00:29<00:00, 23.21it/s]\n","[3 / 30]   Val: Loss = 4.29612, PPX = 73.41: 100%|██████████| 19/19 [00:00<00:00, 21.18it/s]\n","[4 / 30] Train: Loss = 4.53873, PPX = 93.57:   0%|          | 2/677 [00:00<00:40, 16.58it/s] "],"name":"stderr"},{"output_type":"stream","text":["\n","когда ты в новом классе \n","мне было двадцать пять рублей \n","и я твой долгожданный секс с улыбкой \n","по скрипкой \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[4 / 30] Train: Loss = 4.46339, PPX = 86.78: 100%|██████████| 677/677 [00:28<00:00, 23.36it/s]\n","[4 / 30]   Val: Loss = 4.28093, PPX = 72.31: 100%|██████████| 19/19 [00:00<00:00, 21.59it/s]\n","[5 / 30] Train: Loss = 4.65945, PPX = 105.58:   0%|          | 2/677 [00:00<00:43, 15.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я в загсе на убийцу поиск \n","но секс же не совсем не зря \n","и вот уже не из меня а \n","не знаю \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[5 / 30] Train: Loss = 4.43414, PPX = 84.28: 100%|██████████| 677/677 [00:28<00:00, 23.43it/s]\n","[5 / 30]   Val: Loss = 4.26685, PPX = 71.30: 100%|██████████| 19/19 [00:00<00:00, 21.75it/s]\n","[6 / 30] Train: Loss = 4.33706, PPX = 76.48:   0%|          | 2/677 [00:00<00:41, 16.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","меня не надо в этих суток \n","не в смысле я не по любви \n","а это просто я не знаю \n","и даже \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[6 / 30] Train: Loss = 4.41082, PPX = 82.34: 100%|██████████| 677/677 [00:28<00:00, 23.38it/s]\n","[6 / 30]   Val: Loss = 4.25700, PPX = 70.60: 100%|██████████| 19/19 [00:00<00:00, 20.97it/s]\n","[7 / 30] Train: Loss = 4.43128, PPX = 84.04:   0%|          | 2/677 [00:00<00:41, 16.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я напишу тебе что больше \n","и не в меня не просто так \n","когда я был в большой квартире \n","в нём \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[7 / 30] Train: Loss = 4.39252, PPX = 80.84: 100%|██████████| 677/677 [00:28<00:00, 23.15it/s]\n","[7 / 30]   Val: Loss = 4.24964, PPX = 70.08: 100%|██████████| 19/19 [00:00<00:00, 21.20it/s]\n","[8 / 30] Train: Loss = 4.37976, PPX = 79.82:   0%|          | 2/677 [00:00<00:40, 16.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я не люблю сказал геннадий \n","а ты и сразу же и ты \n","а я не знаю что с тобой я \n","и не \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[8 / 30] Train: Loss = 4.37853, PPX = 79.72: 100%|██████████| 677/677 [00:28<00:00, 24.02it/s]\n","[8 / 30]   Val: Loss = 4.24183, PPX = 69.53: 100%|██████████| 19/19 [00:00<00:00, 21.30it/s]\n","[9 / 30] Train: Loss = 4.33858, PPX = 76.60:   0%|          | 2/677 [00:00<00:40, 16.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я не люблю когда ты в жопу \n","и я в моей руке держу \n","а вот на мне не надо было \n","и не \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 30] Train: Loss = 4.36379, PPX = 78.55: 100%|██████████| 677/677 [00:28<00:00, 24.11it/s]\n","[9 / 30]   Val: Loss = 4.23552, PPX = 69.10: 100%|██████████| 19/19 [00:00<00:00, 21.48it/s]\n","[10 / 30] Train: Loss = 4.18628, PPX = 65.78:   0%|          | 2/677 [00:00<00:40, 16.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я не умел быть по другому \n","и не хочу в себе а не \n","на главную в конец дороги \n","с тобой \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 30] Train: Loss = 4.35092, PPX = 77.55: 100%|██████████| 677/677 [00:28<00:00, 23.27it/s]\n","[10 / 30]   Val: Loss = 4.22951, PPX = 68.68: 100%|██████████| 19/19 [00:00<00:00, 21.57it/s]\n","[11 / 30] Train: Loss = 4.51939, PPX = 91.78:   0%|          | 2/677 [00:00<00:46, 14.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","да что вы знаете о жизни \n","не только в жизни а не мы \n","а вы \n","не я \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[11 / 30] Train: Loss = 4.33829, PPX = 76.58: 100%|██████████| 677/677 [00:28<00:00, 23.61it/s]\n","[11 / 30]   Val: Loss = 4.22406, PPX = 68.31: 100%|██████████| 19/19 [00:00<00:00, 21.70it/s]\n","[12 / 30] Train: Loss = 4.35976, PPX = 78.24:   0%|          | 2/677 [00:00<00:39, 16.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я вас люблю сказал аркадий \n","и я не верю мы друзья \n","но тут как в детстве без пожара \n","но не \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 30] Train: Loss = 4.32917, PPX = 75.88: 100%|██████████| 677/677 [00:28<00:00, 23.91it/s]\n","[12 / 30]   Val: Loss = 4.21899, PPX = 67.97: 100%|██████████| 19/19 [00:00<00:00, 21.00it/s]\n","[13 / 30] Train: Loss = 4.45063, PPX = 85.68:   0%|          | 2/677 [00:00<00:42, 16.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","когда ты мне не отвечаешь \n","а я не знаю что сказать \n","и просто так что был на этот \n","я не \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 30] Train: Loss = 4.31905, PPX = 75.12: 100%|██████████| 677/677 [00:28<00:00, 23.44it/s]\n","[13 / 30]   Val: Loss = 4.21464, PPX = 67.67: 100%|██████████| 19/19 [00:00<00:00, 21.25it/s]\n","[14 / 30] Train: Loss = 4.33561, PPX = 76.37:   0%|          | 2/677 [00:00<00:40, 16.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","а он лежит на дне рожденья \n","в них больше нет ни в ад ни зря \n","не первый в жизни как то просто \n","не в смысле \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 30] Train: Loss = 4.30686, PPX = 74.21: 100%|██████████| 677/677 [00:28<00:00, 23.64it/s]\n","[14 / 30]   Val: Loss = 4.21129, PPX = 67.44: 100%|██████████| 19/19 [00:00<00:00, 21.32it/s]\n","[15 / 30] Train: Loss = 4.20630, PPX = 67.11:   0%|          | 2/677 [00:00<00:41, 16.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","а я не скажет на работе \n","не в смысле помню что с женой \n","и как же ты его подруга \n","не в шутку \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 30] Train: Loss = 4.30034, PPX = 73.72: 100%|██████████| 677/677 [00:28<00:00, 23.39it/s]\n","[15 / 30]   Val: Loss = 4.20773, PPX = 67.20: 100%|██████████| 19/19 [00:00<00:00, 21.23it/s]\n","[16 / 30] Train: Loss = 4.40807, PPX = 82.11:   0%|          | 2/677 [00:00<00:39, 17.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","мой дядя самых честных правил \n","на всякий случай в мире мир \n","и я был не для вас в кармане \n","не верю \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[16 / 30] Train: Loss = 4.29228, PPX = 73.13: 100%|██████████| 677/677 [00:28<00:00, 23.40it/s]\n","[16 / 30]   Val: Loss = 4.20449, PPX = 66.99: 100%|██████████| 19/19 [00:00<00:00, 21.45it/s]\n","[17 / 30] Train: Loss = 4.23113, PPX = 68.80:   0%|          | 2/677 [00:00<00:40, 16.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","с тобой как в морг на всех и грозы \n","в него как в марте в час \n","не для меня по жизни по \n","не очень \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[17 / 30] Train: Loss = 4.28505, PPX = 72.61: 100%|██████████| 677/677 [00:28<00:00, 23.91it/s]\n","[17 / 30]   Val: Loss = 4.20032, PPX = 66.71: 100%|██████████| 19/19 [00:00<00:00, 21.36it/s]\n","[18 / 30] Train: Loss = 4.24631, PPX = 69.85:   0%|          | 2/677 [00:00<00:40, 16.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","мы жили в мире у поэта \n","и все на мне и не хотят \n","и где то там в него не видно \n","как ты \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[18 / 30] Train: Loss = 4.27617, PPX = 71.96: 100%|██████████| 677/677 [00:28<00:00, 23.19it/s]\n","[18 / 30]   Val: Loss = 4.19724, PPX = 66.50: 100%|██████████| 19/19 [00:00<00:00, 21.87it/s]\n","[19 / 30] Train: Loss = 4.16918, PPX = 64.66:   0%|          | 2/677 [00:00<00:40, 16.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я видел в этом всё и не то \n","то вдруг не знаю как любовь \n","а он как мне не будеш сука \n","ну ты \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[19 / 30] Train: Loss = 4.27026, PPX = 71.54: 100%|██████████| 677/677 [00:28<00:00, 24.16it/s]\n","[19 / 30]   Val: Loss = 4.19374, PPX = 66.27: 100%|██████████| 19/19 [00:00<00:00, 21.30it/s]\n","[20 / 30] Train: Loss = 4.10126, PPX = 60.42:   0%|          | 2/677 [00:00<00:39, 17.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я в дверь себя на этом свете \n","читаю в пять утра и днём \n","в моей руке держу вагоны \n","и в снах \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[20 / 30] Train: Loss = 4.26179, PPX = 70.94: 100%|██████████| 677/677 [00:28<00:00, 23.37it/s]\n","[20 / 30]   Val: Loss = 4.19152, PPX = 66.12: 100%|██████████| 19/19 [00:00<00:00, 21.23it/s]\n","[21 / 30] Train: Loss = 4.02923, PPX = 56.22:   0%|          | 2/677 [00:00<00:41, 16.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","а вот и дети не уходят \n","и не евгений и не то \n","а у оксаны не хватает \n","а даже \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[21 / 30] Train: Loss = 4.25522, PPX = 70.47: 100%|██████████| 677/677 [00:28<00:00, 22.80it/s]\n","[21 / 30]   Val: Loss = 4.18762, PPX = 65.87: 100%|██████████| 19/19 [00:00<00:00, 20.89it/s]\n","[22 / 30] Train: Loss = 4.19591, PPX = 66.41:   0%|          | 2/677 [00:00<00:39, 16.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в моей квартире не боялся \n","в саду \n","и даже не хватает в нём же \n","на пять \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[22 / 30] Train: Loss = 4.24838, PPX = 69.99: 100%|██████████| 677/677 [00:28<00:00, 23.72it/s]\n","[22 / 30]   Val: Loss = 4.18421, PPX = 65.64: 100%|██████████| 19/19 [00:00<00:00, 21.70it/s]\n","[23 / 30] Train: Loss = 4.05325, PPX = 57.58:   0%|          | 2/677 [00:00<00:41, 16.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в столовой в бочке у олега \n","сидит на трассе киев \n","а в нём как май еще и лето \n","и вот \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[23 / 30] Train: Loss = 4.24314, PPX = 69.63: 100%|██████████| 677/677 [00:29<00:00, 23.53it/s]\n","[23 / 30]   Val: Loss = 4.18311, PPX = 65.57: 100%|██████████| 19/19 [00:00<00:00, 21.55it/s]\n","[24 / 30] Train: Loss = 4.28278, PPX = 72.44:   0%|          | 2/677 [00:00<00:40, 16.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","ты не рассказывай нам правит \n","олег сегодня не прошёл \n","а я не так сегодня в полночь \n","и не \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[24 / 30] Train: Loss = 4.23731, PPX = 69.22: 100%|██████████| 677/677 [00:29<00:00, 23.59it/s]\n","[24 / 30]   Val: Loss = 4.17965, PPX = 65.34: 100%|██████████| 19/19 [00:00<00:00, 21.34it/s]\n","[25 / 30] Train: Loss = 4.42059, PPX = 83.15:   0%|          | 2/677 [00:00<00:41, 16.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","а если бы не в шутку \n","то он ещё не завели \n","а я я тоже был уверен \n","не будет \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[25 / 30] Train: Loss = 4.22878, PPX = 68.63: 100%|██████████| 677/677 [00:28<00:00, 23.56it/s]\n","[25 / 30]   Val: Loss = 4.17614, PPX = 65.11: 100%|██████████| 19/19 [00:00<00:00, 21.36it/s]\n","[26 / 30] Train: Loss = 4.32943, PPX = 75.90:   0%|          | 2/677 [00:00<00:39, 17.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","в лесу я в душу не в разводе \n","и так и даже не люблю \n","но даже вот уже не спится \n","уже \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[26 / 30] Train: Loss = 4.22402, PPX = 68.31: 100%|██████████| 677/677 [00:28<00:00, 23.27it/s]\n","[26 / 30]   Val: Loss = 4.17428, PPX = 64.99: 100%|██████████| 19/19 [00:00<00:00, 21.74it/s]\n","[27 / 30] Train: Loss = 4.30430, PPX = 74.02:   0%|          | 2/677 [00:00<00:42, 15.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я не люблю а я не буду \n","быть может быть иль не хочу \n","но не родился в этом мире \n","не в силах \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[27 / 30] Train: Loss = 4.22047, PPX = 68.07: 100%|██████████| 677/677 [00:28<00:00, 23.83it/s]\n","[27 / 30]   Val: Loss = 4.17215, PPX = 64.85: 100%|██████████| 19/19 [00:00<00:00, 21.50it/s]\n","[28 / 30] Train: Loss = 3.98147, PPX = 53.60:   0%|          | 2/677 [00:00<00:41, 16.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","оксана подошла к олегу \n","в глаза и не на юг в лицо \n","а в зале по лицу с начинкой \n","на вкус \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[28 / 30] Train: Loss = 4.21331, PPX = 67.58: 100%|██████████| 677/677 [00:28<00:00, 22.76it/s]\n","[28 / 30]   Val: Loss = 4.17046, PPX = 64.75: 100%|██████████| 19/19 [00:00<00:00, 21.15it/s]\n","[29 / 30] Train: Loss = 4.19819, PPX = 66.57:   0%|          | 1/677 [00:00<00:57, 11.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","когда ты будешь спать с порога \n","я буду всё на свете зла \n","но я не стала и с собою \n","и в воду \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[29 / 30] Train: Loss = 4.20872, PPX = 67.27: 100%|██████████| 677/677 [00:29<00:00, 23.65it/s]\n","[29 / 30]   Val: Loss = 4.16738, PPX = 64.55: 100%|██████████| 19/19 [00:00<00:00, 21.25it/s]\n","[30 / 30] Train: Loss = 4.10489, PPX = 60.64:   0%|          | 2/677 [00:00<00:41, 16.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","я по утрам хожу в деревне \n","до самой смерти где \n","теперь ты в нём же в рот не знаешь \n","но даже им не в шутку \n","\n"],"name":"stdout"},{"output_type":"stream","text":["[30 / 30] Train: Loss = 4.20388, PPX = 66.95: 100%|██████████| 677/677 [00:29<00:00, 23.47it/s]\n","[30 / 30]   Val: Loss = 4.16396, PPX = 64.33: 100%|██████████| 19/19 [00:00<00:00, 20.81it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["\n","я не могу тебе на свете \n","не только в теле никогда \n","и только ты не отвечаешь \n","в пивной \n","\n"],"name":"stdout"}]},{"metadata":{"id":"agshwfiDea7D","colab_type":"text"},"cell_type":"markdown","source":["### Conditional language model\n","\n","Ещё лучше - просто учиться на обоих корпусах сразу. Объедините пирожки и порошки, для каждого храните индекс 0/1 - был ли это пирожок или порошок. Добавьте вход - этот индекс и конкатенируйте его либо к каждому эмбеддингу слов, либо к каждому выходу из LSTM.\n","\n","**Задание** Научите единую модель, у которой можно просить сгенерировать пирожок или порошок."]},{"metadata":{"id":"QU49TeEqoKcF","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_poem(path):\n","    poem = []\n","    with open(path, encoding='utf8') as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if len(line) == 0:\n","                yield poem\n","                poem = []\n","                continue\n","            \n","            poem.extend(line.split() + ['\\\\n'])\n","            \n","perashki = list(read_poem('perashki.txt'))\n","poroshki = list(read_poem('poroshki.txt'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kzcNfoqPpmAA","colab_type":"code","colab":{}},"cell_type":"code","source":["labels = [0] * len(perashki) + [1] * len(poroshki)\n","data = perashki + poroshki"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KgjOkDKronZJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"2b651ff9-1cc4-4fa8-bc85-8661da6b7324","executionInfo":{"status":"ok","timestamp":1542206688348,"user_tz":-180,"elapsed":2448,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["from torchtext.data import Field, Example, Dataset, BucketIterator\n","\n","text_field = Field(init_token='<s>', eos_token='</s>')\n","label_field = Field(sequential=False)\n","        \n","fields = [('text', text_field), ('label', label_field)]\n","examples = [Example.fromlist([poem, label], fields) for poem, label in zip(data, labels)]\n","dataset = Dataset(examples, fields)\n","\n","text_field.build_vocab(dataset, min_freq=7)\n","label_field.build_vocab(dataset)\n","\n","print('Text vocab size =', len(text_field.vocab))\n","print('Label vocab size =', len(label_field.vocab))\n","train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n","\n","train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n","                                              shuffle=True, device=DEVICE, sort=False)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Text vocab size = 18260\n","Label vocab size = 3\n"],"name":"stdout"}]},{"metadata":{"id":"5a0SglfEqMUo","colab_type":"code","colab":{}},"cell_type":"code","source":["def sample(probs, temp):\n","    probs = F.log_softmax(probs.squeeze(), dim=0)\n","    probs = (probs / temp).exp()\n","    probs /= probs.sum()\n","    probs = probs.cpu().numpy()\n","\n","    return np.random.choice(np.arange(len(probs)), p=probs)\n","\n","\n","def generate(model, label, temp=0.6):\n","    model.eval()\n","    with torch.no_grad():        \n","        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n","        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n","        \n","        hidden = None\n","        for _ in range(150):\n","            probs, hidden = model(LongTensor([[prev_token]]), LongTensor([label]),hidden)\n","            prev_token = sample(probs, temp)\n","            \n","            if prev_token == end_token:\n","                return\n","            \n","            if train_iter.dataset.fields['text'].vocab.itos[prev_token] == '\\\\n':\n","                print()\n","            else:\n","                print(train_iter.dataset.fields['text'].vocab.itos[prev_token], end=' ')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zxpoNy1dqQWt","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):                \n","                logits, _ = model(batch.text, batch.label)\n","\n","                targets = torch.cat((batch.text[1:], batch.text.new_ones((1, batch.text.shape[1])))).view(-1)\n","\n","                loss = criterion(logits.view(-1, logits.shape[-1]), targets)\n","                \n","                mask = (1 - ((targets == unk_idx) + (targets == pad_idx))).float()\n","                \n","                loss = (loss * mask).sum() / mask.sum()\n","                \n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n","            \n","            if best_val_loss and val_loss > best_val_loss:\n","                optimizer.param_groups[0]['lr'] /= 4.\n","                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n","            else:\n","                best_val_loss = val_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jSxvQTHzqSUE","colab_type":"code","colab":{}},"cell_type":"code","source":["class LargeModel(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=512, lstm_hidden_dim=512, num_layers=1):\n","        super().__init__()\n","\n","        self._dropout = nn.Dropout(0.4)\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers)\n","        \n","        self._out_layer = nn.Linear(lstm_hidden_dim+1, vocab_size)\n","        \n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._emb.weight.data.uniform_(-init_range, init_range)\n","        self._out_layer.bias.data.zero_()\n","        self._out_layer.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, inputs, labels, hidden=None):\n","        embs = self._emb(inputs)\n","        output, hidden = self._rnn(embs, hidden)\n","        labels = labels.unsqueeze(0).unsqueeze(2).expand(output.shape[0], output.shape[1], 1).float()\n","        output = torch.cat((output, labels), -1)\n","        return self._out_layer(self._dropout(output)), hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eKR_JSkEqUL5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1343},"outputId":"348aed96-0c7d-4694-d84d-328865d4a324","executionInfo":{"status":"ok","timestamp":1542200573794,"user_tz":-180,"elapsed":3718273,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = LargeModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[1 / 30] Train: Loss = 5.83513, PPX = 342.11: 100%|██████████| 2063/2063 [02:00<00:00, 17.13it/s]\n","[1 / 30]   Val: Loss = 5.28998, PPX = 198.34: 100%|██████████| 58/58 [00:03<00:00, 15.58it/s]\n","[2 / 30] Train: Loss = 5.00738, PPX = 149.51: 100%|██████████| 2063/2063 [02:00<00:00, 17.43it/s]\n","[2 / 30]   Val: Loss = 4.87838, PPX = 131.42: 100%|██████████| 58/58 [00:03<00:00, 15.63it/s]\n","[3 / 30] Train: Loss = 4.79145, PPX = 120.48: 100%|██████████| 2063/2063 [02:01<00:00, 17.08it/s]\n","[3 / 30]   Val: Loss = 4.65493, PPX = 105.10: 100%|██████████| 58/58 [00:03<00:00, 15.35it/s]\n","[4 / 30] Train: Loss = 4.62869, PPX = 102.38: 100%|██████████| 2063/2063 [02:01<00:00, 16.96it/s]\n","[4 / 30]   Val: Loss = 4.66477, PPX = 106.14: 100%|██████████| 58/58 [00:03<00:00, 15.58it/s]\n","[5 / 30] Train: Loss = 4.35676, PPX = 78.00:   0%|          | 1/2063 [00:00<05:48,  5.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5\n"],"name":"stdout"},{"output_type":"stream","text":["[5 / 30] Train: Loss = 4.26556, PPX = 71.20: 100%|██████████| 2063/2063 [02:00<00:00, 17.08it/s]\n","[5 / 30]   Val: Loss = 4.46601, PPX = 87.01: 100%|██████████| 58/58 [00:03<00:00, 15.64it/s]\n","[6 / 30] Train: Loss = 4.16846, PPX = 64.62: 100%|██████████| 2063/2063 [02:00<00:00, 17.78it/s]\n","[6 / 30]   Val: Loss = 4.44335, PPX = 85.06: 100%|██████████| 58/58 [00:03<00:00, 15.88it/s]\n","[7 / 30] Train: Loss = 4.09397, PPX = 59.98: 100%|██████████| 2063/2063 [02:00<00:00, 17.27it/s]\n","[7 / 30]   Val: Loss = 4.42385, PPX = 83.42: 100%|██████████| 58/58 [00:03<00:00, 15.40it/s]\n","[8 / 30] Train: Loss = 4.02200, PPX = 55.81: 100%|██████████| 2063/2063 [02:00<00:00, 17.27it/s]\n","[8 / 30]   Val: Loss = 4.41242, PPX = 82.47: 100%|██████████| 58/58 [00:03<00:00, 15.69it/s]\n","[9 / 30] Train: Loss = 3.95195, PPX = 52.04: 100%|██████████| 2063/2063 [02:00<00:00, 17.14it/s]\n","[9 / 30]   Val: Loss = 4.41333, PPX = 82.54: 100%|██████████| 58/58 [00:03<00:00, 15.71it/s]\n","[10 / 30] Train: Loss = 3.89562, PPX = 49.19:   0%|          | 1/2063 [00:00<05:56,  5.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.25\n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 30] Train: Loss = 3.78405, PPX = 43.99: 100%|██████████| 2063/2063 [02:00<00:00, 17.01it/s]\n","[10 / 30]   Val: Loss = 4.38570, PPX = 80.29: 100%|██████████| 58/58 [00:03<00:00, 15.85it/s]\n","[11 / 30] Train: Loss = 3.73921, PPX = 42.06: 100%|██████████| 2063/2063 [01:59<00:00, 17.50it/s]\n","[11 / 30]   Val: Loss = 4.38487, PPX = 80.23: 100%|██████████| 58/58 [00:03<00:00, 15.70it/s]\n","[12 / 30] Train: Loss = 3.70546, PPX = 40.67: 100%|██████████| 2063/2063 [01:59<00:00, 17.23it/s]\n","[12 / 30]   Val: Loss = 4.38802, PPX = 80.48: 100%|██████████| 58/58 [00:03<00:00, 15.74it/s]\n","[13 / 30] Train: Loss = 3.80151, PPX = 44.77:   0%|          | 1/2063 [00:00<06:07,  5.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.3125\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 30] Train: Loss = 3.64560, PPX = 38.31: 100%|██████████| 2063/2063 [01:59<00:00, 16.97it/s]\n","[13 / 30]   Val: Loss = 4.38772, PPX = 80.46: 100%|██████████| 58/58 [00:03<00:00, 15.64it/s]\n","[14 / 30] Train: Loss = 3.70709, PPX = 40.73:   0%|          | 1/2063 [00:00<05:59,  5.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.078125\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 30] Train: Loss = 3.62708, PPX = 37.60: 100%|██████████| 2063/2063 [02:00<00:00, 17.26it/s]\n","[14 / 30]   Val: Loss = 4.38621, PPX = 80.34: 100%|██████████| 58/58 [00:03<00:00, 15.77it/s]\n","[15 / 30] Train: Loss = 3.76748, PPX = 43.27:   0%|          | 1/2063 [00:00<05:57,  5.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0195312\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 30] Train: Loss = 3.62112, PPX = 37.38: 100%|██████████| 2063/2063 [01:59<00:00, 17.51it/s]\n","[15 / 30]   Val: Loss = 4.38574, PPX = 80.30: 100%|██████████| 58/58 [00:03<00:00, 15.85it/s]\n","[16 / 30] Train: Loss = 3.70641, PPX = 40.71:   0%|          | 1/2063 [00:00<05:35,  6.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.00488281\n"],"name":"stdout"},{"output_type":"stream","text":["[16 / 30] Train: Loss = 3.61971, PPX = 37.33: 100%|██████████| 2063/2063 [01:59<00:00, 17.11it/s]\n","[16 / 30]   Val: Loss = 4.38697, PPX = 80.40: 100%|██████████| 58/58 [00:03<00:00, 15.59it/s]\n","[17 / 30] Train: Loss = 3.66335, PPX = 38.99:   0%|          | 1/2063 [00:00<05:56,  5.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0012207\n"],"name":"stdout"},{"output_type":"stream","text":["[17 / 30] Train: Loss = 3.61951, PPX = 37.32: 100%|██████████| 2063/2063 [02:00<00:00, 17.62it/s]\n","[17 / 30]   Val: Loss = 4.38729, PPX = 80.42: 100%|██████████| 58/58 [00:03<00:00, 15.53it/s]\n","[18 / 30] Train: Loss = 3.54564, PPX = 34.66:   0%|          | 1/2063 [00:00<05:55,  5.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.000305176\n"],"name":"stdout"},{"output_type":"stream","text":["[18 / 30] Train: Loss = 3.61942, PPX = 37.32: 100%|██████████| 2063/2063 [01:59<00:00, 16.84it/s]\n","[18 / 30]   Val: Loss = 4.38655, PPX = 80.36: 100%|██████████| 58/58 [00:03<00:00, 15.60it/s]\n","[19 / 30] Train: Loss = 3.57473, PPX = 35.69:   0%|          | 1/2063 [00:00<06:02,  5.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.62939e-05\n"],"name":"stdout"},{"output_type":"stream","text":["[19 / 30] Train: Loss = 3.61957, PPX = 37.32: 100%|██████████| 2063/2063 [02:00<00:00, 17.10it/s]\n","[19 / 30]   Val: Loss = 4.38513, PPX = 80.25: 100%|██████████| 58/58 [00:03<00:00, 15.67it/s]\n","[20 / 30] Train: Loss = 3.47910, PPX = 32.43:   0%|          | 1/2063 [00:00<06:06,  5.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.90735e-05\n"],"name":"stdout"},{"output_type":"stream","text":["[20 / 30] Train: Loss = 3.61912, PPX = 37.30: 100%|██████████| 2063/2063 [02:00<00:00, 17.41it/s]\n","[20 / 30]   Val: Loss = 4.38524, PPX = 80.26: 100%|██████████| 58/58 [00:03<00:00, 15.61it/s]\n","[21 / 30] Train: Loss = 3.58707, PPX = 36.13:   0%|          | 1/2063 [00:00<06:17,  5.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.76837e-06\n"],"name":"stdout"},{"output_type":"stream","text":["[21 / 30] Train: Loss = 3.61988, PPX = 37.33: 100%|██████████| 2063/2063 [02:00<00:00, 17.52it/s]\n","[21 / 30]   Val: Loss = 4.38658, PPX = 80.37: 100%|██████████| 58/58 [00:03<00:00, 15.62it/s]\n","[22 / 30] Train: Loss = 3.77311, PPX = 43.52:   0%|          | 1/2063 [00:00<05:56,  5.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.19209e-06\n"],"name":"stdout"},{"output_type":"stream","text":["[22 / 30] Train: Loss = 3.61976, PPX = 37.33: 100%|██████████| 2063/2063 [01:59<00:00, 17.20it/s]\n","[22 / 30]   Val: Loss = 4.38441, PPX = 80.19: 100%|██████████| 58/58 [00:03<00:00, 15.67it/s]\n","[23 / 30] Train: Loss = 3.61886, PPX = 37.30: 100%|██████████| 2063/2063 [01:59<00:00, 17.84it/s]\n","[23 / 30]   Val: Loss = 4.38543, PPX = 80.27: 100%|██████████| 58/58 [00:03<00:00, 15.66it/s]\n","[24 / 30] Train: Loss = 3.75688, PPX = 42.81:   0%|          | 1/2063 [00:00<05:40,  6.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.98023e-07\n"],"name":"stdout"},{"output_type":"stream","text":["[24 / 30] Train: Loss = 3.61972, PPX = 37.33: 100%|██████████| 2063/2063 [02:00<00:00, 17.11it/s]\n","[24 / 30]   Val: Loss = 4.38342, PPX = 80.11: 100%|██████████| 58/58 [00:03<00:00, 15.57it/s]\n","[25 / 30] Train: Loss = 3.61934, PPX = 37.31: 100%|██████████| 2063/2063 [01:59<00:00, 17.48it/s]\n","[25 / 30]   Val: Loss = 4.38714, PPX = 80.41: 100%|██████████| 58/58 [00:03<00:00, 15.56it/s]\n","[26 / 30] Train: Loss = 3.51073, PPX = 33.47:   0%|          | 1/2063 [00:00<05:48,  5.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.45058e-08\n"],"name":"stdout"},{"output_type":"stream","text":["[26 / 30] Train: Loss = 3.61898, PPX = 37.30: 100%|██████████| 2063/2063 [01:59<00:00, 17.33it/s]\n","[26 / 30]   Val: Loss = 4.38445, PPX = 80.19: 100%|██████████| 58/58 [00:03<00:00, 15.46it/s]\n","[27 / 30] Train: Loss = 3.67947, PPX = 39.63:   0%|          | 1/2063 [00:00<05:54,  5.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.86265e-08\n"],"name":"stdout"},{"output_type":"stream","text":["[27 / 30] Train: Loss = 3.61982, PPX = 37.33: 100%|██████████| 2063/2063 [02:00<00:00, 17.15it/s]\n","[27 / 30]   Val: Loss = 4.38304, PPX = 80.08: 100%|██████████| 58/58 [00:03<00:00, 15.71it/s]\n","[28 / 30] Train: Loss = 3.62074, PPX = 37.37: 100%|██████████| 2063/2063 [02:00<00:00, 17.72it/s]\n","[28 / 30]   Val: Loss = 4.38541, PPX = 80.27: 100%|██████████| 58/58 [00:03<00:00, 15.56it/s]\n","[29 / 30] Train: Loss = 3.53129, PPX = 34.17:   0%|          | 1/2063 [00:00<05:58,  5.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.65661e-09\n"],"name":"stdout"},{"output_type":"stream","text":["[29 / 30] Train: Loss = 3.61956, PPX = 37.32: 100%|██████████| 2063/2063 [02:00<00:00, 17.52it/s]\n","[29 / 30]   Val: Loss = 4.38520, PPX = 80.25: 100%|██████████| 58/58 [00:03<00:00, 15.49it/s]\n","[30 / 30] Train: Loss = 3.75155, PPX = 42.59:   0%|          | 1/2063 [00:00<05:38,  6.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.16415e-09\n"],"name":"stdout"},{"output_type":"stream","text":["[30 / 30] Train: Loss = 3.61964, PPX = 37.32: 100%|██████████| 2063/2063 [01:59<00:00, 17.18it/s]\n","[30 / 30]   Val: Loss = 4.38587, PPX = 80.31: 100%|██████████| 58/58 [00:03<00:00, 15.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.91038e-10\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"metadata":{"id":"Zikcb64E4zU4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"b039c375-d296-4123-aac4-77a3fa0501a7","executionInfo":{"status":"ok","timestamp":1542200848568,"user_tz":-180,"elapsed":594,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["generate(model, 0)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["мы шли в атаку на трамвае \n","и я хочу тебя как в той \n","и что в меня я не заметил \n","что в этот день тебя люблю \n"],"name":"stdout"}]},{"metadata":{"id":"fYq8DarF5sc6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"5be857cb-3ce3-467e-f604-c08619d29cbf","executionInfo":{"status":"ok","timestamp":1542200857548,"user_tz":-180,"elapsed":490,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["generate(model, 1)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["я ржу ищу тебя не буду \n","и не могу уснуть на грудь \n","и не прощу с собой а в уши \n","к концу \n"],"name":"stdout"}]},{"metadata":{"id":"WnP743CM-bY6","colab_type":"text"},"cell_type":"markdown","source":["### Variational & word dropout\n","\n","**Задание** На прошлом занятии приводились примеры более приспособленных к RNN'ам dropout'ов. Добавьте их."]},{"metadata":{"id":"ihpNn1f76bJi","colab_type":"code","colab":{}},"cell_type":"code","source":["from scipy.stats import bernoulli\n","\n","class LockedDropout(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, inputs, dropout=0.5):\n","        if not self.training or not dropout:\n","            return inputs\n","        \n","        mask = FloatTensor(bernoulli.rvs(1 - dropout, size=(1, inputs.shape[1], inputs.shape[2])) / (1 - dropout))\n","        return mask * inputs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-g1rddlv4M54","colab_type":"code","colab":{}},"cell_type":"code","source":["class LargeModel(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=512, lstm_hidden_dim=512, num_layers=1):\n","        super().__init__()\n","\n","        self._dropout = LockedDropout()\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers)\n","        \n","        self._out_layer = nn.Linear(lstm_hidden_dim+1, vocab_size)\n","        \n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._emb.weight.data.uniform_(-init_range, init_range)\n","        self._out_layer.bias.data.zero_()\n","        self._out_layer.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, inputs, labels, hidden=None):\n","        embs = self._emb(inputs)\n","        output, hidden = self._rnn(embs, hidden)\n","        labels = labels.unsqueeze(0).unsqueeze(2).expand(output.shape[0], output.shape[1], 1).float()\n","        output = torch.cat((output, labels), -1)\n","        return self._out_layer(self._dropout(output)), hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GFyVzl6y6f9u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1326},"outputId":"d6e0aa47-8dbc-4eed-de10-71acb0a5373b","executionInfo":{"status":"ok","timestamp":1542210431641,"user_tz":-180,"elapsed":3734168,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = LargeModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["[1 / 30] Train: Loss = 6.15442, PPX = 470.79: 100%|██████████| 2063/2063 [02:00<00:00, 17.06it/s]\n","[1 / 30]   Val: Loss = 5.52321, PPX = 250.44: 100%|██████████| 58/58 [00:03<00:00, 15.39it/s]\n","[2 / 30] Train: Loss = 5.27878, PPX = 196.13: 100%|██████████| 2063/2063 [01:59<00:00, 17.61it/s]\n","[2 / 30]   Val: Loss = 4.97649, PPX = 144.96: 100%|██████████| 58/58 [00:03<00:00, 15.75it/s]\n","[3 / 30] Train: Loss = 4.96829, PPX = 143.78: 100%|██████████| 2063/2063 [02:01<00:00, 16.66it/s]\n","[3 / 30]   Val: Loss = 4.73834, PPX = 114.24: 100%|██████████| 58/58 [00:03<00:00, 15.67it/s]\n","[4 / 30] Train: Loss = 4.80148, PPX = 121.69: 100%|██████████| 2063/2063 [02:02<00:00, 16.97it/s]\n","[4 / 30]   Val: Loss = 4.67796, PPX = 107.55: 100%|██████████| 58/58 [00:03<00:00, 15.29it/s]\n","[5 / 30] Train: Loss = 4.66822, PPX = 106.51: 100%|██████████| 2063/2063 [02:00<00:00, 17.24it/s]\n","[5 / 30]   Val: Loss = 4.66843, PPX = 106.53: 100%|██████████| 58/58 [00:03<00:00, 15.71it/s]\n","[6 / 30] Train: Loss = 4.56018, PPX = 95.60: 100%|██████████| 2063/2063 [02:00<00:00, 17.48it/s]\n","[6 / 30]   Val: Loss = 4.58915, PPX = 98.41: 100%|██████████| 58/58 [00:03<00:00, 15.19it/s]\n","[7 / 30] Train: Loss = 4.46540, PPX = 86.96: 100%|██████████| 2063/2063 [02:01<00:00, 17.29it/s]\n","[7 / 30]   Val: Loss = 4.53566, PPX = 93.29: 100%|██████████| 58/58 [00:03<00:00, 15.53it/s]\n","[8 / 30] Train: Loss = 4.37860, PPX = 79.73: 100%|██████████| 2063/2063 [02:00<00:00, 16.93it/s]\n","[8 / 30]   Val: Loss = 4.52095, PPX = 91.92: 100%|██████████| 58/58 [00:03<00:00, 15.64it/s]\n","[9 / 30] Train: Loss = 4.30889, PPX = 74.36: 100%|██████████| 2063/2063 [02:00<00:00, 17.66it/s]\n","[9 / 30]   Val: Loss = 4.48732, PPX = 88.88: 100%|██████████| 58/58 [00:03<00:00, 15.49it/s]\n","[10 / 30] Train: Loss = 4.23345, PPX = 68.95: 100%|██████████| 2063/2063 [02:00<00:00, 17.34it/s]\n","[10 / 30]   Val: Loss = 4.47181, PPX = 87.52: 100%|██████████| 58/58 [00:03<00:00, 15.53it/s]\n","[11 / 30] Train: Loss = 4.17948, PPX = 65.33: 100%|██████████| 2063/2063 [02:00<00:00, 17.29it/s]\n","[11 / 30]   Val: Loss = 4.50350, PPX = 90.33: 100%|██████████| 58/58 [00:03<00:00, 15.43it/s]\n","[12 / 30] Train: Loss = 3.97066, PPX = 53.02:   0%|          | 1/2063 [00:00<05:54,  5.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 30] Train: Loss = 3.76410, PPX = 43.12: 100%|██████████| 2063/2063 [02:00<00:00, 17.03it/s]\n","[12 / 30]   Val: Loss = 4.37157, PPX = 79.17: 100%|██████████| 58/58 [00:03<00:00, 15.81it/s]\n","[13 / 30] Train: Loss = 3.64877, PPX = 38.43: 100%|██████████| 2063/2063 [02:00<00:00, 17.07it/s]\n","[13 / 30]   Val: Loss = 4.37487, PPX = 79.43: 100%|██████████| 58/58 [00:03<00:00, 15.28it/s]\n","[14 / 30] Train: Loss = 3.69972, PPX = 40.44:   0%|          | 1/2063 [00:00<06:04,  5.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.25\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 30] Train: Loss = 3.50549, PPX = 33.30: 100%|██████████| 2063/2063 [02:00<00:00, 16.97it/s]\n","[14 / 30]   Val: Loss = 4.36598, PPX = 78.73: 100%|██████████| 58/58 [00:03<00:00, 15.45it/s]\n","[15 / 30] Train: Loss = 3.47044, PPX = 32.15: 100%|██████████| 2063/2063 [02:00<00:00, 17.35it/s]\n","[15 / 30]   Val: Loss = 4.36050, PPX = 78.30: 100%|██████████| 58/58 [00:03<00:00, 15.69it/s]\n","[16 / 30] Train: Loss = 3.44202, PPX = 31.25: 100%|██████████| 2063/2063 [02:00<00:00, 17.22it/s]\n","[16 / 30]   Val: Loss = 4.36279, PPX = 78.48: 100%|██████████| 58/58 [00:03<00:00, 15.88it/s]\n","[17 / 30] Train: Loss = 3.42182, PPX = 30.63:   0%|          | 1/2063 [00:00<05:45,  5.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.3125\n"],"name":"stdout"},{"output_type":"stream","text":["[17 / 30] Train: Loss = 3.39932, PPX = 29.94: 100%|██████████| 2063/2063 [02:00<00:00, 17.93it/s]\n","[17 / 30]   Val: Loss = 4.36826, PPX = 78.91: 100%|██████████| 58/58 [00:03<00:00, 15.60it/s]\n","[18 / 30] Train: Loss = 3.32895, PPX = 27.91:   0%|          | 1/2063 [00:00<06:02,  5.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.078125\n"],"name":"stdout"},{"output_type":"stream","text":["[18 / 30] Train: Loss = 3.38580, PPX = 29.54: 100%|██████████| 2063/2063 [02:00<00:00, 17.04it/s]\n","[18 / 30]   Val: Loss = 4.36685, PPX = 78.80: 100%|██████████| 58/58 [00:03<00:00, 15.51it/s]\n","[19 / 30] Train: Loss = 3.31335, PPX = 27.48:   0%|          | 1/2063 [00:00<06:27,  5.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0195312\n"],"name":"stdout"},{"output_type":"stream","text":["[19 / 30] Train: Loss = 3.38341, PPX = 29.47: 100%|██████████| 2063/2063 [02:00<00:00, 17.27it/s]\n","[19 / 30]   Val: Loss = 4.36569, PPX = 78.70: 100%|██████████| 58/58 [00:03<00:00, 15.79it/s]\n","  0%|          | 1/2063 [00:00<06:35,  5.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.00488281\n"],"name":"stdout"},{"output_type":"stream","text":["[20 / 30] Train: Loss = 3.38140, PPX = 29.41: 100%|██████████| 2063/2063 [02:02<00:00, 16.98it/s]\n","[20 / 30]   Val: Loss = 4.36626, PPX = 78.75: 100%|██████████| 58/58 [00:03<00:00, 15.46it/s]\n","[21 / 30] Train: Loss = 3.20282, PPX = 24.60:   0%|          | 1/2063 [00:00<06:30,  5.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.0012207\n"],"name":"stdout"},{"output_type":"stream","text":["[21 / 30] Train: Loss = 3.38316, PPX = 29.46: 100%|██████████| 2063/2063 [02:00<00:00, 17.37it/s]\n","[21 / 30]   Val: Loss = 4.36874, PPX = 78.94: 100%|██████████| 58/58 [00:03<00:00, 15.94it/s]\n","[22 / 30] Train: Loss = 3.60608, PPX = 36.82:   0%|          | 1/2063 [00:00<06:14,  5.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.000305176\n"],"name":"stdout"},{"output_type":"stream","text":["[22 / 30] Train: Loss = 3.38244, PPX = 29.44: 100%|██████████| 2063/2063 [02:00<00:00, 17.48it/s]\n","[22 / 30]   Val: Loss = 4.36416, PPX = 78.58: 100%|██████████| 58/58 [00:03<00:00, 15.64it/s]\n","[23 / 30] Train: Loss = 3.36635, PPX = 28.97:   0%|          | 1/2063 [00:00<06:01,  5.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.62939e-05\n"],"name":"stdout"},{"output_type":"stream","text":["[23 / 30] Train: Loss = 3.38234, PPX = 29.44: 100%|██████████| 2063/2063 [02:00<00:00, 17.07it/s]\n","[23 / 30]   Val: Loss = 4.36705, PPX = 78.81: 100%|██████████| 58/58 [00:03<00:00, 15.62it/s]\n","[24 / 30] Train: Loss = 3.47064, PPX = 32.16:   0%|          | 1/2063 [00:00<05:52,  5.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.90735e-05\n"],"name":"stdout"},{"output_type":"stream","text":["[24 / 30] Train: Loss = 3.38154, PPX = 29.42: 100%|██████████| 2063/2063 [02:01<00:00, 17.01it/s]\n","[24 / 30]   Val: Loss = 4.36410, PPX = 78.58: 100%|██████████| 58/58 [00:03<00:00, 15.29it/s]\n","  0%|          | 1/2063 [00:00<06:39,  5.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.76837e-06\n"],"name":"stdout"},{"output_type":"stream","text":["[25 / 30] Train: Loss = 3.38290, PPX = 29.46: 100%|██████████| 2063/2063 [02:00<00:00, 17.51it/s]\n","[25 / 30]   Val: Loss = 4.36839, PPX = 78.92: 100%|██████████| 58/58 [00:03<00:00, 15.62it/s]\n","[26 / 30] Train: Loss = 3.30558, PPX = 27.26:   0%|          | 1/2063 [00:00<05:51,  5.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.19209e-06\n"],"name":"stdout"},{"output_type":"stream","text":["[26 / 30] Train: Loss = 3.38119, PPX = 29.41: 100%|██████████| 2063/2063 [02:00<00:00, 17.17it/s]\n","[26 / 30]   Val: Loss = 4.36497, PPX = 78.65: 100%|██████████| 58/58 [00:03<00:00, 15.53it/s]\n","[27 / 30] Train: Loss = 3.43468, PPX = 31.02:   0%|          | 1/2063 [00:00<05:57,  5.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.98023e-07\n"],"name":"stdout"},{"output_type":"stream","text":["[27 / 30] Train: Loss = 3.38027, PPX = 29.38: 100%|██████████| 2063/2063 [02:00<00:00, 17.07it/s]\n","[27 / 30]   Val: Loss = 4.36354, PPX = 78.53: 100%|██████████| 58/58 [00:03<00:00, 15.72it/s]\n","  0%|          | 1/2063 [00:00<06:36,  5.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 7.45058e-08\n"],"name":"stdout"},{"output_type":"stream","text":["[28 / 30] Train: Loss = 3.38129, PPX = 29.41: 100%|██████████| 2063/2063 [02:00<00:00, 17.29it/s]\n","[28 / 30]   Val: Loss = 4.36678, PPX = 78.79: 100%|██████████| 58/58 [00:03<00:00, 15.67it/s]\n","[29 / 30] Train: Loss = 3.36007, PPX = 28.79:   0%|          | 1/2063 [00:00<06:06,  5.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.86265e-08\n"],"name":"stdout"},{"output_type":"stream","text":["[29 / 30] Train: Loss = 3.38072, PPX = 29.39: 100%|██████████| 2063/2063 [02:00<00:00, 17.09it/s]\n","[29 / 30]   Val: Loss = 4.36647, PPX = 78.77: 100%|██████████| 58/58 [00:03<00:00, 15.60it/s]\n","[30 / 30] Train: Loss = 3.56372, PPX = 35.29:   0%|          | 1/2063 [00:00<06:01,  5.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 4.65661e-09\n"],"name":"stdout"},{"output_type":"stream","text":["[30 / 30] Train: Loss = 3.38174, PPX = 29.42: 100%|██████████| 2063/2063 [02:00<00:00, 16.96it/s]\n","[30 / 30]   Val: Loss = 4.36682, PPX = 78.79: 100%|██████████| 58/58 [00:03<00:00, 15.55it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.16415e-09\n"],"name":"stdout"}]},{"metadata":{"id":"QL6SbuU56iAR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"44fda341-0701-49e4-e6da-0f3d9aeddfd1","executionInfo":{"status":"ok","timestamp":1542210782215,"user_tz":-180,"elapsed":446,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["generate(model, 0)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["олег лежит на дне колодца \n","а в нём картинку и с ней снег \n","еще один живет мужчина \n","и с ним на улице исус \n"],"name":"stdout"}]},{"metadata":{"id":"UQLriG936jYK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"d5355fc7-680d-4d99-8cba-313f959d6186","executionInfo":{"status":"ok","timestamp":1542210809547,"user_tz":-180,"elapsed":673,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["generate(model, 1)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["я на тебя смотрю на небо \n","и вижу что ты мне за то \n","что я люблю тебя родная \n","как ты \n"],"name":"stdout"}]},{"metadata":{"id":"dL6-Lobu4NOB","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Кроме этого, попробуйте увеличивать размер модели или количество слоев в ней, чтобы улучшить качество."]},{"metadata":{"id":"VJAl84wK4O6D","colab_type":"code","colab":{}},"cell_type":"code","source":["class LargeModel(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=1024, lstm_hidden_dim=1024, num_layers=1):\n","        super().__init__()\n","\n","        self._dropout = LockedDropout()\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers)\n","        self._low_out_layer = nn.Linear(lstm_hidden_dim + 1, 1024)\n","        self._high_out_layer = nn.Linear(1024, vocab_size)\n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._emb.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, inputs, labels, hidden=None):\n","        embs = self._emb(inputs)\n","        output, hidden = self._rnn(embs, hidden)\n","        labels = labels.unsqueeze(0).unsqueeze(2).expand(output.shape[0], output.shape[1], 1).float()\n","        output = torch.cat((output, labels), -1)\n","        output = self._low_out_layer(self._dropout(output))\n","        return self._high_out_layer(self._dropout(output)), hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yXak3N_e7IWE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":697},"outputId":"9e0b5f4b-7373-46ae-d4f9-5dac0c47ff22","executionInfo":{"status":"ok","timestamp":1542220095177,"user_tz":-180,"elapsed":4122623,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = LargeModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=15, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["[1 / 15] Train: Loss = 5.32717, PPX = 205.86: 100%|██████████| 2063/2063 [04:27<00:00,  7.79it/s]\n","[1 / 15]   Val: Loss = 4.87408, PPX = 130.85: 100%|██████████| 58/58 [00:06<00:00,  8.18it/s]\n","[2 / 15] Train: Loss = 4.77815, PPX = 118.88: 100%|██████████| 2063/2063 [04:28<00:00,  7.99it/s]\n","[2 / 15]   Val: Loss = 4.64462, PPX = 104.02: 100%|██████████| 58/58 [00:06<00:00,  8.54it/s]\n","[3 / 15] Train: Loss = 4.52334, PPX = 92.14: 100%|██████████| 2063/2063 [04:27<00:00,  7.79it/s]\n","[3 / 15]   Val: Loss = 4.53785, PPX = 93.49: 100%|██████████| 58/58 [00:06<00:00,  8.17it/s]\n","[4 / 15] Train: Loss = 4.32134, PPX = 75.29: 100%|██████████| 2063/2063 [04:27<00:00,  7.76it/s]\n","[4 / 15]   Val: Loss = 4.49019, PPX = 89.14: 100%|██████████| 58/58 [00:06<00:00,  8.79it/s]\n","[5 / 15] Train: Loss = 4.14139, PPX = 62.89: 100%|██████████| 2063/2063 [04:27<00:00,  7.53it/s]\n","[5 / 15]   Val: Loss = 4.47345, PPX = 87.66: 100%|██████████| 58/58 [00:06<00:00,  7.98it/s]\n","[6 / 15] Train: Loss = 3.97132, PPX = 53.05: 100%|██████████| 2063/2063 [04:27<00:00,  7.66it/s]\n","[6 / 15]   Val: Loss = 4.48172, PPX = 88.39: 100%|██████████| 58/58 [00:06<00:00,  9.19it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.00025\n"],"name":"stdout"},{"output_type":"stream","text":["[7 / 15] Train: Loss = 3.57366, PPX = 35.65: 100%|██████████| 2063/2063 [04:27<00:00,  7.50it/s]\n","[7 / 15]   Val: Loss = 4.55424, PPX = 95.03: 100%|██████████| 58/58 [00:06<00:00,  8.38it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 6.25e-05\n"],"name":"stdout"},{"output_type":"stream","text":["[8 / 15] Train: Loss = 3.38333, PPX = 29.47: 100%|██████████| 2063/2063 [04:27<00:00,  7.49it/s]\n","[8 / 15]   Val: Loss = 4.60304, PPX = 99.79: 100%|██████████| 58/58 [00:07<00:00,  8.20it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.5625e-05\n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 15] Train: Loss = 3.32959, PPX = 27.93: 100%|██████████| 2063/2063 [04:28<00:00,  7.90it/s]\n","[9 / 15]   Val: Loss = 4.63163, PPX = 102.68: 100%|██████████| 58/58 [00:06<00:00,  8.44it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 3.90625e-06\n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 15] Train: Loss = 3.31688, PPX = 27.57: 100%|██████████| 2063/2063 [04:27<00:00,  7.79it/s]\n","[10 / 15]   Val: Loss = 4.63611, PPX = 103.14: 100%|██████████| 58/58 [00:07<00:00,  8.59it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 9.76563e-07\n"],"name":"stdout"},{"output_type":"stream","text":["[11 / 15] Train: Loss = 3.31328, PPX = 27.48: 100%|██████████| 2063/2063 [04:27<00:00,  7.84it/s]\n","[11 / 15]   Val: Loss = 4.64117, PPX = 103.67: 100%|██████████| 58/58 [00:06<00:00,  8.06it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.44141e-07\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 15] Train: Loss = 3.31490, PPX = 27.52: 100%|██████████| 2063/2063 [04:27<00:00,  7.34it/s]\n","[12 / 15]   Val: Loss = 4.63332, PPX = 102.86: 100%|██████████| 58/58 [00:06<00:00,  8.30it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 6.10352e-08\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 15] Train: Loss = 3.31194, PPX = 27.44: 100%|██████████| 2063/2063 [04:27<00:00,  7.70it/s]\n","[13 / 15]   Val: Loss = 4.64155, PPX = 103.71: 100%|██████████| 58/58 [00:06<00:00,  8.42it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.52588e-08\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 15] Train: Loss = 3.31247, PPX = 27.45: 100%|██████████| 2063/2063 [04:27<00:00,  7.62it/s]\n","[14 / 15]   Val: Loss = 4.63940, PPX = 103.48: 100%|██████████| 58/58 [00:06<00:00,  8.42it/s]\n","  0%|          | 0/2063 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 3.8147e-09\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 15] Train: Loss = 3.31221, PPX = 27.45: 100%|██████████| 2063/2063 [04:27<00:00,  7.80it/s]\n","[15 / 15]   Val: Loss = 4.64122, PPX = 103.67: 100%|██████████| 58/58 [00:06<00:00,  8.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 9.53674e-10\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"metadata":{"id":"DWnznuGp7Jsa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"9a4362a1-647e-4089-b747-8b0e5a0fd816","executionInfo":{"status":"ok","timestamp":1542220395326,"user_tz":-180,"elapsed":490,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["generate(model, 0)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["я не люблю тебя родная \n","я буду больше никогда \n","как я не знаю как обычно \n","а ты и вправду человек \n"],"name":"stdout"}]},{"metadata":{"id":"RSvG-xtJ7Kx8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"814f8d83-82f8-42c0-a52d-257d02ce9c41","executionInfo":{"status":"ok","timestamp":1542220398163,"user_tz":-180,"elapsed":470,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["generate(model, 1)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["олег на голову собрался \n","в слезах олег кричит ура \n","и спрашивает папа с криком \n","давай \n"],"name":"stdout"}]},{"metadata":{"id":"Ejqx6BC0JcG2","colab_type":"text"},"cell_type":"markdown","source":["## Multi-task learning\n","\n","Ещё один важный способ улучшения модели - multi-task learning. Это когда одна модель учится делать предсказания сразу для нескольких задач.\n","\n","В нашем случае это может быть предсказанием отдельно леммы слова и отдельно - его грамматического значения:\n","![](https://hsto.org/web/e97/8a8/6e8/e978a86e8a874d8d946bb15e6a49a713.png =x350)\n","\n","В итоге модель выучивает как языковую модель по леммам, так и модель POS tagging'а. Одновременно!\n","\n","Возьмем корпус из universal dependencies - он уже размечен, как нужно.\n","\n","Почитаем его:"]},{"metadata":{"id":"YT-kzC2_KuLX","colab_type":"code","colab":{}},"cell_type":"code","source":["from corpus_iterator import Token, CorpusIterator\n","\n","fields = [('word', Field()), ('lemma', Field()), ('gram_val', Field())]\n","examples = []\n","\n","with CorpusIterator('UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu') as corpus_iter:\n","    for sent in corpus_iter:\n","        words = ['<s>'] + [tok.token.lower() for tok in sent] + ['</s>']\n","        lemmas = ['<s>'] + [tok.lemma.lower() for tok in sent] + ['</s>']\n","        gr_vals = ['<s>'] + [tok.grammar_value for tok in sent] + ['</s>']\n","        examples.append(Example.fromlist([words, lemmas, gr_vals], fields))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l_3xaD-2KwNW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":108},"outputId":"9415461c-697e-46d3-ddb9-33b71f44aba8","executionInfo":{"status":"ok","timestamp":1542220419043,"user_tz":-180,"elapsed":481,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["print('Words:', examples[1].word)\n","print('Lemmas:', examples[1].lemma)\n","print('Grammar vals:', examples[1].gram_val)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Words: ['<s>', 'начальник', 'областного', 'управления', 'связи', 'семен', 'еремеевич', 'был', 'человек', 'простой', ',', 'приходил', 'на', 'работу', 'всегда', 'вовремя', ',', 'здоровался', 'с', 'секретаршей', 'за', 'руку', 'и', 'иногда', 'даже', 'писал', 'в', 'стенгазету', 'заметки', 'под', 'псевдонимом', '\"', 'муха', '\"', '.', '</s>']\n","Lemmas: ['<s>', 'начальник', 'областной', 'управление', 'связь', 'семен', 'еремеевич', 'быть', 'человек', 'простой', ',', 'приходить', 'на', 'работа', 'всегда', 'вовремя', ',', 'здороваться', 'с', 'секретарша', 'за', 'рука', 'и', 'иногда', 'даже', 'писать', 'в', 'стенгазета', 'заметка', 'под', 'псевдоним', '\"', 'муха', '\"', '.', '</s>']\n","Grammar vals: ['<s>', 'NOUN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing', 'ADJ|Case=Gen|Degree=Pos|Gender=Neut|Number=Sing', 'NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing', 'NOUN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing', 'PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing', 'PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing', 'AUX|Aspect=Imp|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act', 'NOUN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing', 'ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing', 'PUNCT|_', 'VERB|Aspect=Imp|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act', 'ADP|_', 'NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing', 'ADV|Degree=Pos', 'ADV|Degree=Pos', 'PUNCT|_', 'VERB|Aspect=Imp|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Mid', 'ADP|_', 'NOUN|Animacy=Anim|Case=Ins|Gender=Fem|Number=Sing', 'ADP|_', 'NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing', 'CCONJ|_', 'ADV|Degree=Pos', 'PART|_', 'VERB|Aspect=Imp|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act', 'ADP|_', 'NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing', 'NOUN|Animacy=Inan|Case=Acc|Gender=Fem|Number=Plur', 'ADP|_', 'NOUN|Animacy=Inan|Case=Ins|Gender=Masc|Number=Sing', 'PUNCT|_', 'NOUN|Animacy=Anim|Case=Nom|Gender=Fem|Number=Sing', 'PUNCT|_', 'PUNCT|_', '</s>']\n"],"name":"stdout"}]},{"metadata":{"id":"HcGm5fPsLESH","colab_type":"text"},"cell_type":"markdown","source":["Таким образом, размер словаря может быть существенно сокращен - лемм меньше, чем слов, а предсказание грамматики вынуждает модель быть более осведомленной о согласовании слов."]},{"metadata":{"id":"xZe5HimdLb9i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"30baa526-d20d-41ff-ca2d-fb1e21bc1951","executionInfo":{"status":"ok","timestamp":1542220425522,"user_tz":-180,"elapsed":1963,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["dataset = Dataset(examples, fields)\n","\n","dataset.fields['word'].build_vocab(dataset, min_freq=3)\n","print('Word vocab size =', len(dataset.fields['word'].vocab))\n","dataset.fields['lemma'].build_vocab(dataset, min_freq=3)\n","print('Lemma vocab size =', len(dataset.fields['lemma'].vocab))\n","dataset.fields['gram_val'].build_vocab(dataset)\n","print('Grammar val vocab size =', len(dataset.fields['gram_val'].vocab))\n","\n","train_dataset, test_dataset = dataset.split(split_ratio=0.75)\n","\n","train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n","                                              shuffle=True, device=DEVICE, sort=False)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Word vocab size = 29984\n","Lemma vocab size = 17059\n","Grammar val vocab size = 737\n"],"name":"stdout"}]},{"metadata":{"id":"Y7xlr15lLm78","colab_type":"text"},"cell_type":"markdown","source":["Построим маппинг из пары (лемма, грамматическое значение) в слово - если бы у нас под рукой был морфологический словарь, маппинг можно было бы пополнить, добавить слова для лемм из корпуса, которые не встретились в обучении."]},{"metadata":{"id":"_AvT2MgeLmP8","colab_type":"code","colab":{}},"cell_type":"code","source":["dictionary = {\n","    (lemma, gram_val): word\n","    for example in train_iter.dataset.examples \n","    for word, lemma, gram_val in zip(example.word, example.lemma, example.gram_val)\n","}"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VaP8Krx1LeJl","colab_type":"text"},"cell_type":"markdown","source":["**Задание**  Обновите генератор - например, можно сэмплировать лемму и находить самое вероятное грамматическое значение, которое встречается  в паре с этой леммой в `dictionary`."]},{"metadata":{"id":"PeBH0WYjMQ5h","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate(model, temp=0.7):\n","    model.eval()\n","    with torch.no_grad():\n","        prev_lemma = train_iter.dataset.fields['lemma'].vocab.stoi['<s>']\n","        end_lemma = train_iter.dataset.fields['lemma'].vocab.stoi['</s>']\n","        prev_gram_val = train_iter.dataset.fields['gram_val'].vocab.stoi['<s>']\n","        end_gram_val = train_iter.dataset.fields['gram_val'].vocab.stoi['</s>']\n","        \n","        hidden = None\n","        for _ in range(150):\n","            lemma_probs, gram_val_probs, hidden = model(LongTensor([[prev_lemma]]), LongTensor([[prev_gram_val]]), hidden)\n","            prev_lemma = sample(lemma_probs, temp)\n","            \n","            word = None\n","            \n","            for i in np.argsort(gram_val_probs).cpu().numpy()[0][0][::-1]:\n","                try:\n","                    word = dictionary[train_iter.dataset.fields['lemma'].vocab.itos[prev_lemma],\n","                                      train_iter.dataset.fields['gram_val'].vocab.itos[i]]\n","                    prev_gram_val = i\n","                except KeyError:\n","                    pass\n","            \n","            if (prev_lemma == end_lemma) or (prev_gram_val == end_gram_val):\n","                return\n","            \n","            print(word, end=' ')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w3GzOZ8dMVMJ","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Обновите модель и функцию обучения.\n","\n","Модель должна принимать пары `lemma, gr_val`, конкатенировать их эмбеддинги и предсказывать следующие `lemma, gr_val` по выходу из LSTM.\n","\n","Функция `do_epoch` должна суммировать потери по предсказанию леммы (делая маскинг для `<unk>` и `<pad>`) + потери по предсказанию грамматического значения (с маскингом по `<pad>`)."]},{"metadata":{"id":"QJwjDBHpHNx-","colab_type":"code","colab":{}},"cell_type":"code","source":["class MultitaskLearningLMModel(nn.Module):\n","    def __init__(self, vocab_size, gr_val_size, emb_dim=512, lstm_hidden_dim=512, num_layers=1):\n","        super().__init__()\n","\n","        self._lemma_emb = nn.Embedding(vocab_size, emb_dim)\n","        self._gr_val_emb = nn.Embedding(gr_val_size, emb_dim)\n","        self._rnn = nn.LSTM(input_size=2*emb_dim, hidden_size=lstm_hidden_dim, num_layers=num_layers)\n","        \n","        self._lemma_out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n","        self._gr_val_out_layer = nn.Linear(lstm_hidden_dim, gr_val_size)\n","        \n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._lemma_emb.weight.data.uniform_(-init_range, init_range)\n","        self._lemma_out_layer.bias.data.zero_()\n","        self._lemma_out_layer.weight.data.uniform_(-init_range, init_range)\n","        \n","        self._gr_val_emb.weight.data.uniform_(-init_range, init_range)\n","        self._gr_val_out_layer.bias.data.zero_()\n","        self._gr_val_out_layer.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, lemmas, gram_vals, hidden=None):\n","        lemma_embs = self._lemma_emb(lemmas)\n","        gr_val_embs = self._gr_val_emb(gram_vals)\n","        \n","        embs = torch.cat((lemma_embs, gr_val_embs), -1)\n","        \n","        output, hidden = self._rnn(embs, hidden)\n","        lemma_out = self._lemma_out_layer(output)\n","        gr_val_out = self._gr_val_out_layer(output)\n","        return lemma_out, gr_val_out, hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Cx1mcW5cLgFl","colab_type":"code","colab":{}},"cell_type":"code","source":["batch = next(iter(train_iter))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kK-2CfY6KPl1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2091},"outputId":"6febcd58-b28e-47f1-fa60-98e29195c82d","executionInfo":{"status":"ok","timestamp":1542220438206,"user_tz":-180,"elapsed":1011,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = MultitaskLearningLMModel(vocab_size=len(train_iter.dataset.fields['lemma'].vocab),\n","                                 gr_val_size=len(train_iter.dataset.fields['gram_val'].vocab)).to(DEVICE)\n","\n","model(batch.lemma, batch.gram_val)"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[ 0.0041, -0.0030,  0.0415,  ..., -0.0023, -0.0255, -0.0090],\n","          [ 0.0041, -0.0030,  0.0415,  ..., -0.0023, -0.0255, -0.0090],\n","          [ 0.0041, -0.0030,  0.0415,  ..., -0.0023, -0.0255, -0.0090],\n","          ...,\n","          [ 0.0041, -0.0030,  0.0415,  ..., -0.0023, -0.0255, -0.0090],\n","          [ 0.0041, -0.0030,  0.0415,  ..., -0.0023, -0.0255, -0.0090],\n","          [ 0.0041, -0.0030,  0.0415,  ..., -0.0023, -0.0255, -0.0090]],\n"," \n","         [[-0.0125, -0.0014,  0.0141,  ..., -0.0143, -0.0084, -0.0096],\n","          [ 0.0121, -0.0043,  0.0406,  ..., -0.0074, -0.0156, -0.0305],\n","          [-0.0060,  0.0048,  0.0391,  ...,  0.0092, -0.0245, -0.0268],\n","          ...,\n","          [-0.0097, -0.0097,  0.0383,  ...,  0.0098, -0.0330, -0.0398],\n","          [-0.0097, -0.0097,  0.0383,  ...,  0.0098, -0.0330, -0.0398],\n","          [ 0.0004, -0.0132,  0.0187,  ..., -0.0123, -0.0392,  0.0120]],\n"," \n","         [[-0.0289,  0.0102,  0.0286,  ..., -0.0093, -0.0051,  0.0123],\n","          [ 0.0038,  0.0009,  0.0172,  ..., -0.0379, -0.0083, -0.0381],\n","          [-0.0394,  0.0140,  0.0451,  ...,  0.0041, -0.0076,  0.0195],\n","          ...,\n","          [-0.0387,  0.0050,  0.0436,  ...,  0.0061, -0.0061,  0.0172],\n","          [-0.0121, -0.0029,  0.0305,  ...,  0.0110, -0.0390, -0.0203],\n","          [-0.0048, -0.0216,  0.0018,  ..., -0.0128, -0.0371, -0.0283]],\n"," \n","         ...,\n"," \n","         [[-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0041, -0.0158, -0.0240,  ...,  0.0409,  0.0056,  0.0044],\n","          ...,\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044]],\n"," \n","         [[-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0041, -0.0158, -0.0240,  ...,  0.0409,  0.0056,  0.0044],\n","          ...,\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044]],\n"," \n","         [[-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0158, -0.0240,  ...,  0.0409,  0.0056,  0.0044],\n","          ...,\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044],\n","          [-0.0042, -0.0157, -0.0240,  ...,  0.0409,  0.0057,  0.0044]]],\n","        device='cuda:0', grad_fn=<ThAddBackward>),\n"," tensor([[[ 0.0068, -0.0116, -0.0053,  ..., -0.0011, -0.0129, -0.0234],\n","          [ 0.0068, -0.0116, -0.0053,  ..., -0.0011, -0.0129, -0.0234],\n","          [ 0.0068, -0.0116, -0.0053,  ..., -0.0011, -0.0129, -0.0234],\n","          ...,\n","          [ 0.0068, -0.0116, -0.0053,  ..., -0.0011, -0.0129, -0.0234],\n","          [ 0.0068, -0.0116, -0.0053,  ..., -0.0011, -0.0129, -0.0234],\n","          [ 0.0068, -0.0116, -0.0053,  ..., -0.0011, -0.0129, -0.0234]],\n"," \n","         [[ 0.0083, -0.0131, -0.0084,  ...,  0.0018, -0.0098, -0.0221],\n","          [-0.0033, -0.0070, -0.0230,  ...,  0.0041, -0.0436, -0.0229],\n","          [-0.0249, -0.0369, -0.0152,  ..., -0.0148, -0.0256, -0.0310],\n","          ...,\n","          [-0.0489, -0.0183,  0.0131,  ...,  0.0004, -0.0338, -0.0380],\n","          [-0.0489, -0.0183,  0.0131,  ...,  0.0004, -0.0338, -0.0380],\n","          [ 0.0199, -0.0077, -0.0008,  ..., -0.0086, -0.0261, -0.0421]],\n"," \n","         [[-0.0152, -0.0265, -0.0141,  ...,  0.0229, -0.0270, -0.0525],\n","          [ 0.0354, -0.0143, -0.0279,  ...,  0.0443, -0.0091, -0.0423],\n","          [-0.0289, -0.0233,  0.0026,  ...,  0.0147, -0.0514, -0.0389],\n","          ...,\n","          [-0.0414, -0.0150,  0.0164,  ...,  0.0286, -0.0523, -0.0388],\n","          [-0.0453, -0.0130,  0.0114,  ...,  0.0085, -0.0424, -0.0500],\n","          [-0.0189, -0.0093, -0.0213,  ...,  0.0018, -0.0332, -0.0513]],\n"," \n","         ...,\n"," \n","         [[ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0871,  ...,  0.0650, -0.0086, -0.0149],\n","          ...,\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148]],\n"," \n","         [[ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0650, -0.0086, -0.0148],\n","          ...,\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148]],\n"," \n","         [[ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0650, -0.0086, -0.0148],\n","          ...,\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148],\n","          [ 0.0125, -0.0341, -0.0872,  ...,  0.0649, -0.0087, -0.0148]]],\n","        device='cuda:0', grad_fn=<ThAddBackward>),\n"," (tensor([[[ 1.5567e-03,  2.9333e-02,  2.0962e-02,  ..., -2.2549e-02,\n","            -1.2963e-03,  7.0258e-03],\n","           [ 1.5567e-03,  2.9333e-02,  2.0961e-02,  ..., -2.2549e-02,\n","            -1.2961e-03,  7.0258e-03],\n","           [ 1.5539e-03,  2.9347e-02,  2.0950e-02,  ..., -2.2567e-02,\n","            -1.2589e-03,  7.0147e-03],\n","           ...,\n","           [ 1.5567e-03,  2.9333e-02,  2.0961e-02,  ..., -2.2549e-02,\n","            -1.2961e-03,  7.0258e-03],\n","           [ 1.5566e-03,  2.9334e-02,  2.0961e-02,  ..., -2.2549e-02,\n","            -1.2956e-03,  7.0258e-03],\n","           [ 1.5567e-03,  2.9333e-02,  2.0962e-02,  ..., -2.2549e-02,\n","            -1.2962e-03,  7.0258e-03]]],\n","         device='cuda:0', grad_fn=<CudnnRnnBackward>),\n","  tensor([[[ 0.0031,  0.0608,  0.0414,  ..., -0.0452, -0.0028,  0.0141],\n","           [ 0.0031,  0.0608,  0.0414,  ..., -0.0452, -0.0028,  0.0141],\n","           [ 0.0031,  0.0608,  0.0414,  ..., -0.0453, -0.0027,  0.0141],\n","           ...,\n","           [ 0.0031,  0.0608,  0.0414,  ..., -0.0452, -0.0028,  0.0141],\n","           [ 0.0031,  0.0608,  0.0414,  ..., -0.0452, -0.0028,  0.0141],\n","           [ 0.0031,  0.0608,  0.0414,  ..., -0.0452, -0.0028,  0.0141]]],\n","         device='cuda:0', grad_fn=<CudnnRnnBackward>)))"]},"metadata":{"tags":[]},"execution_count":62}]},{"metadata":{"id":"U8p3aMxuGpss","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):                \n","                lemma_logits, gram_val_logits, _ = model(batch.lemma, batch.gram_val)\n","\n","                lemma_targets = torch.cat((batch.lemma[1:], batch.lemma.new_ones((1, batch.lemma.shape[1])))).view(-1)\n","                gram_val_targets = torch.cat((batch.gram_val[1:], batch.gram_val.new_ones((1, batch.gram_val.shape[1])))).view(-1)\n","\n","                lemma_loss = criterion(lemma_logits.view(-1, lemma_logits.shape[-1]), lemma_targets)\n","                gram_val_loss = criterion(gram_val_logits.view(-1, gram_val_logits.shape[-1]), gram_val_targets)\n","                \n","                lemma_mask = (1 - ((lemma_targets == unk_idx) + (lemma_targets == pad_idx))).float()\n","                gram_val_mask = (1 - (gram_val_targets == pad_idx)).float()\n","                \n","                loss = ((lemma_loss * lemma_mask).sum() / lemma_mask.sum()) + ((gram_val_loss * gram_val_mask).sum() / gram_val_mask.sum())\n","                \n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n","            \n","            if best_val_loss and val_loss > best_val_loss:\n","                optimizer.param_groups[0]['lr'] /= 4.\n","                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n","            else:\n","                best_val_loss = val_loss\n","        print()\n","        generate(model)\n","        print()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Us02uX6CJ_W-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":765},"outputId":"3c2bf882-f9fa-47be-ceed-83548af9dcbe","executionInfo":{"status":"ok","timestamp":1542223212031,"user_tz":-180,"elapsed":1239071,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["# I train only 10 epoches, because the model starts overfitting\n","model = MultitaskLearningLMModel(vocab_size=len(train_iter.dataset.fields['lemma'].vocab),\n","                                 gr_val_size=len(train_iter.dataset.fields['gram_val'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['lemma'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['lemma'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=10, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":65,"outputs":[{"output_type":"stream","text":["[1 / 10] Train: Loss = 9.42209, PPX = 12358.36: 100%|██████████| 1145/1145 [01:49<00:00, 10.45it/s]\n","[1 / 10]   Val: Loss = 8.70358, PPX = 6024.43: 100%|██████████| 96/96 [00:13<00:00,  6.37it/s]\n","  0%|          | 0/1145 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","этом получался , что этом этом _ ! \" \n"],"name":"stdout"},{"output_type":"stream","text":["[2 / 10] Train: Loss = 8.26822, PPX = 3898.00: 100%|██████████| 1145/1145 [01:50<00:00, 10.39it/s]\n","[2 / 10]   Val: Loss = 8.30499, PPX = 4043.99: 100%|██████████| 96/96 [00:13<00:00,  7.93it/s]\n","[3 / 10] Train: Loss = 7.68100, PPX = 2166.79:   0%|          | 1/1145 [00:00<03:06,  6.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","не бывший . \n"],"name":"stdout"},{"output_type":"stream","text":["[3 / 10] Train: Loss = 7.89125, PPX = 2673.78: 100%|██████████| 1145/1145 [01:51<00:00, 10.31it/s]\n","[3 / 10]   Val: Loss = 8.20618, PPX = 3663.53: 100%|██████████| 96/96 [00:13<00:00,  7.02it/s]\n","  0%|          | 0/1145 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","как передававшими итар-тасс в продаж , заявленных на темы управлениях с у различно , как бы ни требующий от их дню . \n"],"name":"stdout"},{"output_type":"stream","text":["[4 / 10] Train: Loss = 7.60656, PPX = 2011.34: 100%|██████████| 1145/1145 [01:49<00:00, 10.44it/s]\n","[4 / 10]   Val: Loss = 8.11546, PPX = 3345.78: 100%|██████████| 96/96 [00:13<00:00,  7.00it/s]\n","  0%|          | 0/1145 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","не прибежала в империализм \" , - спросите одного из самой популярных \" . \n"],"name":"stdout"},{"output_type":"stream","text":["[5 / 10] Train: Loss = 7.37087, PPX = 1589.02: 100%|██████████| 1145/1145 [01:50<00:00, 10.27it/s]\n","[5 / 10]   Val: Loss = 8.07340, PPX = 3207.98: 100%|██████████| 96/96 [00:13<00:00,  7.12it/s]\n","  0%|          | 0/1145 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","нем по в пятницей также сообщит в пресс-служба . \n"],"name":"stdout"},{"output_type":"stream","text":["[6 / 10] Train: Loss = 7.15981, PPX = 1286.66: 100%|██████████| 1145/1145 [01:50<00:00, 10.40it/s]\n","[6 / 10]   Val: Loss = 8.14059, PPX = 3430.94: 100%|██████████| 96/96 [00:13<00:00,  6.51it/s]\n","  0%|          | 0/1145 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5\n","\n","я еще не , а в , ради что всем , чем и. бывшие доступное хороша ! \n"],"name":"stdout"},{"output_type":"stream","text":["[7 / 10] Train: Loss = 6.34995, PPX = 572.46: 100%|██████████| 1145/1145 [01:49<00:00, 11.83it/s]\n","[7 / 10]   Val: Loss = 8.03655, PPX = 3091.93: 100%|██████████| 96/96 [00:13<00:00,  6.99it/s]\n","  0%|          | 0/1145 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","как передающие заявлении академиками , в региону в областями , по информации , \" единстве \" никем не обещав в правительствам . \n"],"name":"stdout"},{"output_type":"stream","text":["[8 / 10] Train: Loss = 6.01130, PPX = 408.01: 100%|██████████| 1145/1145 [01:50<00:00, 10.39it/s]\n","[8 / 10]   Val: Loss = 8.14507, PPX = 3446.33: 100%|██████████| 96/96 [00:13<00:00,  7.95it/s]\n","  0%|          | 0/1145 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.25\n","\n","практическом этим нас ходившей не понимался , что в условиями зарплате энергии никакой не ставших . \n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 10] Train: Loss = 5.53659, PPX = 253.81: 100%|██████████| 1145/1145 [01:50<00:00, 10.35it/s]\n","[9 / 10]   Val: Loss = 8.28028, PPX = 3945.29: 100%|██████████| 96/96 [00:13<00:00,  7.07it/s]\n","  0%|          | 0/1145 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.3125\n","\n","в рядами случаем , после называемым немецкой фирмы , найдешь и. том , что нас всех политическом , скорее всех , в такой не по , котором еще будьте делах . \n"],"name":"stdout"},{"output_type":"stream","text":["[10 / 10] Train: Loss = 5.33797, PPX = 208.09: 100%|██████████| 1145/1145 [01:50<00:00, 10.38it/s]\n","[10 / 10]   Val: Loss = 8.34652, PPX = 4215.49: 100%|██████████| 96/96 [00:13<00:00,  7.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.078125\n","\n","как сообщило \" риа - новостями \" в - предсказания , \" не бывшую на базам \" , а , химии и. названных нацию \" . \n"],"name":"stdout"}]},{"metadata":{"id":"vL2xPe-BNRhu","colab_type":"text"},"cell_type":"markdown","source":["## Контролируемая генерация\n","\n","Хочется сделать генерацию более контролируемой - в идеале, задавать тему.\n","\n","Простой способ - сделать тематическое моделирование и найти в текстах какие-то темы - а потом передавать вектор тем вместе с эмбеддингом слова, чтобы модель училась генерировать тематически-согласованный текст."]},{"metadata":{"id":"amBlqg3jLMG4","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_poem(path):\n","    poem = []\n","    with open(path, encoding='utf8') as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if len(line) == 0:\n","                yield poem\n","                poem = []\n","                continue\n","            \n","            poem.extend(line.split() + ['\\\\n'])\n","            \n","perashki = list(read_poem('perashki.txt'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"exopH1jlN4fc","colab_type":"code","colab":{}},"cell_type":"code","source":["from gensim import corpora, models\n","\n","docs = [[word for word in poem if word != '\\\\n'] for poem in perashki]\n","\n","dictionary = corpora.Dictionary(docs)\n","dictionary.filter_n_most_frequent(100)\n","\n","bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n","\n","lda_model = models.LdaModel(bow_corpus, num_topics=5, id2word=dictionary, passes=5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LLPO9U1-Pakp","colab_type":"text"},"cell_type":"markdown","source":["Посмотреть, что выучилось, можно так:"]},{"metadata":{"id":"hzEg-8SZs8t7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":861},"outputId":"b0a15708-e86a-445c-d105-eca36e954303","executionInfo":{"status":"ok","timestamp":1542225596669,"user_tz":-180,"elapsed":18609,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["import pyLDAvis\n","import pyLDAvis.gensim\n","\n","pyLDAvis.enable_notebook()\n","pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n","\n","\n","<div id=\"ldavis_el621399806952063121410830656\"></div>\n","<script type=\"text/javascript\">\n","\n","var ldavis_el621399806952063121410830656_data = {\"mdsDat\": {\"Freq\": [22.56072235107422, 20.748170852661133, 20.24285125732422, 19.17510986328125, 17.27313995361328], \"cluster\": [1, 1, 1, 1, 1], \"topics\": [1, 2, 3, 4, 5], \"x\": [0.08468013525160804, 0.12954136108875117, 0.03251514615863743, 0.003210340440178298, -0.24994698293917497], \"y\": [0.10758920170391902, 0.12254688934204863, -0.21367127741446157, -0.08750818019228887, 0.07104336656078263]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [544.0, 295.0, 523.0, 475.0, 305.0, 193.0, 373.0, 216.0, 311.0, 382.0, 171.0, 222.0, 573.0, 244.0, 348.0, 262.0, 169.0, 453.0, 420.0, 318.0, 512.0, 384.0, 256.0, 251.0, 204.0, 153.0, 314.0, 332.0, 136.0, 277.0, 215.48020935058594, 157.77833557128906, 137.66757202148438, 135.57815551757812, 131.6661376953125, 124.2506332397461, 117.64904022216797, 106.42682647705078, 105.54650115966797, 94.48314666748047, 91.06513214111328, 90.72359466552734, 89.92772674560547, 88.56156921386719, 88.76294708251953, 88.38316345214844, 88.07148742675781, 83.75505828857422, 80.0928726196289, 78.74813842773438, 76.44832611083984, 74.3479995727539, 73.79720306396484, 74.23841094970703, 73.97032928466797, 72.78995513916016, 71.73197937011719, 71.68559265136719, 68.62005615234375, 68.74114990234375, 69.714599609375, 75.78679656982422, 188.98292541503906, 469.1324768066406, 160.03787231445312, 309.66473388671875, 189.044921875, 210.6497039794922, 294.26336669921875, 148.88946533203125, 246.81288146972656, 148.43756103515625, 188.81826782226562, 102.82459259033203, 280.0487365722656, 297.0688171386719, 281.8368225097656, 170.558349609375, 314.3563232421875, 223.70333862304688, 198.8029327392578, 261.1738586425781, 257.09832763671875, 274.8329772949219, 194.5504913330078, 271.3603515625, 228.9679718017578, 267.7101135253906, 199.1771240234375, 217.1068572998047, 199.97842407226562, 215.21168518066406, 225.99705505371094, 205.20616149902344, 213.83639526367188, 182.5011749267578, 194.38119506835938, 189.78338623046875, 138.29965209960938, 133.59185791015625, 122.99073028564453, 118.69528198242188, 118.1490707397461, 114.25981903076172, 114.2003402709961, 113.74592590332031, 109.46334838867188, 108.77906799316406, 111.09022521972656, 103.32518005371094, 101.70632934570312, 100.74222564697266, 101.44386291503906, 101.59271240234375, 97.72150421142578, 97.38276672363281, 97.0621566772461, 91.73699188232422, 85.15328979492188, 86.09657287597656, 82.48482513427734, 80.41710662841797, 77.61402893066406, 77.88988494873047, 77.03720092773438, 76.24126434326172, 75.49910736083984, 74.91155242919922, 206.47291564941406, 211.36331176757812, 104.70690155029297, 243.91639709472656, 296.318115234375, 119.30451202392578, 255.77223205566406, 178.24493408203125, 151.49557495117188, 235.44552612304688, 213.53480529785156, 170.29461669921875, 176.83306884765625, 161.0359344482422, 148.87461853027344, 159.8679656982422, 176.8207244873047, 163.5396728515625, 143.40699768066406, 147.00674438476562, 158.73110961914062, 135.7715606689453, 148.75730895996094, 155.48959350585938, 145.07498168945312, 126.38580322265625, 141.55393981933594, 143.4384765625, 130.8144989013672, 132.7220916748047, 128.23130798339844, 295.1991882324219, 152.77783203125, 122.59669494628906, 112.78263092041016, 111.1805648803711, 111.7945327758789, 112.82443237304688, 107.02420806884766, 101.83973693847656, 101.08124542236328, 101.96935272216797, 98.59864044189453, 97.07404327392578, 95.9854965209961, 94.4266586303711, 94.29872131347656, 90.4820556640625, 89.50550079345703, 89.4179458618164, 89.24849700927734, 86.6180191040039, 85.85209655761719, 83.82884216308594, 83.28020477294922, 80.41947174072266, 80.6662368774414, 77.83322143554688, 74.98350524902344, 75.53434753417969, 74.4505386352539, 275.2845458984375, 186.0561981201172, 312.5684814453125, 202.05548095703125, 181.04525756835938, 146.85704040527344, 219.6218719482422, 224.79376220703125, 197.6610565185547, 165.24343872070312, 209.92520141601562, 197.1324462890625, 176.288818359375, 130.94393920898438, 187.24542236328125, 197.1350860595703, 153.83746337890625, 162.71405029296875, 143.09385681152344, 184.62484741210938, 141.73072814941406, 158.30621337890625, 146.7048797607422, 137.9446563720703, 155.56057739257812, 132.1623992919922, 141.9621124267578, 132.3907012939453, 133.7913818359375, 133.75808715820312, 133.1250457763672, 169.0056610107422, 129.91468811035156, 113.98397827148438, 112.57349395751953, 111.64505004882812, 108.27418518066406, 106.2907943725586, 105.84234619140625, 105.33758544921875, 101.26799774169922, 99.40013122558594, 95.1938247680664, 89.26518249511719, 89.65164947509766, 86.58980560302734, 81.3088607788086, 81.54857635498047, 80.97628784179688, 80.89070892333984, 79.6073989868164, 74.31461334228516, 70.61386108398438, 111.38707733154297, 67.90847778320312, 67.64985656738281, 67.24401092529297, 67.89498138427734, 65.80755615234375, 65.650634765625, 64.54444885253906, 306.5082702636719, 188.8790283203125, 125.64201354980469, 158.3059844970703, 103.70780944824219, 156.73875427246094, 83.2322998046875, 176.29428100585938, 169.66090393066406, 171.88919067382812, 192.09329223632812, 157.5220184326172, 114.89494323730469, 189.52151489257812, 153.46031188964844, 155.2198944091797, 177.8054656982422, 128.63560485839844, 155.0647430419922, 143.96690368652344, 129.78482055664062, 121.21124267578125, 130.02333068847656, 139.2646026611328, 132.24620056152344, 121.31298065185547, 129.8822021484375, 130.95216369628906, 131.0509796142578, 124.48013305664062, 122.95381164550781, 193.02328491210938, 170.31024169921875, 135.35157775878906, 125.76963806152344, 113.17639923095703, 107.3941879272461, 103.91551971435547, 102.1401596069336, 96.69588470458984, 92.65465545654297, 87.16626739501953, 83.07439422607422, 82.12947082519531, 80.70584869384766, 78.95864868164062, 77.67640686035156, 76.05817413330078, 76.32337951660156, 74.71907043457031, 74.81864166259766, 73.27330780029297, 73.62720489501953, 72.83432006835938, 70.95024108886719, 67.65298461914062, 66.45879364013672, 66.41905975341797, 65.30713653564453, 65.49076080322266, 63.417198181152344, 64.5877685546875, 209.94613647460938, 94.76482391357422, 96.21199035644531, 140.10801696777344, 107.39032745361328, 142.08908081054688, 129.2754669189453, 141.17103576660156, 131.940673828125, 95.89292907714844, 126.88178253173828, 121.02654266357422, 121.89092254638672, 107.00995635986328, 111.30430603027344, 109.4765396118164, 101.54443359375, 102.54798889160156, 104.3980941772461, 101.72991180419922, 102.92281341552734, 101.35552978515625, 97.4928207397461], \"Term\": [\"\\u043b\\u0435\\u0442\", \"\\u0441\\u043d\\u0435\\u0433\", \"\\u043b\\u044e\\u0431\\u043e\\u0432\\u044c\", \"\\u0433\\u043e\\u0434\", \"\\u043a\\u0443\\u043f\\u0438\\u043b\", \"\\u043a\\u043e\\u0444\\u0435\", \"\\u0432\\u0441\\u0435\\u0433\\u043e\", \"\\u0441\\u043e\\u0440\\u043e\\u043a\", \"\\u043e\\u043a\\u043d\\u043e\", \"\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435\", \"\\u0437\\u0430\\u0442\\u0435\\u043c\", \"\\u043f\\u0443\\u0441\\u0442\\u044c\", \"\\u0447\\u0443\\u0442\\u044c\", \"\\u0431\\u044b\\u0432\\u0430\\u0435\\u0442\", \"\\u0431\\u0443\\u0434\\u0443\", \"\\u043f\\u0438\\u0448\\u0435\\u0442\", \"\\u043e\\u0441\\u0435\\u043d\\u044c\", \"\\u0436\\u0438\\u0437\\u043d\\u0438\", \"\\u043d\\u0438\\u043a\\u0442\\u043e\", \"\\u0434\\u0440\\u0443\\u0433\", \"\\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\", \"\\u0441\\u043c\\u043e\\u0442\\u0440\\u0438\\u0442\", \"\\u044d\\u0442\\u043e\\u043c\", \"\\u0432\\u043e\\u043a\\u0440\\u0443\\u0433\", \"\\u0433\\u043e\\u0434\\u0430\", \"\\u0434\\u043e\\u043a\\u0442\\u043e\\u0440\", \"\\u0445\\u043e\\u0442\\u044c\", \"\\u0434\\u0435\\u0442\\u0435\\u0439\", \"\\u043f\\u0440\\u0438\\u0448\\u043e\\u043b\", \"\\u0441\\u043e\\u043b\\u043d\\u0446\\u0435\", \"\\u0441\\u043e\\u0440\\u043e\\u043a\", \"\\u0442\\u0440\\u0443\\u0441\\u044b\", \"\\u0431\\u044b\\u043b\\u0438\", \"\\u0434\\u043d\\u0435\\u0439\", \"\\u0441\\u0442\\u0440\\u0430\\u0448\\u043d\\u0435\\u0439\", \"\\u0441\\u0438\\u0441\\u044c\\u043a\\u0438\", \"\\u0443\\u0442\\u0440\\u043e\", \"\\u0444\\u043e\\u0442\\u043e\", \"\\u043f\\u0440\\u0438\\u0448\\u043b\\u043e\\u0441\\u044c\", \"\\u043f\\u043e\\u0439\\u0434\\u0443\", \"\\u0442\\u0430\\u0442\\u044c\\u044f\\u043d\\u0430\", \"\\u043e\\u0441\\u0442\\u0430\\u043b\\u0441\\u044f\", \"\\u0432\\u0434\\u043e\\u043b\\u044c\", \"\\u043f\\u043e\\u043d\\u0438\\u043c\\u0430\\u0435\\u0442\", \"\\u0436\\u0438\\u0432\\u0451\\u0442\", \"\\u0436\\u0434\\u0443\\u0442\", \"\\u0442\\u0432\\u043e\\u044e\", \"\\u043b\\u0435\\u043d\\u0438\\u043d\", \"\\u0432\\u043e\\u0434\\u043a\\u0443\", \"\\u043a\\u0440\\u0443\\u0433\\u043e\\u043c\", \"\\u043e\\u0440\\u0433\\u0430\\u0437\\u043c\", \"\\u0433\\u043e\\u0434\\u044b\", \"\\u043f\\u043e\\u043f\\u0430\\u043b\", \"\\u0442\\u0435\\u043d\\u044c\", \"\\u0447\\u0438\\u0442\\u0430\\u044e\", \"\\u0443\\u043c\\u0438\\u0440\\u0430\\u0442\\u044c\", \"\\u0440\\u0430\\u0437\\u043d\\u044b\\u0445\", \"\\u043d\\u0430\\u0448\\u0435\\u043b\", \"\\u0434\\u0430\\u043b\\u0438\", \"\\u043d\\u0438\\u043a\\u043e\\u043c\\u0443\", \"\\u043b\\u0438\\u0446\\u0443\", \"\\u0436\\u0430\\u043b\\u044c\", \"\\u0433\\u043e\\u0434\\u0430\", \"\\u043b\\u0435\\u0442\", \"\\u043c\\u0438\\u0440\\u0435\", \"\\u0432\\u0441\\u0435\\u0433\\u043e\", \"\\u0440\\u0430\\u0431\\u043e\\u0442\\u0443\", \"\\u044d\\u0442\\u043e\\u043c\", \"\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435\", \"\\u043f\\u044f\\u0442\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0445\\u043e\\u0442\\u044c\", \"\\u0442\\u0440\\u0438\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0432\\u043e\\u0441\\u0435\\u043c\\u044c\", \"\\u043c\\u0435\\u0441\\u0442\\u0435\", \"\\u0436\\u0438\\u0437\\u043d\\u0438\", \"\\u0441\\u0435\\u043a\\u0441\", \"\\u0431\\u044b\\u043b\\u043e\", \"\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0434\\u0430\\u0436\\u0435\", \"\\u0431\", \"\\u043d\\u0430\\u0437\\u0430\\u0434\", \"\\u0441\\u043a\\u0430\\u0437\\u0430\\u043b\\u0430\", \"\\u0434\\u0432\\u0435\", \"\\u0447\\u0435\\u043c\", \"\\u0434\\u0430\\u0432\\u043d\\u043e\", \"\\u0436\\u0438\\u0437\\u043d\\u044c\", \"\\u043b\\u044e\\u0431\\u0432\\u0438\", \"\\u0433\\u043b\\u0435\\u0431\", \"\\u0431\\u0443\\u0434\\u0443\", \"\\u043c\\u043d\\u043e\\u0433\\u043e\", \"\\u043e\\u043b\\u0435\\u0433\\u0443\", \"\\u043b\\u044e\\u0431\\u043b\\u044e\", \"\\u043b\\u044e\\u0431\\u043e\\u0432\\u044c\", \"\\u043f\\u044f\\u0442\\u044c\", \"\\u0435\\u0449\\u0435\", \"\\u0445\\u043e\\u0442\\u0435\\u043b\", \"\\u043d\\u0438\\u043a\\u0442\\u043e\", \"\\u043b\\u044e\\u0434\\u0435\\u0439\", \"\\u0445\\u043e\\u0447\\u0435\\u0442\\u0441\\u044f\", \"\\u043c\\u0435\\u0441\\u0442\\u0430\", \"\\u0434\\u0443\\u0448\\u0430\", \"\\u0445\\u0432\\u0430\\u0442\\u0430\\u0435\\u0442\", \"\\u043c\\u043e\\u044e\", \"\\u043a\\u0440\\u0430\\u0441\\u0438\\u0432\\u044b\\u0439\", \"\\u043f\\u043e\\u043b\\u0435\", \"\\u043b\\u0435\\u0441\", \"\\u0434\\u043e\\u0431\\u0440\\u044b\\u0439\", \"\\u044d\\u0442\\u0438\\u043c\", \"\\u0443\\u043f\\u0430\\u043b\", \"\\u0433\\u043e\\u0441\\u043f\\u043e\\u0434\\u044c\", \"\\u0433\\u043e\\u043b\\u043e\\u0432\\u0443\", \"\\u0432\\u0440\\u043e\\u0434\\u0435\", \"\\u0440\\u0438\\u0441\\u0443\\u0435\\u0442\", \"\\u0434\\u043e\\u0436\\u0434\\u044f\", \"\\u0436\\u0434\\u0443\", \"\\u043f\\u043e\\u0441\\u0442\\u0430\\u0432\\u0438\\u043b\", \"\\u0433\\u043e\\u043b\\u043e\\u0432\\u044b\", \"\\u043d\\u0435\\u0434\\u0430\\u0432\\u043d\\u043e\", \"\\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0445\", \"\\u0443\\u0448\\u0451\\u043b\", \"\\u0435\\u0434\\u0443\", \"\\u043f\\u0430\\u043c\\u044f\\u0442\\u044c\", \"\\u0437\\u043e\\u0438\", \"\\u0440\\u0435\\u043a\\u0438\", \"\\u043f\\u0430\\u0440\\u043a\\u0435\", \"\\u0432\\u0435\\u0440\\u0438\\u0442\", \"\\u043f\\u0440\\u0435\\u0437\\u0438\\u0434\\u0435\\u043d\\u0442\", \"\\u0441\\u043a\\u0430\\u0436\\u0435\\u0442\", \"\\u043f\\u0443\\u0441\\u0442\\u044c\", \"\\u0431\\u044b\\u0432\\u0430\\u0435\\u0442\", \"\\u0431\\u0435\\u043b\\u044b\\u0439\", \"\\u0434\\u0440\\u0443\\u0433\", \"\\u043b\\u044e\\u0431\\u043e\\u0432\\u044c\", \"\\u0431\\u0443\\u0434\\u0435\\u043c\", \"\\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\", \"\\u043a\\u0430\\u043a\\u043e\\u0439\", \"\\u043f\\u0443\\u0442\\u0438\\u043d\", \"\\u0431\\u044b\\u0442\\u044c\", \"\\u043d\\u0430\\u0434\\u043e\", \"\\u0442\\u043e\\u0431\\u043e\\u0439\", \"\\u0443\\u043c\\u0435\\u0440\", \"\\u0445\\u043e\\u0440\\u043e\\u0448\\u043e\", \"\\u0437\\u043d\\u0430\\u0447\\u0438\\u0442\", \"\\u0441\\u0430\\u043c\", \"\\u0447\\u0435\\u043c\", \"\\u0434\\u0430\\u0432\\u0430\\u0439\", \"\\u043b\\u0438\", \"\\u0431\\u0443\\u0434\\u0443\", \"\\u043f\\u043e\\u0441\\u043b\\u0435\", \"\\u0437\\u043d\\u0430\\u044e\", \"\\u0442\\u043e\\u0436\\u0435\", \"\\u0437\\u0434\\u0435\\u0441\\u044c\", \"\\u0437\\u0430\\u0447\\u0435\\u043c\", \"\\u0434\\u0443\\u043c\\u0430\\u043b\", \"\\u0441\\u043e\\u0432\\u0441\\u0435\\u043c\", \"\\u0440\\u0443\\u043a\\u0438\", \"\\u0441\\u0432\\u0435\\u0442\", \"\\u0441\\u0435\\u0439\\u0447\\u0430\\u0441\", \"\\u0442\\u0430\\u043a\\u043e\\u0439\", \"\\u0441\\u043d\\u0435\\u0433\", \"\\u0434\\u043e\\u043a\\u0442\\u043e\\u0440\", \"\\u0442\\u0432\\u043e\\u0438\\u0445\", \"\\u043a\\u043e\\u043d\\u0441\\u0442\\u0430\\u043d\\u0442\\u0438\\u043d\", \"\\u043e\\u043a\\u043d\\u0435\", \"\\u0431\\u0443\\u0434\\u0443\\u0442\", \"\\u043e\\u043a\\u043d\\u043e\\u043c\", \"\\u0441\\u0430\\u043c\\u043e\\u043c\", \"\\u0445\\u043e\\u0434\\u0438\\u0442\\u044c\", \"\\u043f\\u043e\\u043b\\u0443\", \"\\u0441\\u043f\\u0438\\u043d\\u0435\", \"\\u0434\\u0435\\u043b\\u043e\", \"\\u043f\\u0440\\u0438\\u0445\\u043e\\u0434\\u044f\\u0442\", \"\\u0433\\u0440\\u043e\\u0431\\u0443\", \"\\u0431\\u043e\\u043b\\u044c\", \"\\u0436\\u0434\\u0430\\u0442\\u044c\", \"\\u043d\\u0430\\u0434\\u043f\\u0438\\u0441\\u044c\", \"\\u0442\\u0435\\u043c\\u043d\\u043e\\u0442\\u0435\", \"\\u0441\\u043b\\u044b\\u0448\\u043d\\u043e\", \"\\u0442\\u0443\\u043c\\u0430\\u043d\", \"\\u0434\\u0435\\u043b\\u0435\", \"\\u043d\\u043e\\u0433\\u043e\\u0439\", \"\\u0441\\u0442\\u0430\\u043d\\u043e\\u0432\\u0438\\u0442\\u0441\\u044f\", \"\\u0431\\u0435\\u0440\\u0435\\u0442\", \"\\u043b\\u0435\\u0436\\u0430\\u043b\", \"\\u043f\\u043e\\u043b\\u043d\\u043e\\u0447\\u044c\", \"\\u043c\\u0438\\u0433\", \"\\u043b\\u0435\\u0442\\u044f\\u0442\", \"\\u0436\\u0435\\u043d\\u0438\\u0442\\u044c\\u0441\\u044f\", \"\\u043c\\u0443\\u0436\\u0447\\u0438\\u043d\", \"\\u0441\\u043c\\u043e\\u0442\\u0440\\u0438\\u0442\", \"\\u0432\\u043e\\u043a\\u0440\\u0443\\u0433\", \"\\u0447\\u0443\\u0442\\u044c\", \"\\u043e\\u043a\\u043d\\u043e\", \"\\u0441\\u043e\\u043b\\u043d\\u0446\\u0435\", \"\\u0432\\u0438\\u0434\\u0435\\u043b\", \"\\u0431\\u0443\\u0434\\u0442\\u043e\", \"\\u0440\\u0443\\u043a\\u0438\", \"\\u043d\\u0438\\u043a\\u0442\\u043e\", \"\\u0441\\u0438\\u0434\\u0438\\u0442\", \"\\u043d\\u0430\\u0434\", \"\\u043b\\u0435\\u0436\\u0438\\u0442\", \"\\u043f\\u0440\\u0438\\u0445\\u043e\\u0434\\u0438\\u0442\", \"\\u0432\\u0435\\u0442\\u0435\\u0440\", \"\\u044d\\u0442\\u043e\\u0442\", \"\\u0441\\u0442\\u043e\\u0438\\u0442\", \"\\u0442\\u0435\\u0445\", \"\\u043a\\u043e\", \"\\u043d\\u043e\\u0433\\u0438\", \"\\u0437\\u0434\\u0435\\u0441\\u044c\", \"\\u043c\\u043e\\u0440\\u0435\", \"\\u043c\\u043e\\u0436\\u043d\\u043e\", \"\\u0442\\u0430\\u043a\\u043e\\u0439\", \"\\u0434\\u043e\\u043c\", \"\\u043d\\u0435\\u0439\", \"\\u0434\\u043e\\u0436\\u0434\\u044c\", \"\\u043d\\u0435\\u0433\\u043e\", \"\\u043d\\u0435\\u0431\\u043e\", \"\\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\", \"\\u0432\\u0440\\u0435\\u043c\\u044f\", \"\\u0432\\u0441\\u0435\\u0433\\u0434\\u0430\", \"\\u043e\\u0441\\u0435\\u043d\\u044c\", \"\\u0441\\u043f\\u0440\\u043e\\u0441\\u0438\\u043b\\u0430\", \"\\u043c\\u043e\\u0437\\u0433\", \"\\u0432\\u0432\\u0435\\u0440\\u0445\", \"\\u043d\\u043e\\u0441\\u043a\\u0438\", \"\\u0432\\u043e\\u0434\\u0430\", \"\\u043e\\u0442\\u043a\\u0443\\u0434\\u0430\", \"\\u043c\\u0430\\u043c\\u044b\", \"\\u043f\\u0435\\u0442\\u0440\\u0443\", \"\\u0441\\u043a\\u043e\\u0440\\u0435\\u0435\", \"\\u0441\\u0432\\u043e\\u0438\\u043c\", \"\\u0437\\u0443\\u0445\\u0440\\u044b\", \"\\u043a\\u043e\\u0441\\u0442\\u044e\\u043c\\u0435\", \"\\u043a\\u0440\\u0438\\u043a\\u043d\\u0443\\u043b\", \"\\u0448\\u043b\\u0438\", \"\\u043e\\u0447\\u043a\\u0438\", \"\\u0432\\u044b\\u0448\\u043b\\u0438\", \"\\u0447\\u0430\\u0441\\u043e\\u0432\", \"\\u0440\\u0430\\u0431\\u043e\\u0442\\u0435\", \"\\u0442\\u0430\\u043a\\u0443\\u044e\", \"\\u0432\\u0438\\u0434\", \"\\u0432\\u044b\\u043f\\u0438\\u043b\", \"\\u0440\\u043e\\u0434\\u0438\\u043b\\u0441\\u044f\", \"\\u0437\\u0440\\u044f\", \"\\u043f\\u043e\\u0451\\u0442\", \"\\u043f\\u0440\\u0435\\u0434\\u043b\\u043e\\u0436\\u0438\\u043b\", \"\\u043f\\u043e\\u043b\\u0447\\u0430\\u0441\\u0430\", \"\\u043c\\u043e\\u0441\\u043a\\u0432\\u0435\", \"\\u0433\\u043b\\u044f\\u0436\\u0443\", \"\\u0441\\u0442\\u043e\\u043b\\u044c\\u043a\\u043e\", \"\\u0433\\u043e\\u0434\", \"\\u043f\\u0438\\u0448\\u0435\\u0442\", \"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0438\\u043b\", \"\\u0430\\u043d\\u0442\\u043e\\u043d\", \"\\u0440\\u0443\\u0431\\u043b\\u0435\\u0439\", \"\\u043d\\u043e\\u0432\\u044b\\u0439\", \"\\u0442\\u044b\\u0441\\u044f\\u0447\", \"\\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0439\", \"\\u0438\\u0441\\u0443\\u0441\", \"\\u043a\\u0430\\u0436\\u0434\\u044b\\u0439\", \"\\u043d\\u0438\\u043a\\u043e\\u043b\\u0430\\u0439\", \"\\u043e\\u043a\\u0441\\u0430\\u043d\\u0435\", \"\\u0434\\u0435\\u0434\", \"\\u0435\\u0439\", \"\\u043d\\u043e\\u0447\\u044c\", \"\\u043c\\u0430\\u043c\\u0430\", \"\\u0433\\u043b\\u0435\\u0431\", \"\\u043f\\u0435\\u0442\\u0440\\u0430\", \"\\u043d\\u0435\\u0439\", \"\\u043b\\u0438\\u0446\\u043e\", \"\\u043a\\u0440\\u0438\\u0447\\u0438\\u0442\", \"\\u0438\\u0434\\u0443\\u0442\", \"\\u043f\\u0440\\u0438\\u0445\\u043e\\u0434\\u0438\\u0442\", \"\\u043e\\u043f\\u044f\\u0442\\u044c\", \"\\u043f\\u044f\\u0442\\u044c\", \"\\u043e\\u0434\\u043d\\u0430\\u0436\\u0434\\u044b\", \"\\u0434\\u043e\\u043c\\u043e\\u0439\", \"\\u0435\\u0432\\u0433\\u0435\\u043d\\u0438\\u0439\", \"\\u0441\\u0442\\u043e\\u0438\\u0442\", \"\\u043d\\u0438\\u0445\", \"\\u0432\\u0435\\u0441\\u044c\", \"\\u043a\\u043e\\u0444\\u0435\", \"\\u0437\\u0430\\u0442\\u0435\\u043c\", \"\\u043f\\u0440\\u0438\\u0448\\u043e\\u043b\", \"\\u043a\\u0430\", \"\\u0441\\u044e\\u0434\\u0430\", \"\\u0432\\u043d\\u043e\\u0432\\u044c\", \"\\u043d\\u043e\\u043c\\u0435\\u0440\", \"\\u0438\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442\", \"\\u0434\\u043e\\u0441\\u0442\\u0430\\u043b\", \"\\u0434\\u044f\\u0434\\u044f\", \"\\u0438\\u043b\\u044c\\u044e\", \"\\u0440\\u043e\\u044f\\u043b\\u044c\", \"\\u0432\\u0441\\u043b\\u0435\\u0434\", \"\\u0442\\u0432\\u043e\\u0451\", \"\\u0441\\u0435\\u043c\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0431\\u0440\\u043e\\u0441\\u0438\\u043b\", \"\\u0435\\u043b\", \"\\u043e\\u0442\\u0432\\u0435\\u0447\\u0430\\u0435\\u0442\", \"\\u0432\\u0445\\u043e\\u0434\\u0438\\u0442\", \"\\u0445\\u043e\\u0442\\u0435\\u043b\\u043e\\u0441\\u044c\", \"\\u0432\\u0441\\u0435\\u0439\", \"\\u0448\\u0451\\u043b\", \"\\u0431\\u043e\\u043b\\u044c\\u0448\\u0438\\u043c\", \"\\u0431\\u0443\\u0442\\u044b\\u043b\\u043a\\u0443\", \"\\u0434\\u0435\\u0442\\u0441\\u0442\\u0432\\u0430\", \"\\u043f\\u043e\\u0448\\u043b\\u0438\", \"\\u0437\\u0438\\u043c\\u043e\\u0439\", \"\\u043f\\u043e\\u043a\\u0430\\u0436\\u0443\", \"\\u0434\\u043e\\u043c\\u0443\", \"\\u043b\\u044c\", \"\\u0441\\u043d\\u0435\\u0433\\u0443\", \"\\u043a\\u0443\\u043f\\u0438\\u043b\", \"\\u0441\\u043b\\u0443\\u0447\\u0430\\u0439\", \"\\u0434\\u0440\\u0443\\u0433\\u0438\\u0445\", \"\\u0433\\u043b\\u0435\\u0431\\u0430\", \"\\u0432\\u043e\\u0434\\u044b\", \"\\u0434\\u0435\\u0442\\u0435\\u0439\", \"\\u043c\\u043e\\u0436\\u043d\\u043e\", \"\\u043e\\u043f\\u044f\\u0442\\u044c\", \"\\u0434\\u0430\\u0432\\u0430\\u0439\", \"\\u0436\\u0435\\u043d\\u0449\\u0438\\u043d\", \"\\u043b\\u044e\\u0431\\u043b\\u044e\", \"\\u043a\\u0443\\u0434\\u0430\", \"\\u0431\\u0443\\u0434\\u0442\\u043e\", \"\\u043e\\u043a\\u043d\\u043e\", \"\\u043d\\u0438\\u043c\", \"\\u0433\\u043e\\u0434\", \"\\u043f\\u0435\\u0440\\u0435\\u0434\", \"\\u0441\\u0430\\u043c\", \"\\u0434\\u043e\\u043c\\u043e\\u0439\", \"\\u0441\\u0442\\u0430\\u043b\", \"\\u0432\\u0435\\u0441\\u044c\", \"\\u043b\\u0438\\u0446\\u043e\", \"\\u0438\\u043c\"], \"Total\": [544.0, 295.0, 523.0, 475.0, 305.0, 193.0, 373.0, 216.0, 311.0, 382.0, 171.0, 222.0, 573.0, 244.0, 348.0, 262.0, 169.0, 453.0, 420.0, 318.0, 512.0, 384.0, 256.0, 251.0, 204.0, 153.0, 314.0, 332.0, 136.0, 277.0, 216.17633056640625, 158.4861602783203, 138.36326599121094, 136.27333068847656, 132.36180114746094, 124.95114135742188, 118.35676574707031, 107.12779998779297, 106.24567413330078, 95.17945861816406, 91.762451171875, 91.42514038085938, 90.63800048828125, 89.26260375976562, 89.46792602539062, 89.0871810913086, 88.77347564697266, 84.45183563232422, 80.79283905029297, 79.4515609741211, 77.15123748779297, 75.04368591308594, 74.4935531616211, 74.94559478759766, 74.67648315429688, 73.48674774169922, 72.42626953125, 72.40536499023438, 69.3163070678711, 69.44014739990234, 70.42936706542969, 76.84268951416016, 204.614013671875, 544.6622924804688, 172.2738037109375, 373.95721435546875, 220.30081176757812, 256.6351623535156, 382.6479797363281, 171.1482696533203, 314.157470703125, 175.15000915527344, 237.74362182617188, 111.39976501464844, 453.09014892578125, 498.5888977050781, 489.602294921875, 235.04176330566406, 610.0460205078125, 369.6694030761719, 311.16632080078125, 496.6947021484375, 484.54656982421875, 553.0687866210938, 314.99542236328125, 577.0258178710938, 438.9873046875, 619.7046508789062, 348.656494140625, 429.188232421875, 358.58990478515625, 458.5895080566406, 523.5149536132812, 403.6670227050781, 474.1827392578125, 302.6196594238281, 420.91485595703125, 476.60174560546875, 138.99795532226562, 134.29144287109375, 123.68889617919922, 119.40217590332031, 118.8536148071289, 114.95816040039062, 114.90155792236328, 114.45550537109375, 110.15983581542969, 109.47515106201172, 111.8319320678711, 104.02620697021484, 102.40536499023438, 101.43824005126953, 102.14850616455078, 102.3031234741211, 98.42411041259766, 98.08857727050781, 97.76663970947266, 92.43419647216797, 85.85411071777344, 86.81485748291016, 83.18245697021484, 81.12376403808594, 78.31398010253906, 78.594482421875, 77.7428207397461, 76.94027709960938, 76.1920166015625, 75.61346435546875, 222.61614990234375, 244.4769287109375, 112.61661529541016, 318.7145690917969, 523.5149536132812, 150.13467407226562, 512.8983764648438, 325.8477783203125, 244.4193878173828, 556.0104370117188, 510.58282470703125, 323.0531311035156, 362.5904235839844, 304.5760803222656, 292.4457702636719, 382.87103271484375, 553.0687866210938, 444.928466796875, 315.1905212402344, 348.656494140625, 518.6195678710938, 275.11712646484375, 415.34283447265625, 563.29736328125, 418.95404052734375, 215.3045196533203, 442.7806396484375, 495.2846984863281, 294.755615234375, 373.7975158691406, 330.68511962890625, 295.8968200683594, 153.47337341308594, 123.29901885986328, 113.4896469116211, 111.88426208496094, 112.50321960449219, 113.54784393310547, 107.71833038330078, 102.54000854492188, 101.77798461914062, 102.67437744140625, 99.29753875732422, 97.77730560302734, 96.68864440917969, 95.13621520996094, 95.02407836914062, 91.1855697631836, 90.2052001953125, 90.11861419677734, 89.95189666748047, 87.31013488769531, 86.55462646484375, 84.53276824951172, 83.9882583618164, 81.12454986572266, 81.38003540039062, 78.53314208984375, 75.68240356445312, 76.2466049194336, 75.15570068359375, 384.5147705078125, 251.254150390625, 573.0380249023438, 311.07562255859375, 277.91845703125, 211.90272521972656, 461.8274841308594, 495.2846984863281, 420.91485595703125, 299.6629638671875, 477.89947509765625, 445.0104675292969, 356.6329040527344, 194.46701049804688, 437.4946594238281, 522.4606323242188, 296.6550598144531, 364.71923828125, 269.9554138183594, 563.29736328125, 270.5556640625, 406.76141357421875, 330.68511962890625, 277.9328308105469, 510.1299133300781, 279.11590576171875, 427.45147705078125, 303.4329833984375, 512.8983764648438, 567.6118774414062, 403.1087646484375, 169.71054077148438, 130.61251831054688, 114.68466186523438, 113.27365112304688, 112.34090423583984, 108.97249603271484, 106.9906005859375, 106.54155731201172, 106.04053497314453, 101.97135162353516, 100.09586334228516, 95.89912414550781, 89.95929718017578, 90.35523223876953, 87.28596496582031, 82.00733184814453, 82.24950408935547, 81.67327880859375, 81.59935760498047, 80.30908203125, 75.01164245605469, 71.31404876708984, 112.50509643554688, 68.60520935058594, 68.34395599365234, 67.94552612304688, 68.6070327758789, 66.49960327148438, 66.34527587890625, 65.24222564697266, 475.76300048828125, 262.11102294921875, 163.24761962890625, 225.55772399902344, 133.34506225585938, 271.1673889160156, 95.85369110107422, 341.4421081542969, 348.6075134277344, 375.6268310546875, 478.190673828125, 366.92291259765625, 182.73580932617188, 612.01953125, 373.23455810546875, 391.0216064453125, 619.7046508789062, 270.32080078125, 510.1299133300781, 444.673583984375, 307.87469482421875, 246.6261444091797, 356.6329040527344, 529.9131469726562, 403.6670227050781, 260.8346252441406, 447.6407165527344, 518.2484130859375, 522.4606323242188, 527.2091064453125, 487.1401672363281, 193.73049926757812, 171.02169799804688, 136.04440307617188, 126.46572875976562, 113.87926483154297, 108.09243774414062, 104.62141418457031, 102.84539794921875, 97.3922119140625, 93.35169982910156, 87.86494445800781, 83.76765441894531, 82.8342514038086, 81.41703033447266, 79.66268157958984, 78.37248229980469, 76.75554656982422, 77.03121185302734, 75.4204330444336, 75.52153015136719, 73.96807861328125, 74.33078002929688, 73.53455352783203, 71.64472198486328, 68.3522720336914, 67.155029296875, 67.12559509277344, 66.01624298095703, 66.20478820800781, 64.113037109375, 65.2986068725586, 305.89459228515625, 107.57032775878906, 119.66133117675781, 244.62612915039062, 166.95367431640625, 332.7461853027344, 406.76141357421875, 529.9131469726562, 444.928466796875, 172.9197998046875, 458.5895080566406, 401.7246398925781, 461.8274841308594, 311.07562255859375, 394.39166259765625, 475.76300048828125, 335.7907409667969, 382.87103271484375, 447.6407165527344, 412.38909912109375, 487.1401672363281, 444.673583984375, 444.3913879394531], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.485700011253357, 1.4845000505447388, 1.4838999509811401, 1.4838000535964966, 1.4837000370025635, 1.483299970626831, 1.4830000400543213, 1.4823999404907227, 1.4823999404907227, 1.481600046157837, 1.4812999963760376, 1.4812999963760376, 1.4810999631881714, 1.4810999631881714, 1.4809999465942383, 1.4809999465942383, 1.4809999465942383, 1.4807000160217285, 1.480299949645996, 1.4801000356674194, 1.4797999858856201, 1.479599952697754, 1.479599952697754, 1.4795000553131104, 1.4795000553131104, 1.4794000387191772, 1.4793000221252441, 1.4789999723434448, 1.4788999557495117, 1.4788000583648682, 1.4788000583648682, 1.475100040435791, 1.409500002861023, 1.3396999835968018, 1.4153000116348267, 1.3003000020980835, 1.3359999656677246, 1.2914999723434448, 1.2263000011444092, 1.3495999574661255, 1.2476999759674072, 1.3235000371932983, 1.2585999965667725, 1.4089000225067139, 1.0077999830245972, 0.9710999727249146, 0.9366999864578247, 1.1683000326156616, 0.8259999752044678, 0.9866999983787537, 1.0408999919891357, 0.8461999893188477, 0.8551999926567078, 0.7896000146865845, 1.007099986076355, 0.734499990940094, 0.838100016117096, 0.6496000289916992, 0.929099977016449, 0.8075000047683716, 0.9049999713897705, 0.7324000000953674, 0.6488999724388123, 0.8123999834060669, 0.6926000118255615, 0.9832000136375427, 0.7164000272750854, 0.5681999921798706, 1.5677000284194946, 1.5674999952316284, 1.5671000480651855, 1.5667999982833862, 1.5667999982833862, 1.56659996509552, 1.56659996509552, 1.566499948501587, 1.5664000511169434, 1.5663000345230103, 1.566100001335144, 1.565999984741211, 1.5658999681472778, 1.5657999515533447, 1.5657999515533447, 1.5657000541687012, 1.565500020980835, 1.565500020980835, 1.565500020980835, 1.5650999546051025, 1.5644999742507935, 1.5643999576568604, 1.5642999410629272, 1.5640000104904175, 1.5636999607086182, 1.5636999607086182, 1.563599944114685, 1.563599944114685, 1.563599944114685, 1.5634000301361084, 1.4974000453948975, 1.4271999597549438, 1.499899983406067, 1.3051999807357788, 1.003600001335144, 1.342900037765503, 0.8769000172615051, 0.9693999886512756, 1.0944000482559204, 0.7134000062942505, 0.7009999752044678, 0.9323999881744385, 0.8546000123023987, 0.9354000091552734, 0.8974999785423279, 0.699400007724762, 0.4323999881744385, 0.5719000101089478, 0.7851999998092651, 0.7091000080108643, 0.3887999951839447, 0.8665000200271606, 0.5458999872207642, 0.2854999899864197, 0.5121999979019165, 1.0399999618530273, 0.43230000138282776, 0.3334999978542328, 0.7602999806404114, 0.5372999906539917, 0.6254000067710876, 1.5950000286102295, 1.5928000211715698, 1.5916999578475952, 1.5910999774932861, 1.5910999774932861, 1.590999960899353, 1.590999960899353, 1.59089994430542, 1.590499997138977, 1.590499997138977, 1.590499997138977, 1.5902999639511108, 1.5901000499725342, 1.5901000499725342, 1.589900016784668, 1.5896999835968018, 1.5895999670028687, 1.5895999670028687, 1.5895999670028687, 1.5894999504089355, 1.589400053024292, 1.5892000198364258, 1.5889999866485596, 1.5888999700546265, 1.5886000394821167, 1.5886000394821167, 1.5884000062942505, 1.5880999565124512, 1.5880000591278076, 1.5879000425338745, 1.263200044631958, 1.2970000505447388, 0.9911999702453613, 1.1658999919891357, 1.1687999963760376, 1.2307000160217285, 0.8540999889373779, 0.8073999881744385, 0.8414999842643738, 1.0020999908447266, 0.7746999859809875, 0.7831000089645386, 0.892799973487854, 1.2019000053405762, 0.7487000226974487, 0.6226999759674072, 0.9406999945640564, 0.7901999950408936, 0.9625999927520752, 0.48190000653266907, 0.9508000016212463, 0.6536999940872192, 0.784600019454956, 0.8967999815940857, 0.4097000062465668, 0.8497999906539917, 0.4950999915599823, 0.7680000066757202, 0.25360000133514404, 0.15199999511241913, 0.4894999861717224, 1.6474000215530396, 1.6461999416351318, 1.645400047302246, 1.645400047302246, 1.645300030708313, 1.6450999975204468, 1.6449999809265137, 1.6449999809265137, 1.6448999643325806, 1.6446000337600708, 1.6446000337600708, 1.6441999673843384, 1.6438000202178955, 1.6437000036239624, 1.6434999704360962, 1.6430000066757202, 1.6430000066757202, 1.6430000066757202, 1.642799973487854, 1.642799973487854, 1.642199993133545, 1.641700029373169, 1.6416000127792358, 1.6412999629974365, 1.6412999629974365, 1.6411999464035034, 1.6411000490188599, 1.6411000490188599, 1.6410000324249268, 1.6407999992370605, 1.211899995803833, 1.3238999843597412, 1.3897000551223755, 1.2975000143051147, 1.4002000093460083, 1.1033999919891357, 1.5104000568389893, 0.9904999732971191, 0.9314000010490417, 0.8697999715805054, 0.7394999861717224, 0.8059999942779541, 1.1875, 0.47929999232292175, 0.7627999782562256, 0.7275999784469604, 0.40299999713897705, 0.9089000225067139, 0.46070000529289246, 0.5238000154495239, 0.7876999974250793, 0.9412000179290771, 0.6425999999046326, 0.31520000100135803, 0.5356000065803528, 0.8859999775886536, 0.4142000079154968, 0.2759000062942505, 0.2685999870300293, 0.20810000598430634, 0.27480000257492065, 1.7524000406265259, 1.7517999410629272, 1.7509000301361084, 1.750499963760376, 1.7497999668121338, 1.749500036239624, 1.7491999864578247, 1.7490999698638916, 1.7488000392913818, 1.7484999895095825, 1.7480000257492065, 1.7476999759674072, 1.747499942779541, 1.7472000122070312, 1.7470999956130981, 1.7470999956130981, 1.746899962425232, 1.7467999458312988, 1.7467000484466553, 1.7467000484466553, 1.7466000318527222, 1.746500015258789, 1.746399998664856, 1.7462999820709229, 1.7457000017166138, 1.7455999851226807, 1.7453999519348145, 1.7452000379562378, 1.7452000379562378, 1.7451000213623047, 1.7451000213623047, 1.3796000480651855, 1.6292999982833862, 1.5378999710083008, 1.198699951171875, 1.3148000240325928, 0.9050999879837036, 0.6097000241279602, 0.4332999885082245, 0.5404999852180481, 1.1663999557495117, 0.47110000252723694, 0.5562999844551086, 0.42399999499320984, 0.6888999938964844, 0.4909000098705292, 0.28679999709129333, 0.5600000023841858, 0.43869999051094055, 0.3001999855041504, 0.3564000129699707, 0.2013999968767166, 0.27730000019073486, 0.23909999430179596], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.350399971008301, -6.662099838256836, -6.798399925231934, -6.813700199127197, -6.8429999351501465, -6.900899887084961, -6.95550012588501, -7.055799961090088, -7.0640997886657715, -7.174799919128418, -7.211699962615967, -7.215400218963623, -7.2241997718811035, -7.23960018157959, -7.237299919128418, -7.241600036621094, -7.245100021362305, -7.295400142669678, -7.340099811553955, -7.35699987411499, -7.386600017547607, -7.4145002365112305, -7.421899795532227, -7.415999889373779, -7.419600009918213, -7.435699939727783, -7.450300216674805, -7.451000213623047, -7.494699954986572, -7.4928998947143555, -7.478799819946289, -7.395299911499023, -6.481599807739258, -5.572400093078613, -6.647799968719482, -5.987800121307373, -6.481299877166748, -6.3730998039245605, -6.03879976272583, -6.71999979019165, -6.214600086212158, -6.723100185394287, -6.482500076293945, -7.090199947357178, -6.0883002281188965, -6.029300212860107, -6.081900119781494, -6.584199905395508, -5.972700119018555, -6.312900066375732, -6.4309000968933105, -6.158100128173828, -6.173799991607666, -6.107100009918213, -6.452600002288818, -6.119800090789795, -6.289700031280518, -6.133299827575684, -6.429100036621094, -6.342899799346924, -6.425000190734863, -6.351600170135498, -6.302700042724609, -6.399199962615967, -6.357999801635742, -6.516499996185303, -6.453400135040283, -6.477399826049805, -6.710100173950195, -6.744699954986572, -6.827400207519531, -6.8628997802734375, -6.867599964141846, -6.901000022888184, -6.901500225067139, -6.9054999351501465, -6.943900108337402, -6.950200080871582, -6.929200172424316, -7.0015997886657715, -7.017399787902832, -7.026899814605713, -7.019999980926514, -7.018499851226807, -7.057400226593018, -7.060800075531006, -7.0640997886657715, -7.12060022354126, -7.195000171661377, -7.184000015258789, -7.226900100708008, -7.252299785614014, -7.287700176239014, -7.284200191497803, -7.295199871063232, -7.305600166320801, -7.315400123596191, -7.323200225830078, -6.309299945831299, -6.285900115966797, -6.98829984664917, -6.1427001953125, -5.9481000900268555, -6.857800006866455, -6.095200061798096, -6.456299781799316, -6.618899822235107, -6.177999973297119, -6.275700092315674, -6.501999855041504, -6.464300155639648, -6.5578999519348145, -6.63640022277832, -6.565100193023682, -6.464399814605713, -6.542399883270264, -6.673799991607666, -6.64900016784668, -6.572299957275391, -6.728499889373779, -6.637199878692627, -6.592899799346924, -6.662199974060059, -6.80019998550415, -6.686800003051758, -6.673600196838379, -6.765699863433838, -6.751200199127197, -6.785699844360352, -5.927199840545654, -6.585899829864502, -6.8059000968933105, -6.889400005340576, -6.90369987487793, -6.898200035095215, -6.888999938964844, -6.941800117492676, -6.991399765014648, -6.998899936676025, -6.990200042724609, -7.023799896240234, -7.039400100708008, -7.050600051879883, -7.066999912261963, -7.068399906158447, -7.1097002029418945, -7.120500087738037, -7.121500015258789, -7.1234002113342285, -7.153299808502197, -7.162199974060059, -7.186100006103516, -7.192599773406982, -7.22760009765625, -7.2245001792907715, -7.260300159454346, -7.297599792480469, -7.290299892425537, -7.304699897766113, -5.997000217437744, -6.388800144195557, -5.869999885559082, -6.306300163269043, -6.416100025177002, -6.625400066375732, -6.222899913787842, -6.199699878692627, -6.3282999992370605, -6.507400035858154, -6.268099784851074, -6.330999851226807, -6.442699909210205, -6.740099906921387, -6.382400035858154, -6.330999851226807, -6.57889986038208, -6.522799968719482, -6.651299953460693, -6.396500110626221, -6.660900115966797, -6.550300121307373, -6.626399993896484, -6.688000202178955, -6.567800045013428, -6.730800151824951, -6.659299850463867, -6.729100227355957, -6.718599796295166, -6.718800067901611, -6.723599910736084, -6.430699825286865, -6.69379997253418, -6.8246002197265625, -6.836999893188477, -6.845300197601318, -6.875999927520752, -6.894499778747559, -6.89870023727417, -6.903500080108643, -6.94290018081665, -6.96150016784668, -7.004700183868408, -7.068999767303467, -7.064700126647949, -7.0995001792907715, -7.162399768829346, -7.1595001220703125, -7.166500091552734, -7.167600154876709, -7.183499813079834, -7.252299785614014, -7.303400039672852, -6.847599983215332, -7.34250020980835, -7.34630012512207, -7.35230016708374, -7.342700004577637, -7.373899936676025, -7.376299858093262, -7.3933000564575195, -5.835400104522705, -6.319499969482422, -6.727200031280518, -6.496099948883057, -6.919099807739258, -6.506100177764893, -7.138999938964844, -6.388500213623047, -6.426799774169922, -6.41379976272583, -6.302700042724609, -6.501100063323975, -6.8165998458862305, -6.316100120544434, -6.527200222015381, -6.5157999992370605, -6.380000114440918, -6.703700065612793, -6.5167999267578125, -6.591100215911865, -6.694799900054932, -6.7631001472473145, -6.69290018081665, -6.624300003051758, -6.676000118255615, -6.76230001449585, -6.693999767303467, -6.685800075531006, -6.685100078582764, -6.736499786376953, -6.748799800872803, -6.193399906158447, -6.318600177764893, -6.548299789428711, -6.621699810028076, -6.727200031280518, -6.779699802398682, -6.812600135803223, -6.829800128936768, -6.8846001625061035, -6.927299976348877, -6.988399982452393, -7.036499977111816, -7.047900199890137, -7.065400123596191, -7.087299823760986, -7.103600025177002, -7.12470006942749, -7.121200084686279, -7.142499923706055, -7.14109992980957, -7.1620001792907715, -7.157199859619141, -7.168000221252441, -7.194200038909912, -7.241799831390381, -7.2596001625061035, -7.260200023651123, -7.277100086212158, -7.2743000984191895, -7.30649995803833, -7.2881999015808105, -6.109300136566162, -6.904799938201904, -6.889599800109863, -6.513800144195557, -6.779699802398682, -6.49970006942749, -6.594200134277344, -6.506199836730957, -6.573800086975098, -6.89300012588501, -6.6128997802734375, -6.660200119018555, -6.65310001373291, -6.783299922943115, -6.743899822235107, -6.760499954223633, -6.835700035095215, -6.825900077819824, -6.808000087738037, -6.833899974822998, -6.822199821472168, -6.837600231170654, -6.876399993896484]}, \"token.table\": {\"Topic\": [2, 4, 1, 2, 3, 4, 1, 2, 3, 3, 5, 5, 2, 5, 1, 3, 4, 5, 1, 2, 3, 3, 5, 2, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 1, 2, 3, 4, 5, 3, 5, 4, 2, 3, 5, 4, 1, 1, 4, 5, 1, 3, 1, 2, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 5, 5, 5, 5, 4, 4, 1, 2, 3, 4, 5, 2, 3, 5, 4, 1, 2, 4, 5, 1, 5, 1, 2, 2, 2, 3, 1, 2, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 1, 1, 4, 1, 2, 4, 5, 3, 4, 3, 3, 1, 3, 4, 5, 5, 1, 2, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 1, 2, 3, 5, 3, 5, 2, 3, 4, 2, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 4, 3, 2, 1, 3, 2, 5, 1, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 5, 2, 3, 4, 2, 3, 4, 5, 2, 4, 4, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 5, 1, 2, 4, 5, 1, 2, 3, 4, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 2, 5, 2, 4, 1, 2, 3, 4, 5, 1, 1, 2, 4, 5, 2, 5, 3, 1, 3, 4, 5, 1, 2, 1, 4, 5, 3, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 1, 3, 4, 5, 4, 2, 1, 5, 3, 1, 5, 1, 2, 3, 4, 5, 1, 3, 5, 4, 2, 3, 4, 5, 4, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 3, 4, 5, 3, 5, 4, 2, 3, 4, 1, 2, 3, 4, 3, 3, 4, 5, 3, 1, 2, 3, 4, 1, 3, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 4, 5, 4, 4, 2, 2, 2, 3, 4, 5, 1, 4, 5, 4, 4, 5, 1, 5, 2, 3, 3, 4, 1, 1, 1, 2, 3, 4, 5, 2, 5, 4, 4, 2, 3, 4, 5, 3, 1, 5, 2, 4, 1, 2, 3, 4, 1, 4, 1, 4, 5, 4, 1, 4, 1, 2, 2, 2, 4, 5, 2, 4, 1, 2, 3, 4, 1, 2, 5, 3, 1, 2, 3, 4, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 4, 2, 5, 3, 1, 3, 4, 5, 3, 5, 1, 2, 3, 4, 1, 3, 5, 1, 3, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 4, 1, 5, 2, 3, 4, 4, 1, 3, 1, 5, 3, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 5, 1, 2, 4, 5, 2, 4, 1, 2, 3, 1, 2, 3, 4, 1, 4, 5, 1, 1, 2, 3, 4, 4, 5, 2, 1, 3, 1, 2, 3], \"Freq\": [0.29704147577285767, 0.70048588514328, 0.6059468388557434, 0.08926895260810852, 0.22181981801986694, 0.08385871350765228, 0.06215779110789299, 0.9323668479919434, 0.9882333874702454, 0.9880569577217102, 0.9927305579185486, 0.9952473044395447, 0.7926216721534729, 0.1998205929994583, 0.1277533322572708, 0.47636836767196655, 0.1320839524269104, 0.2641679048538208, 0.5707623362541199, 0.42161843180656433, 0.00573630491271615, 0.9955270886421204, 0.9910011291503906, 0.8630671501159668, 0.13498206436634064, 0.9973745346069336, 0.5759776830673218, 0.09191133081912994, 0.16135545074939728, 0.1429731845855713, 0.026552163064479828, 0.24639825522899628, 0.4226539433002472, 0.19244243204593658, 0.05755287781357765, 0.08093373477458954, 0.9975841641426086, 0.9929609894752502, 0.9877791404724121, 0.1334318220615387, 0.1724349707365036, 0.22991329431533813, 0.25249406695365906, 0.2114381194114685, 0.6736360788345337, 0.32396239042282104, 0.9865135550498962, 0.3067445158958435, 0.6937145590782166, 0.9898934960365295, 0.9910757541656494, 0.9901867508888245, 0.13177308440208435, 0.22161836922168732, 0.6408963203430176, 0.25870218873023987, 0.7402862906455994, 0.7949740290641785, 0.00841242354363203, 0.19769194722175598, 0.3135945796966553, 0.20084145665168762, 0.23607680201530457, 0.20965029299259186, 0.03875887766480446, 0.9956797361373901, 0.18357329070568085, 0.2579948902130127, 0.329935759305954, 0.12651672959327698, 0.10419024527072906, 0.8289718627929688, 0.11231231689453125, 0.05883026123046875, 0.986912190914154, 0.989928662776947, 0.9944254755973816, 0.9955962300300598, 0.9969664812088013, 0.432464063167572, 0.1016613319516182, 0.12909375131130219, 0.2872335910797119, 0.05002383142709732, 0.1921299248933792, 0.23300863802433014, 0.5723019242286682, 0.9947957992553711, 0.11770566552877426, 0.008407547138631344, 0.6452792882919312, 0.22910566627979279, 0.9236903786659241, 0.07330875843763351, 0.9860922694206238, 0.9960415363311768, 0.9921584725379944, 0.990135133266449, 0.992877721786499, 0.12361537665128708, 0.3685985803604126, 0.21126991510391235, 0.296676903963089, 0.6190566420555115, 0.2666705548763275, 0.1142873764038086, 0.5147152543067932, 0.18851037323474884, 0.1278592050075531, 0.1278592050075531, 0.040980514138936996, 0.9954367876052856, 0.727530300617218, 0.27229204773902893, 0.5303927659988403, 0.1300184577703476, 0.20018716156482697, 0.13827359676361084, 0.3666495382785797, 0.6293238401412964, 0.9964479207992554, 0.9970035552978516, 0.5018840432167053, 0.0030052936635911465, 0.06611645966768265, 0.4267517030239105, 0.9948462247848511, 0.9979942440986633, 0.9894713163375854, 0.07882030308246613, 0.47292181849479675, 0.28661927580833435, 0.16122335195541382, 0.9970369935035706, 0.9969155788421631, 0.0791558176279068, 0.24106544256210327, 0.49652284383773804, 0.0035979915410280228, 0.1834975779056549, 0.2747739255428314, 0.16531114280223846, 0.035742949694395065, 0.29041147232055664, 0.23232917487621307, 0.9818021059036255, 0.9959728717803955, 0.13491696119308472, 0.7655752897262573, 0.0031376036349684, 0.09412810951471329, 0.1922091245651245, 0.8022641539573669, 0.5852176547050476, 0.19971711933612823, 0.20900629460811615, 0.994430422782898, 0.9962325096130371, 0.2913660705089569, 0.20646469295024872, 0.119633749127388, 0.25277453660964966, 0.13121120631694794, 0.9857847690582275, 0.214045450091362, 0.12744691967964172, 0.20424184203147888, 0.3104476034641266, 0.14542019367218018, 0.9901564717292786, 0.45130279660224915, 0.19612692296504974, 0.05904896557331085, 0.19401802122592926, 0.09911790490150452, 0.989033579826355, 0.013013599440455437, 0.9892229437828064, 0.9956910014152527, 0.987796425819397, 0.996765673160553, 0.4395101070404053, 0.5551706552505493, 0.9947699308395386, 0.617978572845459, 0.2714691460132599, 0.11035331338644028, 0.46964970231056213, 0.12651079893112183, 0.2148950695991516, 0.1888996958732605, 0.9940259456634521, 0.39383795857429504, 0.34610000252723694, 0.059672415256500244, 0.20049932599067688, 0.24498605728149414, 0.27516549825668335, 0.3284233510494232, 0.11539198458194733, 0.03550522401928902, 0.9832314848899841, 0.5094961524009705, 0.17781074345111847, 0.3111687898635864, 0.4943349063396454, 0.09087038785219193, 0.3634815514087677, 0.054522231221199036, 0.9959907531738281, 0.9911783933639526, 0.9906242489814758, 0.3973625898361206, 0.1094774454832077, 0.4906211495399475, 0.9901559948921204, 0.21377551555633545, 0.1237647756934166, 0.25203007459640503, 0.19127283990383148, 0.21827605366706848, 0.9917799234390259, 0.00286855548620224, 0.16637621819972992, 0.22374732792377472, 0.487654447555542, 0.11761077493429184, 0.9963173270225525, 0.28219497203826904, 0.2555727958679199, 0.457901269197464, 0.005324433092027903, 0.16572155058383942, 0.5462673306465149, 0.2056174874305725, 0.07979185879230499, 0.1645101010799408, 0.44691911339759827, 0.14531725645065308, 0.24402332305908203, 0.9956855177879333, 0.9893363118171692, 0.08786262571811676, 0.12886518239974976, 0.14350895583629608, 0.515460729598999, 0.12300767749547958, 0.9900516271591187, 0.9962292909622192, 0.9916651248931885, 0.9960684776306152, 0.16240373253822327, 0.058465342968702316, 0.11693068593740463, 0.4222497045993805, 0.2436055988073349, 0.9943165183067322, 0.17424871027469635, 0.2987120747566223, 0.22403405606746674, 0.30120134353637695, 0.31056448817253113, 0.6865109801292419, 0.9861379861831665, 0.1550525277853012, 0.4426862299442291, 0.2584208846092224, 0.14156970381736755, 0.9946497678756714, 0.9960202574729919, 0.8610840439796448, 0.09914400428533554, 0.03855600208044052, 0.9909833073616028, 0.20305179059505463, 0.45369383692741394, 0.2538147270679474, 0.08566247671842575, 0.2316305786371231, 0.21139101684093475, 0.004497681278735399, 0.32383304834365845, 0.2271329015493393, 0.9939035773277283, 0.9826394319534302, 0.5216552019119263, 0.2915802001953125, 0.052393313497304916, 0.13212227821350098, 0.46882885694503784, 0.1504613608121872, 0.05451498553156853, 0.050153784453868866, 0.2769361138343811, 0.4316973090171814, 0.5654088854789734, 0.001910165068693459, 0.3986556828022003, 0.18673871457576752, 0.16575683653354645, 0.24968436360359192, 0.26341256499290466, 0.1380997896194458, 0.39639753103256226, 0.2020348757505417, 0.9949169158935547, 0.997829794883728, 0.924597978591919, 0.0718134343624115, 0.9932112693786621, 0.9287540912628174, 0.06965655833482742, 0.5056056380271912, 0.16309860348701477, 0.0442696213722229, 0.06756941974163055, 0.2190181165933609, 0.2925547957420349, 0.3884340822696686, 0.31713923811912537, 0.9940300583839417, 0.22176583111286163, 0.5248457789421082, 0.2032853364944458, 0.05174535885453224, 0.9924871325492859, 0.9928179383277893, 0.984622597694397, 0.19250909984111786, 0.1485668122768402, 0.43942296504974365, 0.14228934049606323, 0.0774221420288086, 0.33099427819252014, 0.4191288650035858, 0.08617603033781052, 0.09596876055002213, 0.0665905699133873, 0.9869982600212097, 0.6395293474197388, 0.21853265166282654, 0.14140348136425018, 0.9944014549255371, 0.09886861592531204, 0.43502193689346313, 0.17466789484024048, 0.2867189943790436, 0.2152291089296341, 0.2783941626548767, 0.33220145106315613, 0.14972460269927979, 0.023394467309117317, 0.9953026175498962, 0.20190934836864471, 0.15094193816184998, 0.30580446124076843, 0.3038441836833954, 0.039205700159072876, 0.29695266485214233, 0.10456079989671707, 0.08992228657007217, 0.4015134871006012, 0.10456079989671707, 0.9936614632606506, 0.46090081334114075, 0.47040390968322754, 0.068897545337677, 0.26623281836509705, 0.03803325816988945, 0.3321571350097656, 0.08113761991262436, 0.28144609928131104, 0.269342839717865, 0.21812976896762848, 0.22571688890457153, 0.23520079255104065, 0.05121307447552681, 0.09956949204206467, 0.09588173776865005, 0.5789781808853149, 0.2249533087015152, 0.2704150378704071, 0.5297170877456665, 0.01852157711982727, 0.18151146173477173, 0.9935922026634216, 0.9940603375434875, 0.9969654679298401, 0.2330973893404007, 0.35366499423980713, 0.40992987155914307, 0.2376985102891922, 0.20702773332595825, 0.09201232343912125, 0.4638954699039459, 0.9920966625213623, 0.6493597626686096, 0.006429304834455252, 0.3439677953720093, 0.9951752424240112, 0.2834382951259613, 0.1389937698841095, 0.14716987311840057, 0.4306081533432007, 0.5577402114868164, 0.23146218061447144, 0.21194127202033997, 0.190597265958786, 0.18871016800403595, 0.09058088064193726, 0.26230713725090027, 0.2660813331604004, 0.9850781559944153, 0.9958131909370422, 0.9953498244285583, 0.22664955258369446, 0.771833598613739, 0.9866130352020264, 0.9907412528991699, 0.987716555595398, 0.9861475229263306, 0.9904451370239258, 0.2441997081041336, 0.2590899169445038, 0.19357293844223022, 0.3037606179714203, 0.28114745020866394, 0.47721078991889954, 0.2404550462961197, 0.990187406539917, 0.7210685014724731, 0.27469274401664734, 0.9876080751419067, 0.984606146812439, 0.9921536445617676, 0.9953300952911377, 0.9923560619354248, 0.9911520481109619, 0.997058093547821, 0.9933745265007019, 0.27766016125679016, 0.30658310651779175, 0.12340452522039413, 0.13883008062839508, 0.1561838537454605, 0.9889020919799805, 0.9828005433082581, 0.9949672818183899, 0.9860840439796448, 0.984355092048645, 0.49350467324256897, 0.36452049016952515, 0.1402001827955246, 0.9920502305030823, 0.9976876974105835, 0.992323100566864, 0.9253596663475037, 0.07187259197235107, 0.004091328475624323, 0.6177905797958374, 0.171835795044899, 0.20456641912460327, 0.8705901503562927, 0.12854351103305817, 0.5078443288803101, 0.3270021975040436, 0.1635010987520218, 0.9926548600196838, 0.857917845249176, 0.14071668684482574, 0.9941144585609436, 0.9924360513687134, 0.9887564778327942, 0.008888485841453075, 0.9866219758987427, 0.9908359050750732, 0.21748086810112, 0.779931366443634, 0.15344709157943726, 0.28872284293174744, 0.4542841613292694, 0.10297107696533203, 0.31342145800590515, 0.41789528727531433, 0.26902008056640625, 0.9933313727378845, 0.22391431033611298, 0.4444359838962555, 0.1899878978729248, 0.1424909234046936, 0.989051878452301, 0.4119877517223358, 0.3558076024055481, 0.1257365196943283, 0.10700980573892593, 0.5956811308860779, 0.22864528000354767, 0.09025471657514572, 0.03209056705236435, 0.05415283143520355, 0.9916813969612122, 0.21357327699661255, 0.003337082453072071, 0.550618588924408, 0.23025868833065033, 0.9923878908157349, 0.9918868541717529, 0.5254737138748169, 0.11274531483650208, 0.08053236454725266, 0.13086509704589844, 0.1489848792552948, 0.990474283695221, 0.11155492812395096, 0.8831431865692139, 0.9875873327255249, 0.1248326525092125, 0.7151870727539062, 0.08842313289642334, 0.07021836936473846, 0.9969691634178162, 0.9954270720481873, 0.31844210624694824, 0.3207005560398102, 0.20326091349124908, 0.15809182822704315, 0.3274341821670532, 0.6512701511383057, 0.021589066833257675, 0.9945584535598755, 0.9934318661689758, 0.9953104257583618, 0.20126622915267944, 0.2667383849620819, 0.11881982535123825, 0.1673177033662796, 0.24733921885490417, 0.9936975240707397, 0.1933159977197647, 0.10144305229187012, 0.3770619034767151, 0.2507365942001343, 0.07656079530715942, 0.9962872862815857, 0.9972665905952454, 0.9922789931297302, 0.38707518577575684, 0.44453164935112, 0.16632136702537537, 0.996151328086853, 0.9916910529136658, 0.9975748658180237, 0.991287112236023, 0.9948778748512268, 0.9977251887321472, 0.9873828887939453, 0.23596428334712982, 0.5191214084625244, 0.013483673334121704, 0.22922244668006897, 0.3962196409702301, 0.526229202747345, 0.0742911845445633, 0.30095618963241577, 0.35873979330062866, 0.1540895700454712, 0.1540895700454712, 0.03370709344744682, 0.8449899554252625, 0.005709391552954912, 0.005709391552954912, 0.1370254009962082, 0.9969324469566345, 0.9894177317619324, 0.8659030199050903, 0.12519079446792603, 0.29509881138801575, 0.4881540834903717, 0.1434125006198883, 0.013789664022624493, 0.06343245506286621, 0.9933763742446899, 0.992560863494873, 0.996985673904419, 0.9906138181686401, 0.9894723892211914, 0.9966317415237427, 0.9947336912155151, 0.32832518219947815, 0.5286035537719727, 0.12148032337427139, 0.0229827631264925, 0.6047194600105286, 0.09913434088230133, 0.036349259316921234, 0.1321791261434555, 0.12887464463710785, 0.993094265460968, 0.7862299084663391, 0.12414156645536423, 0.015915585681796074, 0.07639481127262115, 0.9928203821182251, 0.9917564392089844, 0.23981358110904694, 0.4991242289543152, 0.26126033067703247, 0.49722567200660706, 0.32003253698349, 0.12114225327968597, 0.06328326463699341, 0.7683302164077759, 0.14373524487018585, 0.08624114841222763, 0.9909411668777466, 0.24780204892158508, 0.029666442424058914, 0.5462116003036499, 0.1762535721063614, 0.9967238306999207, 0.9955499172210693, 0.9956597089767456, 0.822178840637207, 0.17534619569778442, 0.28571778535842896, 0.2834320366382599, 0.4274337887763977], \"Term\": [\"\\u0430\\u043d\\u0442\\u043e\\u043d\", \"\\u0430\\u043d\\u0442\\u043e\\u043d\", \"\\u0431\", \"\\u0431\", \"\\u0431\", \"\\u0431\", \"\\u0431\\u0435\\u043b\\u044b\\u0439\", \"\\u0431\\u0435\\u043b\\u044b\\u0439\", \"\\u0431\\u0435\\u0440\\u0435\\u0442\", \"\\u0431\\u043e\\u043b\\u044c\", \"\\u0431\\u043e\\u043b\\u044c\\u0448\\u0438\\u043c\", \"\\u0431\\u0440\\u043e\\u0441\\u0438\\u043b\", \"\\u0431\\u0443\\u0434\\u0435\\u043c\", \"\\u0431\\u0443\\u0434\\u0435\\u043c\", \"\\u0431\\u0443\\u0434\\u0442\\u043e\", \"\\u0431\\u0443\\u0434\\u0442\\u043e\", \"\\u0431\\u0443\\u0434\\u0442\\u043e\", \"\\u0431\\u0443\\u0434\\u0442\\u043e\", \"\\u0431\\u0443\\u0434\\u0443\", \"\\u0431\\u0443\\u0434\\u0443\", \"\\u0431\\u0443\\u0434\\u0443\", \"\\u0431\\u0443\\u0434\\u0443\\u0442\", \"\\u0431\\u0443\\u0442\\u044b\\u043b\\u043a\\u0443\", \"\\u0431\\u044b\\u0432\\u0430\\u0435\\u0442\", \"\\u0431\\u044b\\u0432\\u0430\\u0435\\u0442\", \"\\u0431\\u044b\\u043b\\u0438\", \"\\u0431\\u044b\\u043b\\u043e\", \"\\u0431\\u044b\\u043b\\u043e\", \"\\u0431\\u044b\\u043b\\u043e\", \"\\u0431\\u044b\\u043b\\u043e\", \"\\u0431\\u044b\\u043b\\u043e\", \"\\u0431\\u044b\\u0442\\u044c\", \"\\u0431\\u044b\\u0442\\u044c\", \"\\u0431\\u044b\\u0442\\u044c\", \"\\u0431\\u044b\\u0442\\u044c\", \"\\u0431\\u044b\\u0442\\u044c\", \"\\u0432\\u0432\\u0435\\u0440\\u0445\", \"\\u0432\\u0434\\u043e\\u043b\\u044c\", \"\\u0432\\u0435\\u0440\\u0438\\u0442\", \"\\u0432\\u0435\\u0441\\u044c\", \"\\u0432\\u0435\\u0441\\u044c\", \"\\u0432\\u0435\\u0441\\u044c\", \"\\u0432\\u0435\\u0441\\u044c\", \"\\u0432\\u0435\\u0441\\u044c\", \"\\u0432\\u0435\\u0442\\u0435\\u0440\", \"\\u0432\\u0435\\u0442\\u0435\\u0440\", \"\\u0432\\u0438\\u0434\", \"\\u0432\\u0438\\u0434\\u0435\\u043b\", \"\\u0432\\u0438\\u0434\\u0435\\u043b\", \"\\u0432\\u043d\\u043e\\u0432\\u044c\", \"\\u0432\\u043e\\u0434\\u0430\", \"\\u0432\\u043e\\u0434\\u043a\\u0443\", \"\\u0432\\u043e\\u0434\\u044b\", \"\\u0432\\u043e\\u0434\\u044b\", \"\\u0432\\u043e\\u0434\\u044b\", \"\\u0432\\u043e\\u043a\\u0440\\u0443\\u0433\", \"\\u0432\\u043e\\u043a\\u0440\\u0443\\u0433\", \"\\u0432\\u043e\\u0441\\u0435\\u043c\\u044c\", \"\\u0432\\u043e\\u0441\\u0435\\u043c\\u044c\", \"\\u0432\\u043e\\u0441\\u0435\\u043c\\u044c\", \"\\u0432\\u0440\\u0435\\u043c\\u044f\", \"\\u0432\\u0440\\u0435\\u043c\\u044f\", \"\\u0432\\u0440\\u0435\\u043c\\u044f\", \"\\u0432\\u0440\\u0435\\u043c\\u044f\", \"\\u0432\\u0440\\u0435\\u043c\\u044f\", \"\\u0432\\u0440\\u043e\\u0434\\u0435\", \"\\u0432\\u0441\\u0435\\u0433\\u0434\\u0430\", \"\\u0432\\u0441\\u0435\\u0433\\u0434\\u0430\", \"\\u0432\\u0441\\u0435\\u0433\\u0434\\u0430\", \"\\u0432\\u0441\\u0435\\u0433\\u0434\\u0430\", \"\\u0432\\u0441\\u0435\\u0433\\u0434\\u0430\", \"\\u0432\\u0441\\u0435\\u0433\\u043e\", \"\\u0432\\u0441\\u0435\\u0433\\u043e\", \"\\u0432\\u0441\\u0435\\u0433\\u043e\", \"\\u0432\\u0441\\u0435\\u0439\", \"\\u0432\\u0441\\u043b\\u0435\\u0434\", \"\\u0432\\u0445\\u043e\\u0434\\u0438\\u0442\", \"\\u0432\\u044b\\u043f\\u0438\\u043b\", \"\\u0432\\u044b\\u0448\\u043b\\u0438\", \"\\u0433\\u043b\\u0435\\u0431\", \"\\u0433\\u043b\\u0435\\u0431\", \"\\u0433\\u043b\\u0435\\u0431\", \"\\u0433\\u043b\\u0435\\u0431\", \"\\u0433\\u043b\\u0435\\u0431\", \"\\u0433\\u043b\\u0435\\u0431\\u0430\", \"\\u0433\\u043b\\u0435\\u0431\\u0430\", \"\\u0433\\u043b\\u0435\\u0431\\u0430\", \"\\u0433\\u043b\\u044f\\u0436\\u0443\", \"\\u0433\\u043e\\u0434\", \"\\u0433\\u043e\\u0434\", \"\\u0433\\u043e\\u0434\", \"\\u0433\\u043e\\u0434\", \"\\u0433\\u043e\\u0434\\u0430\", \"\\u0433\\u043e\\u0434\\u0430\", \"\\u0433\\u043e\\u0434\\u044b\", \"\\u0433\\u043e\\u043b\\u043e\\u0432\\u0443\", \"\\u0433\\u043e\\u043b\\u043e\\u0432\\u044b\", \"\\u0433\\u043e\\u0441\\u043f\\u043e\\u0434\\u044c\", \"\\u0433\\u0440\\u043e\\u0431\\u0443\", \"\\u0434\\u0430\\u0432\\u0430\\u0439\", \"\\u0434\\u0430\\u0432\\u0430\\u0439\", \"\\u0434\\u0430\\u0432\\u0430\\u0439\", \"\\u0434\\u0430\\u0432\\u0430\\u0439\", \"\\u0434\\u0430\\u0432\\u043d\\u043e\", \"\\u0434\\u0430\\u0432\\u043d\\u043e\", \"\\u0434\\u0430\\u0432\\u043d\\u043e\", \"\\u0434\\u0430\\u0436\\u0435\", \"\\u0434\\u0430\\u0436\\u0435\", \"\\u0434\\u0430\\u0436\\u0435\", \"\\u0434\\u0430\\u0436\\u0435\", \"\\u0434\\u0430\\u0436\\u0435\", \"\\u0434\\u0430\\u043b\\u0438\", \"\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0434\\u0432\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0434\\u0432\\u0435\", \"\\u0434\\u0432\\u0435\", \"\\u0434\\u0432\\u0435\", \"\\u0434\\u0432\\u0435\", \"\\u0434\\u0435\\u0434\", \"\\u0434\\u0435\\u0434\", \"\\u0434\\u0435\\u043b\\u0435\", \"\\u0434\\u0435\\u043b\\u043e\", \"\\u0434\\u0435\\u0442\\u0435\\u0439\", \"\\u0434\\u0435\\u0442\\u0435\\u0439\", \"\\u0434\\u0435\\u0442\\u0435\\u0439\", \"\\u0434\\u0435\\u0442\\u0435\\u0439\", \"\\u0434\\u0435\\u0442\\u0441\\u0442\\u0432\\u0430\", \"\\u0434\\u043d\\u0435\\u0439\", \"\\u0434\\u043e\\u0431\\u0440\\u044b\\u0439\", \"\\u0434\\u043e\\u0436\\u0434\\u044c\", \"\\u0434\\u043e\\u0436\\u0434\\u044c\", \"\\u0434\\u043e\\u0436\\u0434\\u044c\", \"\\u0434\\u043e\\u0436\\u0434\\u044c\", \"\\u0434\\u043e\\u0436\\u0434\\u044f\", \"\\u0434\\u043e\\u043a\\u0442\\u043e\\u0440\", \"\\u0434\\u043e\\u043c\", \"\\u0434\\u043e\\u043c\", \"\\u0434\\u043e\\u043c\", \"\\u0434\\u043e\\u043c\", \"\\u0434\\u043e\\u043c\", \"\\u0434\\u043e\\u043c\\u043e\\u0439\", \"\\u0434\\u043e\\u043c\\u043e\\u0439\", \"\\u0434\\u043e\\u043c\\u043e\\u0439\", \"\\u0434\\u043e\\u043c\\u043e\\u0439\", \"\\u0434\\u043e\\u043c\\u043e\\u0439\", \"\\u0434\\u043e\\u043c\\u0443\", \"\\u0434\\u043e\\u0441\\u0442\\u0430\\u043b\", \"\\u0434\\u0440\\u0443\\u0433\", \"\\u0434\\u0440\\u0443\\u0433\", \"\\u0434\\u0440\\u0443\\u0433\", \"\\u0434\\u0440\\u0443\\u0433\", \"\\u0434\\u0440\\u0443\\u0433\\u0438\\u0445\", \"\\u0434\\u0440\\u0443\\u0433\\u0438\\u0445\", \"\\u0434\\u0443\\u043c\\u0430\\u043b\", \"\\u0434\\u0443\\u043c\\u0430\\u043b\", \"\\u0434\\u0443\\u043c\\u0430\\u043b\", \"\\u0434\\u0443\\u0448\\u0430\", \"\\u0434\\u044f\\u0434\\u044f\", \"\\u0435\\u0432\\u0433\\u0435\\u043d\\u0438\\u0439\", \"\\u0435\\u0432\\u0433\\u0435\\u043d\\u0438\\u0439\", \"\\u0435\\u0432\\u0433\\u0435\\u043d\\u0438\\u0439\", \"\\u0435\\u0432\\u0433\\u0435\\u043d\\u0438\\u0439\", \"\\u0435\\u0432\\u0433\\u0435\\u043d\\u0438\\u0439\", \"\\u0435\\u0434\\u0443\", \"\\u0435\\u0439\", \"\\u0435\\u0439\", \"\\u0435\\u0439\", \"\\u0435\\u0439\", \"\\u0435\\u0439\", \"\\u0435\\u043b\", \"\\u0435\\u0449\\u0435\", \"\\u0435\\u0449\\u0435\", \"\\u0435\\u0449\\u0435\", \"\\u0435\\u0449\\u0435\", \"\\u0435\\u0449\\u0435\", \"\\u0436\\u0430\\u043b\\u044c\", \"\\u0436\\u0430\\u043b\\u044c\", \"\\u0436\\u0434\\u0430\\u0442\\u044c\", \"\\u0436\\u0434\\u0443\", \"\\u0436\\u0434\\u0443\\u0442\", \"\\u0436\\u0435\\u043d\\u0438\\u0442\\u044c\\u0441\\u044f\", \"\\u0436\\u0435\\u043d\\u0449\\u0438\\u043d\", \"\\u0436\\u0435\\u043d\\u0449\\u0438\\u043d\", \"\\u0436\\u0438\\u0432\\u0451\\u0442\", \"\\u0436\\u0438\\u0437\\u043d\\u0438\", \"\\u0436\\u0438\\u0437\\u043d\\u0438\", \"\\u0436\\u0438\\u0437\\u043d\\u0438\", \"\\u0436\\u0438\\u0437\\u043d\\u044c\", \"\\u0436\\u0438\\u0437\\u043d\\u044c\", \"\\u0436\\u0438\\u0437\\u043d\\u044c\", \"\\u0436\\u0438\\u0437\\u043d\\u044c\", \"\\u0437\\u0430\\u0442\\u0435\\u043c\", \"\\u0437\\u0430\\u0447\\u0435\\u043c\", \"\\u0437\\u0430\\u0447\\u0435\\u043c\", \"\\u0437\\u0430\\u0447\\u0435\\u043c\", \"\\u0437\\u0430\\u0447\\u0435\\u043c\", \"\\u0437\\u0434\\u0435\\u0441\\u044c\", \"\\u0437\\u0434\\u0435\\u0441\\u044c\", \"\\u0437\\u0434\\u0435\\u0441\\u044c\", \"\\u0437\\u0434\\u0435\\u0441\\u044c\", \"\\u0437\\u0434\\u0435\\u0441\\u044c\", \"\\u0437\\u0438\\u043c\\u043e\\u0439\", \"\\u0437\\u043d\\u0430\\u0447\\u0438\\u0442\", \"\\u0437\\u043d\\u0430\\u0447\\u0438\\u0442\", \"\\u0437\\u043d\\u0430\\u0447\\u0438\\u0442\", \"\\u0437\\u043d\\u0430\\u044e\", \"\\u0437\\u043d\\u0430\\u044e\", \"\\u0437\\u043d\\u0430\\u044e\", \"\\u0437\\u043d\\u0430\\u044e\", \"\\u0437\\u043e\\u0438\", \"\\u0437\\u0440\\u044f\", \"\\u0437\\u0443\\u0445\\u0440\\u044b\", \"\\u0438\\u0434\\u0443\\u0442\", \"\\u0438\\u0434\\u0443\\u0442\", \"\\u0438\\u0434\\u0443\\u0442\", \"\\u0438\\u043b\\u044c\\u044e\", \"\\u0438\\u043c\", \"\\u0438\\u043c\", \"\\u0438\\u043c\", \"\\u0438\\u043c\", \"\\u0438\\u043c\", \"\\u0438\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442\", \"\\u0438\\u0441\\u0443\\u0441\", \"\\u0438\\u0441\\u0443\\u0441\", \"\\u0438\\u0441\\u0443\\u0441\", \"\\u0438\\u0441\\u0443\\u0441\", \"\\u0438\\u0441\\u0443\\u0441\", \"\\u043a\\u0430\", \"\\u043a\\u0430\\u0436\\u0434\\u044b\\u0439\", \"\\u043a\\u0430\\u0436\\u0434\\u044b\\u0439\", \"\\u043a\\u0430\\u0436\\u0434\\u044b\\u0439\", \"\\u043a\\u0430\\u0436\\u0434\\u044b\\u0439\", \"\\u043a\\u0430\\u043a\\u043e\\u0439\", \"\\u043a\\u0430\\u043a\\u043e\\u0439\", \"\\u043a\\u0430\\u043a\\u043e\\u0439\", \"\\u043a\\u0430\\u043a\\u043e\\u0439\", \"\\u043a\\u043e\", \"\\u043a\\u043e\", \"\\u043a\\u043e\", \"\\u043a\\u043e\", \"\\u043a\\u043e\\u043d\\u0441\\u0442\\u0430\\u043d\\u0442\\u0438\\u043d\", \"\\u043a\\u043e\\u0441\\u0442\\u044e\\u043c\\u0435\", \"\\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0439\", \"\\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0439\", \"\\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0439\", \"\\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0439\", \"\\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0439\", \"\\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0445\", \"\\u043a\\u043e\\u0444\\u0435\", \"\\u043a\\u0440\\u0430\\u0441\\u0438\\u0432\\u044b\\u0439\", \"\\u043a\\u0440\\u0438\\u043a\\u043d\\u0443\\u043b\", \"\\u043a\\u0440\\u0438\\u0447\\u0438\\u0442\", \"\\u043a\\u0440\\u0438\\u0447\\u0438\\u0442\", \"\\u043a\\u0440\\u0438\\u0447\\u0438\\u0442\", \"\\u043a\\u0440\\u0438\\u0447\\u0438\\u0442\", \"\\u043a\\u0440\\u0438\\u0447\\u0438\\u0442\", \"\\u043a\\u0440\\u0443\\u0433\\u043e\\u043c\", \"\\u043a\\u0443\\u0434\\u0430\", \"\\u043a\\u0443\\u0434\\u0430\", \"\\u043a\\u0443\\u0434\\u0430\", \"\\u043a\\u0443\\u0434\\u0430\", \"\\u043a\\u0443\\u043f\\u0438\\u043b\", \"\\u043a\\u0443\\u043f\\u0438\\u043b\", \"\\u043b\\u0435\\u0436\\u0430\\u043b\", \"\\u043b\\u0435\\u0436\\u0438\\u0442\", \"\\u043b\\u0435\\u0436\\u0438\\u0442\", \"\\u043b\\u0435\\u0436\\u0438\\u0442\", \"\\u043b\\u0435\\u0436\\u0438\\u0442\", \"\\u043b\\u0435\\u043d\\u0438\\u043d\", \"\\u043b\\u0435\\u0441\", \"\\u043b\\u0435\\u0442\", \"\\u043b\\u0435\\u0442\", \"\\u043b\\u0435\\u0442\", \"\\u043b\\u0435\\u0442\\u044f\\u0442\", \"\\u043b\\u0438\", \"\\u043b\\u0438\", \"\\u043b\\u0438\", \"\\u043b\\u0438\", \"\\u043b\\u0438\\u0446\\u043e\", \"\\u043b\\u0438\\u0446\\u043e\", \"\\u043b\\u0438\\u0446\\u043e\", \"\\u043b\\u0438\\u0446\\u043e\", \"\\u043b\\u0438\\u0446\\u043e\", \"\\u043b\\u0438\\u0446\\u0443\", \"\\u043b\\u044c\", \"\\u043b\\u044e\\u0431\\u0432\\u0438\", \"\\u043b\\u044e\\u0431\\u0432\\u0438\", \"\\u043b\\u044e\\u0431\\u0432\\u0438\", \"\\u043b\\u044e\\u0431\\u0432\\u0438\", \"\\u043b\\u044e\\u0431\\u043b\\u044e\", \"\\u043b\\u044e\\u0431\\u043b\\u044e\", \"\\u043b\\u044e\\u0431\\u043b\\u044e\", \"\\u043b\\u044e\\u0431\\u043b\\u044e\", \"\\u043b\\u044e\\u0431\\u043b\\u044e\", \"\\u043b\\u044e\\u0431\\u043e\\u0432\\u044c\", \"\\u043b\\u044e\\u0431\\u043e\\u0432\\u044c\", \"\\u043b\\u044e\\u0431\\u043e\\u0432\\u044c\", \"\\u043b\\u044e\\u0434\\u0435\\u0439\", \"\\u043b\\u044e\\u0434\\u0435\\u0439\", \"\\u043b\\u044e\\u0434\\u0435\\u0439\", \"\\u043b\\u044e\\u0434\\u0435\\u0439\", \"\\u043c\\u0430\\u043c\\u0430\", \"\\u043c\\u0430\\u043c\\u0430\", \"\\u043c\\u0430\\u043c\\u0430\", \"\\u043c\\u0430\\u043c\\u0430\", \"\\u043c\\u0430\\u043c\\u044b\", \"\\u043c\\u0435\\u0441\\u0442\\u0430\", \"\\u043c\\u0435\\u0441\\u0442\\u0435\", \"\\u043c\\u0435\\u0441\\u0442\\u0435\", \"\\u043c\\u0438\\u0433\", \"\\u043c\\u0438\\u0440\\u0435\", \"\\u043c\\u0438\\u0440\\u0435\", \"\\u043c\\u043d\\u043e\\u0433\\u043e\", \"\\u043c\\u043d\\u043e\\u0433\\u043e\", \"\\u043c\\u043d\\u043e\\u0433\\u043e\", \"\\u043c\\u043d\\u043e\\u0433\\u043e\", \"\\u043c\\u043d\\u043e\\u0433\\u043e\", \"\\u043c\\u043e\\u0436\\u043d\\u043e\", \"\\u043c\\u043e\\u0436\\u043d\\u043e\", \"\\u043c\\u043e\\u0436\\u043d\\u043e\", \"\\u043c\\u043e\\u0437\\u0433\", \"\\u043c\\u043e\\u0440\\u0435\", \"\\u043c\\u043e\\u0440\\u0435\", \"\\u043c\\u043e\\u0440\\u0435\", \"\\u043c\\u043e\\u0440\\u0435\", \"\\u043c\\u043e\\u0441\\u043a\\u0432\\u0435\", \"\\u043c\\u043e\\u044e\", \"\\u043c\\u0443\\u0436\\u0447\\u0438\\u043d\", \"\\u043d\\u0430\\u0434\", \"\\u043d\\u0430\\u0434\", \"\\u043d\\u0430\\u0434\", \"\\u043d\\u0430\\u0434\", \"\\u043d\\u0430\\u0434\", \"\\u043d\\u0430\\u0434\\u043e\", \"\\u043d\\u0430\\u0434\\u043e\", \"\\u043d\\u0430\\u0434\\u043e\", \"\\u043d\\u0430\\u0434\\u043e\", \"\\u043d\\u0430\\u0434\\u043e\", \"\\u043d\\u0430\\u0434\\u043f\\u0438\\u0441\\u044c\", \"\\u043d\\u0430\\u0437\\u0430\\u0434\", \"\\u043d\\u0430\\u0437\\u0430\\u0434\", \"\\u043d\\u0430\\u0437\\u0430\\u0434\", \"\\u043d\\u0430\\u0448\\u0435\\u043b\", \"\\u043d\\u0435\\u0431\\u043e\", \"\\u043d\\u0435\\u0431\\u043e\", \"\\u043d\\u0435\\u0431\\u043e\", \"\\u043d\\u0435\\u0431\\u043e\", \"\\u043d\\u0435\\u0433\\u043e\", \"\\u043d\\u0435\\u0433\\u043e\", \"\\u043d\\u0435\\u0433\\u043e\", \"\\u043d\\u0435\\u0433\\u043e\", \"\\u043d\\u0435\\u0433\\u043e\", \"\\u043d\\u0435\\u0434\\u0430\\u0432\\u043d\\u043e\", \"\\u043d\\u0435\\u0439\", \"\\u043d\\u0435\\u0439\", \"\\u043d\\u0435\\u0439\", \"\\u043d\\u0435\\u0439\", \"\\u043d\\u0435\\u0439\", \"\\u043d\\u0438\\u043a\\u043e\\u043b\\u0430\\u0439\", \"\\u043d\\u0438\\u043a\\u043e\\u043b\\u0430\\u0439\", \"\\u043d\\u0438\\u043a\\u043e\\u043b\\u0430\\u0439\", \"\\u043d\\u0438\\u043a\\u043e\\u043b\\u0430\\u0439\", \"\\u043d\\u0438\\u043a\\u043e\\u043b\\u0430\\u0439\", \"\\u043d\\u0438\\u043a\\u043e\\u043c\\u0443\", \"\\u043d\\u0438\\u043a\\u0442\\u043e\", \"\\u043d\\u0438\\u043a\\u0442\\u043e\", \"\\u043d\\u0438\\u043a\\u0442\\u043e\", \"\\u043d\\u0438\\u043c\", \"\\u043d\\u0438\\u043c\", \"\\u043d\\u0438\\u043c\", \"\\u043d\\u0438\\u043c\", \"\\u043d\\u0438\\u043c\", \"\\u043d\\u0438\\u0445\", \"\\u043d\\u0438\\u0445\", \"\\u043d\\u0438\\u0445\", \"\\u043d\\u0438\\u0445\", \"\\u043d\\u0438\\u0445\", \"\\u043d\\u043e\\u0432\\u044b\\u0439\", \"\\u043d\\u043e\\u0432\\u044b\\u0439\", \"\\u043d\\u043e\\u0432\\u044b\\u0439\", \"\\u043d\\u043e\\u0432\\u044b\\u0439\", \"\\u043d\\u043e\\u0433\\u0438\", \"\\u043d\\u043e\\u0433\\u0438\", \"\\u043d\\u043e\\u0433\\u0438\", \"\\u043d\\u043e\\u0433\\u0438\", \"\\u043d\\u043e\\u0433\\u043e\\u0439\", \"\\u043d\\u043e\\u043c\\u0435\\u0440\", \"\\u043d\\u043e\\u0441\\u043a\\u0438\", \"\\u043d\\u043e\\u0447\\u044c\", \"\\u043d\\u043e\\u0447\\u044c\", \"\\u043d\\u043e\\u0447\\u044c\", \"\\u043e\\u0434\\u043d\\u0430\\u0436\\u0434\\u044b\", \"\\u043e\\u0434\\u043d\\u0430\\u0436\\u0434\\u044b\", \"\\u043e\\u0434\\u043d\\u0430\\u0436\\u0434\\u044b\", \"\\u043e\\u0434\\u043d\\u0430\\u0436\\u0434\\u044b\", \"\\u043e\\u043a\\u043d\\u0435\", \"\\u043e\\u043a\\u043d\\u043e\", \"\\u043e\\u043a\\u043d\\u043e\", \"\\u043e\\u043a\\u043d\\u043e\", \"\\u043e\\u043a\\u043d\\u043e\\u043c\", \"\\u043e\\u043a\\u0441\\u0430\\u043d\\u0435\", \"\\u043e\\u043a\\u0441\\u0430\\u043d\\u0435\", \"\\u043e\\u043a\\u0441\\u0430\\u043d\\u0435\", \"\\u043e\\u043a\\u0441\\u0430\\u043d\\u0435\", \"\\u043e\\u043b\\u0435\\u0433\\u0443\", \"\\u043e\\u043b\\u0435\\u0433\\u0443\", \"\\u043e\\u043b\\u0435\\u0433\\u0443\", \"\\u043e\\u043f\\u044f\\u0442\\u044c\", \"\\u043e\\u043f\\u044f\\u0442\\u044c\", \"\\u043e\\u043f\\u044f\\u0442\\u044c\", \"\\u043e\\u043f\\u044f\\u0442\\u044c\", \"\\u043e\\u043f\\u044f\\u0442\\u044c\", \"\\u043e\\u0440\\u0433\\u0430\\u0437\\u043c\", \"\\u043e\\u0441\\u0435\\u043d\\u044c\", \"\\u043e\\u0441\\u0442\\u0430\\u043b\\u0441\\u044f\", \"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0438\\u043b\", \"\\u043e\\u0442\\u0432\\u0435\\u0442\\u0438\\u043b\", \"\\u043e\\u0442\\u0432\\u0435\\u0447\\u0430\\u0435\\u0442\", \"\\u043e\\u0442\\u043a\\u0443\\u0434\\u0430\", \"\\u043e\\u0447\\u043a\\u0438\", \"\\u043f\\u0430\\u043c\\u044f\\u0442\\u044c\", \"\\u043f\\u0430\\u0440\\u043a\\u0435\", \"\\u043f\\u0435\\u0440\\u0435\\u0434\", \"\\u043f\\u0435\\u0440\\u0435\\u0434\", \"\\u043f\\u0435\\u0440\\u0435\\u0434\", \"\\u043f\\u0435\\u0440\\u0435\\u0434\", \"\\u043f\\u0435\\u0442\\u0440\\u0430\", \"\\u043f\\u0435\\u0442\\u0440\\u0430\", \"\\u043f\\u0435\\u0442\\u0440\\u0430\", \"\\u043f\\u0435\\u0442\\u0440\\u0443\", \"\\u043f\\u0438\\u0448\\u0435\\u0442\", \"\\u043f\\u0438\\u0448\\u0435\\u0442\", \"\\u043f\\u043e\\u0439\\u0434\\u0443\", \"\\u043f\\u043e\\u043a\\u0430\\u0436\\u0443\", \"\\u043f\\u043e\\u043b\\u0435\", \"\\u043f\\u043e\\u043b\\u043d\\u043e\\u0447\\u044c\", \"\\u043f\\u043e\\u043b\\u0443\", \"\\u043f\\u043e\\u043b\\u0447\\u0430\\u0441\\u0430\", \"\\u043f\\u043e\\u043d\\u0438\\u043c\\u0430\\u0435\\u0442\", \"\\u043f\\u043e\\u043f\\u0430\\u043b\", \"\\u043f\\u043e\\u0441\\u043b\\u0435\", \"\\u043f\\u043e\\u0441\\u043b\\u0435\", \"\\u043f\\u043e\\u0441\\u043b\\u0435\", \"\\u043f\\u043e\\u0441\\u043b\\u0435\", \"\\u043f\\u043e\\u0441\\u043b\\u0435\", \"\\u043f\\u043e\\u0441\\u0442\\u0430\\u0432\\u0438\\u043b\", \"\\u043f\\u043e\\u0448\\u043b\\u0438\", \"\\u043f\\u043e\\u0451\\u0442\", \"\\u043f\\u0440\\u0435\\u0434\\u043b\\u043e\\u0436\\u0438\\u043b\", \"\\u043f\\u0440\\u0435\\u0437\\u0438\\u0434\\u0435\\u043d\\u0442\", \"\\u043f\\u0440\\u0438\\u0445\\u043e\\u0434\\u0438\\u0442\", \"\\u043f\\u0440\\u0438\\u0445\\u043e\\u0434\\u0438\\u0442\", \"\\u043f\\u0440\\u0438\\u0445\\u043e\\u0434\\u0438\\u0442\", \"\\u043f\\u0440\\u0438\\u0445\\u043e\\u0434\\u044f\\u0442\", \"\\u043f\\u0440\\u0438\\u0448\\u043b\\u043e\\u0441\\u044c\", \"\\u043f\\u0440\\u0438\\u0448\\u043e\\u043b\", \"\\u043f\\u0443\\u0441\\u0442\\u044c\", \"\\u043f\\u0443\\u0441\\u0442\\u044c\", \"\\u043f\\u0443\\u0442\\u0438\\u043d\", \"\\u043f\\u0443\\u0442\\u0438\\u043d\", \"\\u043f\\u0443\\u0442\\u0438\\u043d\", \"\\u043f\\u0443\\u0442\\u0438\\u043d\", \"\\u043f\\u044f\\u0442\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u043f\\u044f\\u0442\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u043f\\u044f\\u0442\\u044c\", \"\\u043f\\u044f\\u0442\\u044c\", \"\\u043f\\u044f\\u0442\\u044c\", \"\\u0440\\u0430\\u0431\\u043e\\u0442\\u0435\", \"\\u0440\\u0430\\u0431\\u043e\\u0442\\u0443\", \"\\u0440\\u0430\\u0431\\u043e\\u0442\\u0443\", \"\\u0440\\u0430\\u0437\\u043d\\u044b\\u0445\", \"\\u0440\\u0435\\u043a\\u0438\", \"\\u0440\\u0438\\u0441\\u0443\\u0435\\u0442\", \"\\u0440\\u043e\\u0434\\u0438\\u043b\\u0441\\u044f\", \"\\u0440\\u043e\\u0434\\u0438\\u043b\\u0441\\u044f\", \"\\u0440\\u043e\\u044f\\u043b\\u044c\", \"\\u0440\\u0443\\u0431\\u043b\\u0435\\u0439\", \"\\u0440\\u0443\\u0431\\u043b\\u0435\\u0439\", \"\\u0440\\u0443\\u043a\\u0438\", \"\\u0440\\u0443\\u043a\\u0438\", \"\\u0440\\u0443\\u043a\\u0438\", \"\\u0440\\u0443\\u043a\\u0438\", \"\\u0441\\u0430\\u043c\", \"\\u0441\\u0430\\u043c\", \"\\u0441\\u0430\\u043c\", \"\\u0441\\u0430\\u043c\\u043e\\u043c\", \"\\u0441\\u0432\\u0435\\u0442\", \"\\u0441\\u0432\\u0435\\u0442\", \"\\u0441\\u0432\\u0435\\u0442\", \"\\u0441\\u0432\\u0435\\u0442\", \"\\u0441\\u0432\\u043e\\u0438\\u043c\", \"\\u0441\\u0435\\u0439\\u0447\\u0430\\u0441\", \"\\u0441\\u0435\\u0439\\u0447\\u0430\\u0441\", \"\\u0441\\u0435\\u0439\\u0447\\u0430\\u0441\", \"\\u0441\\u0435\\u0439\\u0447\\u0430\\u0441\", \"\\u0441\\u0435\\u043a\\u0441\", \"\\u0441\\u0435\\u043a\\u0441\", \"\\u0441\\u0435\\u043a\\u0441\", \"\\u0441\\u0435\\u043a\\u0441\", \"\\u0441\\u0435\\u043a\\u0441\", \"\\u0441\\u0435\\u043c\\u043d\\u0430\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0441\\u0438\\u0434\\u0438\\u0442\", \"\\u0441\\u0438\\u0434\\u0438\\u0442\", \"\\u0441\\u0438\\u0434\\u0438\\u0442\", \"\\u0441\\u0438\\u0434\\u0438\\u0442\", \"\\u0441\\u0438\\u0441\\u044c\\u043a\\u0438\", \"\\u0441\\u043a\\u0430\\u0436\\u0435\\u0442\", \"\\u0441\\u043a\\u0430\\u0437\\u0430\\u043b\\u0430\", \"\\u0441\\u043a\\u0430\\u0437\\u0430\\u043b\\u0430\", \"\\u0441\\u043a\\u0430\\u0437\\u0430\\u043b\\u0430\", \"\\u0441\\u043a\\u0430\\u0437\\u0430\\u043b\\u0430\", \"\\u0441\\u043a\\u0430\\u0437\\u0430\\u043b\\u0430\", \"\\u0441\\u043a\\u043e\\u0440\\u0435\\u0435\", \"\\u0441\\u043b\\u0443\\u0447\\u0430\\u0439\", \"\\u0441\\u043b\\u0443\\u0447\\u0430\\u0439\", \"\\u0441\\u043b\\u044b\\u0448\\u043d\\u043e\", \"\\u0441\\u043c\\u043e\\u0442\\u0440\\u0438\\u0442\", \"\\u0441\\u043c\\u043e\\u0442\\u0440\\u0438\\u0442\", \"\\u0441\\u043c\\u043e\\u0442\\u0440\\u0438\\u0442\", \"\\u0441\\u043c\\u043e\\u0442\\u0440\\u0438\\u0442\", \"\\u0441\\u043d\\u0435\\u0433\", \"\\u0441\\u043d\\u0435\\u0433\\u0443\", \"\\u0441\\u043e\\u0432\\u0441\\u0435\\u043c\", \"\\u0441\\u043e\\u0432\\u0441\\u0435\\u043c\", \"\\u0441\\u043e\\u0432\\u0441\\u0435\\u043c\", \"\\u0441\\u043e\\u0432\\u0441\\u0435\\u043c\", \"\\u0441\\u043e\\u043b\\u043d\\u0446\\u0435\", \"\\u0441\\u043e\\u043b\\u043d\\u0446\\u0435\", \"\\u0441\\u043e\\u043b\\u043d\\u0446\\u0435\", \"\\u0441\\u043e\\u0440\\u043e\\u043a\", \"\\u0441\\u043f\\u0438\\u043d\\u0435\", \"\\u0441\\u043f\\u0440\\u043e\\u0441\\u0438\\u043b\\u0430\", \"\\u0441\\u0442\\u0430\\u043b\", \"\\u0441\\u0442\\u0430\\u043b\", \"\\u0441\\u0442\\u0430\\u043b\", \"\\u0441\\u0442\\u0430\\u043b\", \"\\u0441\\u0442\\u0430\\u043b\", \"\\u0441\\u0442\\u0430\\u043d\\u043e\\u0432\\u0438\\u0442\\u0441\\u044f\", \"\\u0441\\u0442\\u043e\\u0438\\u0442\", \"\\u0441\\u0442\\u043e\\u0438\\u0442\", \"\\u0441\\u0442\\u043e\\u0438\\u0442\", \"\\u0441\\u0442\\u043e\\u0438\\u0442\", \"\\u0441\\u0442\\u043e\\u0438\\u0442\", \"\\u0441\\u0442\\u043e\\u043b\\u044c\\u043a\\u043e\", \"\\u0441\\u0442\\u0440\\u0430\\u0448\\u043d\\u0435\\u0439\", \"\\u0441\\u044e\\u0434\\u0430\", \"\\u0442\\u0430\\u043a\\u043e\\u0439\", \"\\u0442\\u0430\\u043a\\u043e\\u0439\", \"\\u0442\\u0430\\u043a\\u043e\\u0439\", \"\\u0442\\u0430\\u043a\\u0443\\u044e\", \"\\u0442\\u0430\\u0442\\u044c\\u044f\\u043d\\u0430\", \"\\u0442\\u0432\\u043e\\u0438\\u0445\", \"\\u0442\\u0432\\u043e\\u044e\", \"\\u0442\\u0432\\u043e\\u0451\", \"\\u0442\\u0435\\u043c\\u043d\\u043e\\u0442\\u0435\", \"\\u0442\\u0435\\u043d\\u044c\", \"\\u0442\\u0435\\u0445\", \"\\u0442\\u0435\\u0445\", \"\\u0442\\u0435\\u0445\", \"\\u0442\\u0435\\u0445\", \"\\u0442\\u043e\\u0431\\u043e\\u0439\", \"\\u0442\\u043e\\u0431\\u043e\\u0439\", \"\\u0442\\u043e\\u0431\\u043e\\u0439\", \"\\u0442\\u043e\\u0436\\u0435\", \"\\u0442\\u043e\\u0436\\u0435\", \"\\u0442\\u043e\\u0436\\u0435\", \"\\u0442\\u043e\\u0436\\u0435\", \"\\u0442\\u043e\\u0436\\u0435\", \"\\u0442\\u0440\\u0438\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0442\\u0440\\u0438\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0442\\u0440\\u0438\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0442\\u0440\\u0438\\u0434\\u0446\\u0430\\u0442\\u044c\", \"\\u0442\\u0440\\u0443\\u0441\\u044b\", \"\\u0442\\u0443\\u043c\\u0430\\u043d\", \"\\u0442\\u044b\\u0441\\u044f\\u0447\", \"\\u0442\\u044b\\u0441\\u044f\\u0447\", \"\\u0443\\u043c\\u0435\\u0440\", \"\\u0443\\u043c\\u0435\\u0440\", \"\\u0443\\u043c\\u0435\\u0440\", \"\\u0443\\u043c\\u0435\\u0440\", \"\\u0443\\u043c\\u0435\\u0440\", \"\\u0443\\u043c\\u0438\\u0440\\u0430\\u0442\\u044c\", \"\\u0443\\u043f\\u0430\\u043b\", \"\\u0443\\u0442\\u0440\\u043e\", \"\\u0443\\u0448\\u0451\\u043b\", \"\\u0444\\u043e\\u0442\\u043e\", \"\\u0445\\u0432\\u0430\\u0442\\u0430\\u0435\\u0442\", \"\\u0445\\u043e\\u0434\\u0438\\u0442\\u044c\", \"\\u0445\\u043e\\u0440\\u043e\\u0448\\u043e\", \"\\u0445\\u043e\\u0440\\u043e\\u0448\\u043e\", \"\\u0445\\u043e\\u0440\\u043e\\u0448\\u043e\", \"\\u0445\\u043e\\u0440\\u043e\\u0448\\u043e\", \"\\u0445\\u043e\\u0442\\u0435\\u043b\", \"\\u0445\\u043e\\u0442\\u0435\\u043b\", \"\\u0445\\u043e\\u0442\\u0435\\u043b\", \"\\u0445\\u043e\\u0442\\u0435\\u043b\", \"\\u0445\\u043e\\u0442\\u0435\\u043b\", \"\\u0445\\u043e\\u0442\\u0435\\u043b\\u043e\\u0441\\u044c\", \"\\u0445\\u043e\\u0442\\u044c\", \"\\u0445\\u043e\\u0442\\u044c\", \"\\u0445\\u043e\\u0442\\u044c\", \"\\u0445\\u043e\\u0442\\u044c\", \"\\u0445\\u043e\\u0447\\u0435\\u0442\\u0441\\u044f\", \"\\u0447\\u0430\\u0441\\u043e\\u0432\", \"\\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\", \"\\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\", \"\\u0447\\u0435\\u043b\\u043e\\u0432\\u0435\\u043a\", \"\\u0447\\u0435\\u043c\", \"\\u0447\\u0435\\u043c\", \"\\u0447\\u0435\\u043c\", \"\\u0447\\u0435\\u043c\", \"\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435\", \"\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435\", \"\\u0447\\u0435\\u0442\\u044b\\u0440\\u0435\", \"\\u0447\\u0438\\u0442\\u0430\\u044e\", \"\\u0447\\u0443\\u0442\\u044c\", \"\\u0447\\u0443\\u0442\\u044c\", \"\\u0447\\u0443\\u0442\\u044c\", \"\\u0447\\u0443\\u0442\\u044c\", \"\\u0448\\u043b\\u0438\", \"\\u0448\\u0451\\u043b\", \"\\u044d\\u0442\\u0438\\u043c\", \"\\u044d\\u0442\\u043e\\u043c\", \"\\u044d\\u0442\\u043e\\u043c\", \"\\u044d\\u0442\\u043e\\u0442\", \"\\u044d\\u0442\\u043e\\u0442\", \"\\u044d\\u0442\\u043e\\u0442\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 5, 3, 1, 2]};\n","\n","function LDAvis_load_lib(url, callback){\n","  var s = document.createElement('script');\n","  s.src = url;\n","  s.async = true;\n","  s.onreadystatechange = s.onload = callback;\n","  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n","  document.getElementsByTagName(\"head\")[0].appendChild(s);\n","}\n","\n","if(typeof(LDAvis) !== \"undefined\"){\n","   // already loaded: just create the visualization\n","   !function(LDAvis){\n","       new LDAvis(\"#\" + \"ldavis_el621399806952063121410830656\", ldavis_el621399806952063121410830656_data);\n","   }(LDAvis);\n","}else if(typeof define === \"function\" && define.amd){\n","   // require.js is available: use it to load d3/LDAvis\n","   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n","   require([\"d3\"], function(d3){\n","      window.d3 = d3;\n","      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n","        new LDAvis(\"#\" + \"ldavis_el621399806952063121410830656\", ldavis_el621399806952063121410830656_data);\n","      });\n","    });\n","}else{\n","    // require.js not available: dynamically load d3 & LDAvis\n","    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n","         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n","                 new LDAvis(\"#\" + \"ldavis_el621399806952063121410830656\", ldavis_el621399806952063121410830656_data);\n","            })\n","         });\n","}\n","</script>"],"text/plain":["PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n","topic                                                \n","3      22.560722        1       1  0.084680  0.107589\n","4      20.748171        1       2  0.129541  0.122547\n","2      20.242851        1       3  0.032515 -0.213671\n","0      19.175110        1       4  0.003210 -0.087508\n","1      17.273140        1       5 -0.249947  0.071043, topic_info=      Category        Freq     Term       Total  loglift  logprob\n","term                                                             \n","687    Default  544.000000      лет  544.000000  30.0000  30.0000\n","488    Default  295.000000     снег  295.000000  29.0000  29.0000\n","442    Default  523.000000   любовь  523.000000  28.0000  28.0000\n","4213   Default  475.000000      год  475.000000  27.0000  27.0000\n","49     Default  305.000000    купил  305.000000  26.0000  26.0000\n","5356   Default  193.000000     кофе  193.000000  25.0000  25.0000\n","187    Default  373.000000    всего  373.000000  24.0000  24.0000\n","358    Default  216.000000    сорок  216.000000  23.0000  23.0000\n","25     Default  311.000000     окно  311.000000  22.0000  22.0000\n","513    Default  382.000000   четыре  382.000000  21.0000  21.0000\n","1089   Default  171.000000    затем  171.000000  20.0000  20.0000\n","1963   Default  222.000000    пусть  222.000000  19.0000  19.0000\n","4439   Default  573.000000     чуть  573.000000  18.0000  18.0000\n","3840   Default  244.000000   бывает  244.000000  17.0000  17.0000\n","1039   Default  348.000000     буду  348.000000  16.0000  16.0000\n","848    Default  262.000000    пишет  262.000000  15.0000  15.0000\n","11289  Default  169.000000    осень  169.000000  14.0000  14.0000\n","752    Default  453.000000    жизни  453.000000  13.0000  13.0000\n","147    Default  420.000000    никто  420.000000  12.0000  12.0000\n","1296   Default  318.000000     друг  318.000000  11.0000  11.0000\n","82     Default  512.000000  человек  512.000000  10.0000  10.0000\n","260    Default  384.000000  смотрит  384.000000   9.0000   9.0000\n","588    Default  256.000000     этом  256.000000   8.0000   8.0000\n","791    Default  251.000000   вокруг  251.000000   7.0000   7.0000\n","3451   Default  204.000000     года  204.000000   6.0000   6.0000\n","2606   Default  153.000000   доктор  153.000000   5.0000   5.0000\n","777    Default  314.000000     хоть  314.000000   4.0000   4.0000\n","1000   Default  332.000000    детей  332.000000   3.0000   3.0000\n","932    Default  136.000000   пришол  136.000000   2.0000   2.0000\n","4005   Default  277.000000   солнце  277.000000   1.0000   1.0000\n","...        ...         ...      ...         ...      ...      ...\n","1061    Topic5   67.652985  детства   68.352272   1.7457  -7.2418\n","555     Topic5   66.458794    пошли   67.155029   1.7456  -7.2596\n","6734    Topic5   66.419060    зимой   67.125595   1.7454  -7.2602\n","8858    Topic5   65.307137   покажу   66.016243   1.7452  -7.2771\n","6008    Topic5   65.490761     дому   66.204788   1.7452  -7.2743\n","5068    Topic5   63.417198       ль   64.113037   1.7451  -7.3065\n","2662    Topic5   64.587769    снегу   65.298607   1.7451  -7.2882\n","49      Topic5  209.946136    купил  305.894592   1.3796  -6.1093\n","2759    Topic5   94.764824   случай  107.570328   1.6293  -6.9048\n","7503    Topic5   96.211990   других  119.661331   1.5379  -6.8896\n","6739    Topic5  140.108017    глеба  244.626129   1.1987  -6.5138\n","2865    Topic5  107.390327     воды  166.953674   1.3148  -6.7797\n","1000    Topic5  142.089081    детей  332.746185   0.9051  -6.4997\n","1803    Topic5  129.275467    можно  406.761414   0.6097  -6.5942\n","1388    Topic5  141.171036    опять  529.913147   0.4333  -6.5062\n","957     Topic5  131.940674    давай  444.928467   0.5405  -6.5738\n","3755    Topic5   95.892929   женщин  172.919800   1.1664  -6.8930\n","366     Topic5  126.881783    люблю  458.589508   0.4711  -6.6129\n","493     Topic5  121.026543     куда  401.724640   0.5563  -6.6602\n","151     Topic5  121.890923    будто  461.827484   0.4240  -6.6531\n","25      Topic5  107.009956     окно  311.075623   0.6889  -6.7833\n","131     Topic5  111.304306      ним  394.391663   0.4909  -6.7439\n","4213    Topic5  109.476540      год  475.763000   0.2868  -6.7605\n","18      Topic5  101.544434    перед  335.790741   0.5600  -6.8357\n","2256    Topic5  102.547989      сам  382.871033   0.4387  -6.8259\n","1472    Topic5  104.398094    домой  447.640717   0.3002  -6.8080\n","359     Topic5  101.729912     стал  412.389099   0.3564  -6.8339\n","610     Topic5  102.922813     весь  487.140167   0.2014  -6.8222\n","1654    Topic5  101.355530     лицо  444.673584   0.2773  -6.8376\n","1511    Topic5   97.492821       им  444.391388   0.2391  -6.8764\n","\n","[335 rows x 6 columns], token_table=      Topic      Freq      Term\n","term                           \n","4116      2  0.297041     антон\n","4116      4  0.700486     антон\n","851       1  0.605947         б\n","851       2  0.089269         б\n","851       3  0.221820         б\n","851       4  0.083859         б\n","5983      1  0.062158     белый\n","5983      2  0.932367     белый\n","1197      3  0.988233     берет\n","8331      3  0.988057      боль\n","954       5  0.992731   большим\n","1284      5  0.995247    бросил\n","4109      2  0.792622     будем\n","4109      5  0.199821     будем\n","151       1  0.127753     будто\n","151       3  0.476368     будто\n","151       4  0.132084     будто\n","151       5  0.264168     будто\n","1039      1  0.570762      буду\n","1039      2  0.421618      буду\n","1039      3  0.005736      буду\n","8883      3  0.995527     будут\n","43        5  0.991001   бутылку\n","3840      2  0.863067    бывает\n","3840      5  0.134982    бывает\n","789       1  0.997375      были\n","673       1  0.575978      было\n","673       2  0.091911      было\n","673       3  0.161355      было\n","673       4  0.142973      было\n","...     ...       ...       ...\n","8380      5  0.993094  хотелось\n","777       1  0.786230      хоть\n","777       2  0.124142      хоть\n","777       4  0.015916      хоть\n","777       5  0.076395      хоть\n","74        2  0.992820   хочется\n","7391      4  0.991756     часов\n","82        1  0.239814   человек\n","82        2  0.499124   человек\n","82        3  0.261260   человек\n","279       1  0.497226       чем\n","279       2  0.320033       чем\n","279       3  0.121142       чем\n","279       4  0.063283       чем\n","513       1  0.768330    четыре\n","513       4  0.143735    четыре\n","513       5  0.086241    четыре\n","1459      1  0.990941     читаю\n","4439      1  0.247802      чуть\n","4439      2  0.029666      чуть\n","4439      3  0.546212      чуть\n","4439      4  0.176254      чуть\n","5139      4  0.996724       шли\n","5982      5  0.995550       шёл\n","8724      2  0.995660      этим\n","588       1  0.822179      этом\n","588       3  0.175346      этом\n","362       1  0.285718      этот\n","362       2  0.283432      этот\n","362       3  0.427434      этот\n","\n","[627 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 5, 3, 1, 2])"]},"metadata":{"tags":[]},"execution_count":68}]},{"metadata":{"id":"mHx1GJrWPkM8","colab_type":"text"},"cell_type":"markdown","source":["Предсказывает распределение модель как-то так:"]},{"metadata":{"id":"rTD0CGMdPsF5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"5e0bc66d-13bd-4189-9daf-adaa633358d5","executionInfo":{"status":"ok","timestamp":1542225614390,"user_tz":-180,"elapsed":843,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["for word in perashki[10]:\n","    if word == '\\\\n':\n","        print()\n","    else:\n","        print(word, end=' ')"],"execution_count":69,"outputs":[{"output_type":"stream","text":["олег ваквариуме шарит \n","глазами полными беды \n","а гдеж моя уитни хьюстон \n","у грота нет в кормушке нет \n"],"name":"stdout"}]},{"metadata":{"id":"0m0b6i2MPlKD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"477c9154-5d30-4e33-cc9e-7f6c16a26a02","executionInfo":{"status":"ok","timestamp":1542225616101,"user_tz":-180,"elapsed":415,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["lda_model.get_document_topics(bow_corpus[10])"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 0.7048466),\n"," (1, 0.016812785),\n"," (2, 0.24477454),\n"," (3, 0.016765906),\n"," (4, 0.016800195)]"]},"metadata":{"tags":[]},"execution_count":70}]},{"metadata":{"id":"-imKaGGUQM5K","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Посчитайте для всех текстов вектора тем, передавайте их вместе со словами (конкатенируя к эмбеддингам). Посмотрите, вдруг чего получится."]},{"metadata":{"id":"05S7awO4L3vC","colab_type":"code","colab":{}},"cell_type":"code","source":["themes = []\n","for text in bow_corpus:\n","    topics = lda_model.get_document_topics(text)\n","    \n","    # According to gensim docs it is possible that less than num_topics\n","    # themes are extracted. That's why I decided to fill blanks with\n","    # uniform-distributed residue\n","    if len(topics) != 5:\n","        total = 0\n","        for topic in topics:\n","            total += topic[1]\n","        additional = (1 - total) / (5 - len(topics))\n","        candidates = set(range(5))\n","        for topic in topics:\n","            candidates.remove(topic[0])\n","        for candidate in candidates:\n","            topics.append((candidate, additional))\n","        \n","    themes.append(np.array(topics)[:, 1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gGnhOEe5MeyG","colab_type":"code","colab":{}},"cell_type":"code","source":["from torchtext.data import Field, Example, Dataset, BucketIterator\n","\n","text_field = Field(init_token='<s>', eos_token='</s>')\n","theme_field = Field(sequential=False, use_vocab=False, dtype=torch.float64)\n","        \n","fields = [('text', text_field), ('theme', theme_field)]\n","examples = [Example.fromlist([poem, theme], fields) for poem, theme in zip(perashki, themes)]\n","dataset = Dataset(examples, fields)\n","\n","text_field.build_vocab(dataset, min_freq=7)\n","\n","train_dataset, test_dataset = dataset.split(split_ratio=0.9)\n","\n","train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n","                                              shuffle=True, device=DEVICE, sort=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iPMT0DNuQwE7","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):                \n","                logits, _ = model(batch.text, batch.theme)\n","\n","                targets = torch.cat((batch.text[1:], batch.text.new_ones((1, batch.text.shape[1])))).view(-1)\n","\n","                loss = criterion(logits.view(-1, logits.shape[-1]), targets)\n","                \n","                mask = (1 - ((targets == unk_idx) + (targets == pad_idx))).float()\n","                \n","                loss = (loss * mask).sum() / mask.sum()\n","                \n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n","            \n","            if best_val_loss and val_loss > best_val_loss:\n","                optimizer.param_groups[0]['lr'] /= 4.\n","                print('Optimizer lr = {:g}'.format(optimizer.param_groups[0]['lr']))\n","            else:\n","                best_val_loss = val_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-SAkkWU5QD64","colab_type":"code","colab":{}},"cell_type":"code","source":["class LargeModel(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=512, lstm_hidden_dim=512, num_layers=1):\n","        super().__init__()\n","\n","        self._dropout = nn.Dropout(0.4)\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.LSTM(input_size=emb_dim+5, hidden_size=lstm_hidden_dim, num_layers=num_layers)\n","        \n","        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n","        \n","        self._init_weights()\n","\n","    def _init_weights(self, init_range=0.1):\n","        self._emb.weight.data.uniform_(-init_range, init_range)\n","        self._out_layer.bias.data.zero_()\n","        self._out_layer.weight.data.uniform_(-init_range, init_range)\n","\n","    def forward(self, inputs, theme, hidden=None):\n","        embs = self._emb(inputs)\n","        theme = theme.unsqueeze(0).expand(embs.shape[0], embs.shape[1], theme.shape[1]).float()\n","        embs = torch.cat((embs, theme), -1)\n","        output, hidden = self._rnn(embs, hidden)\n","        return self._out_layer(self._dropout(output)), hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XSbYk-wKR1Tx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1411},"outputId":"e6652136-ba34-4301-8ab5-542da5b6cf3c","executionInfo":{"status":"ok","timestamp":1542227936245,"user_tz":-180,"elapsed":2265966,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["model = LargeModel(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n","\n","pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n","unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n","criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"],"execution_count":75,"outputs":[{"output_type":"stream","text":["[1 / 30] Train: Loss = 5.24728, PPX = 190.05: 100%|██████████| 1386/1386 [01:13<00:00, 18.97it/s]\n","[1 / 30]   Val: Loss = 4.80780, PPX = 122.46: 100%|██████████| 39/39 [00:02<00:00, 19.15it/s]\n","[2 / 30] Train: Loss = 4.64168, PPX = 103.72: 100%|██████████| 1386/1386 [01:13<00:00, 18.91it/s]\n","[2 / 30]   Val: Loss = 4.52490, PPX = 92.29: 100%|██████████| 39/39 [00:01<00:00, 19.60it/s]\n","[3 / 30] Train: Loss = 4.32576, PPX = 75.62: 100%|██████████| 1386/1386 [01:13<00:00, 18.86it/s]\n","[3 / 30]   Val: Loss = 4.36822, PPX = 78.90: 100%|██████████| 39/39 [00:02<00:00, 20.31it/s]\n","[4 / 30] Train: Loss = 4.04956, PPX = 57.37: 100%|██████████| 1386/1386 [01:13<00:00, 18.75it/s]\n","[4 / 30]   Val: Loss = 4.27456, PPX = 71.85: 100%|██████████| 39/39 [00:01<00:00, 19.79it/s]\n","[5 / 30] Train: Loss = 3.79195, PPX = 44.34: 100%|██████████| 1386/1386 [01:13<00:00, 18.77it/s]\n","[5 / 30]   Val: Loss = 4.22186, PPX = 68.16: 100%|██████████| 39/39 [00:01<00:00, 19.78it/s]\n","[6 / 30] Train: Loss = 3.55639, PPX = 35.04: 100%|██████████| 1386/1386 [01:13<00:00, 18.53it/s]\n","[6 / 30]   Val: Loss = 4.19060, PPX = 66.06: 100%|██████████| 39/39 [00:01<00:00, 19.59it/s]\n","[7 / 30] Train: Loss = 3.34085, PPX = 28.24: 100%|██████████| 1386/1386 [01:13<00:00, 18.77it/s]\n","[7 / 30]   Val: Loss = 4.18357, PPX = 65.60: 100%|██████████| 39/39 [00:01<00:00, 19.72it/s]\n","[8 / 30] Train: Loss = 3.14498, PPX = 23.22: 100%|██████████| 1386/1386 [01:13<00:00, 18.83it/s]\n","[8 / 30]   Val: Loss = 4.19106, PPX = 66.09: 100%|██████████| 39/39 [00:01<00:00, 19.79it/s]\n","[9 / 30] Train: Loss = 2.91736, PPX = 18.49:   0%|          | 2/1386 [00:00<02:46,  8.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 0.00025\n"],"name":"stdout"},{"output_type":"stream","text":["[9 / 30] Train: Loss = 2.79897, PPX = 16.43: 100%|██████████| 1386/1386 [01:13<00:00, 18.74it/s]\n","[9 / 30]   Val: Loss = 4.17976, PPX = 65.35: 100%|██████████| 39/39 [00:02<00:00, 19.51it/s]\n","[10 / 30] Train: Loss = 2.69916, PPX = 14.87: 100%|██████████| 1386/1386 [01:13<00:00, 18.82it/s]\n","[10 / 30]   Val: Loss = 4.18817, PPX = 65.90: 100%|██████████| 39/39 [00:01<00:00, 19.74it/s]\n","[11 / 30] Train: Loss = 2.68224, PPX = 14.62:   0%|          | 2/1386 [00:00<02:52,  8.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 6.25e-05\n"],"name":"stdout"},{"output_type":"stream","text":["[11 / 30] Train: Loss = 2.58838, PPX = 13.31: 100%|██████████| 1386/1386 [01:13<00:00, 19.04it/s]\n","[11 / 30]   Val: Loss = 4.18894, PPX = 65.95: 100%|██████████| 39/39 [00:01<00:00, 19.75it/s]\n","[12 / 30] Train: Loss = 2.57315, PPX = 13.11:   0%|          | 2/1386 [00:00<02:54,  7.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.5625e-05\n"],"name":"stdout"},{"output_type":"stream","text":["[12 / 30] Train: Loss = 2.55567, PPX = 12.88: 100%|██████████| 1386/1386 [01:13<00:00, 18.87it/s]\n","[12 / 30]   Val: Loss = 4.19509, PPX = 66.36: 100%|██████████| 39/39 [00:02<00:00, 19.36it/s]\n","[13 / 30] Train: Loss = 2.51445, PPX = 12.36:   0%|          | 2/1386 [00:00<02:42,  8.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 3.90625e-06\n"],"name":"stdout"},{"output_type":"stream","text":["[13 / 30] Train: Loss = 2.54769, PPX = 12.78: 100%|██████████| 1386/1386 [01:13<00:00, 18.82it/s]\n","[13 / 30]   Val: Loss = 4.19795, PPX = 66.55: 100%|██████████| 39/39 [00:01<00:00, 20.27it/s]\n","[14 / 30] Train: Loss = 2.60935, PPX = 13.59:   0%|          | 2/1386 [00:00<02:57,  7.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 9.76563e-07\n"],"name":"stdout"},{"output_type":"stream","text":["[14 / 30] Train: Loss = 2.54340, PPX = 12.72: 100%|██████████| 1386/1386 [01:13<00:00, 19.12it/s]\n","[14 / 30]   Val: Loss = 4.19573, PPX = 66.40: 100%|██████████| 39/39 [00:01<00:00, 19.94it/s]\n","[15 / 30] Train: Loss = 2.41940, PPX = 11.24:   0%|          | 2/1386 [00:00<02:50,  8.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.44141e-07\n"],"name":"stdout"},{"output_type":"stream","text":["[15 / 30] Train: Loss = 2.54476, PPX = 12.74: 100%|██████████| 1386/1386 [01:13<00:00, 18.71it/s]\n","[15 / 30]   Val: Loss = 4.20060, PPX = 66.73: 100%|██████████| 39/39 [00:01<00:00, 19.75it/s]\n","[16 / 30] Train: Loss = 2.37987, PPX = 10.80:   0%|          | 2/1386 [00:00<02:52,  8.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 6.10352e-08\n"],"name":"stdout"},{"output_type":"stream","text":["[16 / 30] Train: Loss = 2.54534, PPX = 12.75: 100%|██████████| 1386/1386 [01:13<00:00, 18.80it/s]\n","[16 / 30]   Val: Loss = 4.19783, PPX = 66.54: 100%|██████████| 39/39 [00:01<00:00, 19.52it/s]\n","[17 / 30] Train: Loss = 2.59876, PPX = 13.45:   0%|          | 2/1386 [00:00<02:52,  8.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.52588e-08\n"],"name":"stdout"},{"output_type":"stream","text":["[17 / 30] Train: Loss = 2.54489, PPX = 12.74: 100%|██████████| 1386/1386 [01:13<00:00, 18.83it/s]\n","[17 / 30]   Val: Loss = 4.19899, PPX = 66.62: 100%|██████████| 39/39 [00:01<00:00, 19.59it/s]\n","[18 / 30] Train: Loss = 2.59403, PPX = 13.38:   0%|          | 2/1386 [00:00<02:47,  8.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 3.8147e-09\n"],"name":"stdout"},{"output_type":"stream","text":["[18 / 30] Train: Loss = 2.54438, PPX = 12.74: 100%|██████████| 1386/1386 [01:13<00:00, 18.87it/s]\n","[18 / 30]   Val: Loss = 4.19775, PPX = 66.54: 100%|██████████| 39/39 [00:02<00:00, 19.32it/s]\n","[19 / 30] Train: Loss = 2.54272, PPX = 12.71:   0%|          | 2/1386 [00:00<02:44,  8.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 9.53674e-10\n"],"name":"stdout"},{"output_type":"stream","text":["[19 / 30] Train: Loss = 2.54454, PPX = 12.74: 100%|██████████| 1386/1386 [01:13<00:00, 18.91it/s]\n","[19 / 30]   Val: Loss = 4.19739, PPX = 66.51: 100%|██████████| 39/39 [00:01<00:00, 19.23it/s]\n","[20 / 30] Train: Loss = 2.60703, PPX = 13.56:   0%|          | 2/1386 [00:00<02:42,  8.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.38419e-10\n"],"name":"stdout"},{"output_type":"stream","text":["[20 / 30] Train: Loss = 2.54347, PPX = 12.72: 100%|██████████| 1386/1386 [01:13<00:00, 18.69it/s]\n","[20 / 30]   Val: Loss = 4.19501, PPX = 66.35: 100%|██████████| 39/39 [00:02<00:00, 19.21it/s]\n","[21 / 30] Train: Loss = 2.82650, PPX = 16.89:   0%|          | 1/1386 [00:00<03:01,  7.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5.96046e-11\n"],"name":"stdout"},{"output_type":"stream","text":["[21 / 30] Train: Loss = 2.54402, PPX = 12.73: 100%|██████████| 1386/1386 [01:13<00:00, 18.82it/s]\n","[21 / 30]   Val: Loss = 4.19821, PPX = 66.57: 100%|██████████| 39/39 [00:01<00:00, 19.68it/s]\n","[22 / 30] Train: Loss = 2.46574, PPX = 11.77:   0%|          | 2/1386 [00:00<02:50,  8.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.49012e-11\n"],"name":"stdout"},{"output_type":"stream","text":["[22 / 30] Train: Loss = 2.54385, PPX = 12.73: 100%|██████████| 1386/1386 [01:13<00:00, 18.76it/s]\n","[22 / 30]   Val: Loss = 4.19683, PPX = 66.48: 100%|██████████| 39/39 [00:02<00:00, 19.39it/s]\n","[23 / 30] Train: Loss = 2.48355, PPX = 11.98:   0%|          | 2/1386 [00:00<02:46,  8.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 3.72529e-12\n"],"name":"stdout"},{"output_type":"stream","text":["[23 / 30] Train: Loss = 2.54285, PPX = 12.72: 100%|██████████| 1386/1386 [01:13<00:00, 18.86it/s]\n","[23 / 30]   Val: Loss = 4.20000, PPX = 66.69: 100%|██████████| 39/39 [00:02<00:00, 19.27it/s]\n","[24 / 30] Train: Loss = 2.30508, PPX = 10.02:   0%|          | 2/1386 [00:00<02:48,  8.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 9.31323e-13\n"],"name":"stdout"},{"output_type":"stream","text":["[24 / 30] Train: Loss = 2.54440, PPX = 12.74: 100%|██████████| 1386/1386 [01:13<00:00, 18.86it/s]\n","[24 / 30]   Val: Loss = 4.19942, PPX = 66.65: 100%|██████████| 39/39 [00:01<00:00, 20.15it/s]\n","[25 / 30] Train: Loss = 2.61798, PPX = 13.71:   0%|          | 2/1386 [00:00<02:47,  8.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.32831e-13\n"],"name":"stdout"},{"output_type":"stream","text":["[25 / 30] Train: Loss = 2.54378, PPX = 12.73: 100%|██████████| 1386/1386 [01:13<00:00, 18.84it/s]\n","[25 / 30]   Val: Loss = 4.19747, PPX = 66.52: 100%|██████████| 39/39 [00:01<00:00, 20.22it/s]\n","[26 / 30] Train: Loss = 2.62729, PPX = 13.84:   0%|          | 2/1386 [00:00<02:48,  8.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5.82077e-14\n"],"name":"stdout"},{"output_type":"stream","text":["[26 / 30] Train: Loss = 2.54439, PPX = 12.74: 100%|██████████| 1386/1386 [01:13<00:00, 18.57it/s]\n","[26 / 30]   Val: Loss = 4.19647, PPX = 66.45: 100%|██████████| 39/39 [00:02<00:00, 19.30it/s]\n","[27 / 30] Train: Loss = 2.50832, PPX = 12.28:   0%|          | 2/1386 [00:00<02:54,  7.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 1.45519e-14\n"],"name":"stdout"},{"output_type":"stream","text":["[27 / 30] Train: Loss = 2.54468, PPX = 12.74: 100%|██████████| 1386/1386 [01:13<00:00, 18.78it/s]\n","[27 / 30]   Val: Loss = 4.19639, PPX = 66.45: 100%|██████████| 39/39 [00:01<00:00, 19.90it/s]\n","[28 / 30] Train: Loss = 2.58302, PPX = 13.24:   0%|          | 2/1386 [00:00<02:50,  8.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 3.63798e-15\n"],"name":"stdout"},{"output_type":"stream","text":["[28 / 30] Train: Loss = 2.54354, PPX = 12.72: 100%|██████████| 1386/1386 [01:13<00:00, 18.80it/s]\n","[28 / 30]   Val: Loss = 4.20063, PPX = 66.73: 100%|██████████| 39/39 [00:02<00:00, 19.25it/s]\n","[29 / 30] Train: Loss = 2.58874, PPX = 13.31:   0%|          | 2/1386 [00:00<02:47,  8.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 9.09495e-16\n"],"name":"stdout"},{"output_type":"stream","text":["[29 / 30] Train: Loss = 2.54404, PPX = 12.73: 100%|██████████| 1386/1386 [01:13<00:00, 18.83it/s]\n","[29 / 30]   Val: Loss = 4.19568, PPX = 66.40: 100%|██████████| 39/39 [00:01<00:00, 19.70it/s]\n","[30 / 30] Train: Loss = 2.63248, PPX = 13.91:   0%|          | 2/1386 [00:00<02:47,  8.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 2.27374e-16\n"],"name":"stdout"},{"output_type":"stream","text":["[30 / 30] Train: Loss = 2.54349, PPX = 12.72: 100%|██████████| 1386/1386 [01:13<00:00, 19.19it/s]\n","[30 / 30]   Val: Loss = 4.19735, PPX = 66.51: 100%|██████████| 39/39 [00:01<00:00, 19.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["Optimizer lr = 5.68434e-17\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"metadata":{"id":"13JLHdG0YCid","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate(model, theme, temp=0.6):\n","    model.eval()\n","    with torch.no_grad():        \n","        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n","        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n","        \n","        hidden = None\n","        for _ in range(150):\n","            probs, hidden = model(LongTensor([[prev_token]]), LongTensor([theme]), hidden)\n","            prev_token = sample(probs, temp)\n","            \n","            if prev_token == end_token:\n","                return\n","            \n","            if train_iter.dataset.fields['text'].vocab.itos[prev_token] == '\\\\n':\n","                print()\n","            else:\n","                print(train_iter.dataset.fields['text'].vocab.itos[prev_token], end=' ')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MH5OTTS4YZUD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"97ede890-8ee9-463b-cf96-827946666b93","executionInfo":{"status":"ok","timestamp":1542228130459,"user_tz":-180,"elapsed":371,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["generate(model, [1, 0, 0, 0, 0])"],"execution_count":77,"outputs":[{"output_type":"stream","text":["я выпил всех на каждом доме \n","и год идет один в тюрьме \n","и там в москве на шпильках в \n","в пустом вагоне за стеклом \n"],"name":"stdout"}]},{"metadata":{"id":"7sjYJv_LYyA8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"5b7c8c0e-2cfc-4381-8628-01b0a818f0c6","executionInfo":{"status":"ok","timestamp":1542228132007,"user_tz":-180,"elapsed":389,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}}},"cell_type":"code","source":["generate(model, [0.3, 0, 0, 0.7, 0])"],"execution_count":78,"outputs":[{"output_type":"stream","text":["оксана говорит что это \n","а не поможет никогда \n","и только я к тебе бы больше \n","послать не приходи ко мне \n"],"name":"stdout"}]},{"metadata":{"id":"w8V0KAz_CNf0","colab_type":"text"},"cell_type":"markdown","source":["# Дополнительные материалы\n","\n","## Статьи\n","\n","Regularizing and Optimizing LSTM Language Models, 2017 [[arxiv]](https://arxiv.org/abs/1708.02182), [[github]](https://github.com/salesforce/awd-lstm-lm) - одна из самых полезных статей про языковые модели + репозиторий, в котором реализовано много полезного, стоит заглянуть\n","\n","Exploring the Limits of Language Modeling, 2016 [[arxiv]](https://arxiv.org/abs/1602.02410)\n","\n","Using the Output Embedding to Improve Language Models, 2017 [[pdf]](http://www.aclweb.org/anthology/E17-2025)\n","\n","\n","## Transfer learning\n","[Transfer learning, cs231n](http://cs231n.github.io/transfer-learning/)  \n","[Transfer learning, Ruder](http://ruder.io/transfer-learning/) - очень подробная статья от чувака из NLP\n","\n","## Multi-task learning\n","[An Overview of Multi-Task Learning in Deep Neural Networks, Ruder](http://ruder.io/multi-task/)  \n","[Multi-Task Learning Objectives for Natural Language Processing, Ruder](http://ruder.io/multi-task-learning-nlp/)"]},{"metadata":{"id":"Vwb5e5hPQebd","colab_type":"text"},"cell_type":"markdown","source":["# Сдача\n","\n","[Форма для сдачи](https://goo.gl/forms/ASLLjYncKUcIHmuO2)  \n","[Feedback](https://goo.gl/forms/9aizSzOUrx7EvGlG3)"]}]}