{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week 13 - Dialogue Systems (Part 2).ipynb","version":"0.3.2","provenance":[{"file_id":"1EplUxEMZdzOSaSHjGtq4MQCtyIg0wUDl","timestamp":1545166842894},{"file_id":"19kQoxDWhv9VOfXxZCHcB39qIM30gziCR","timestamp":1543908667269}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Z4WlMyJVRkzQ","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip3 -qq install torch==0.4.1\n","!pip -qq install torchtext==0.3.1\n","!pip -qq install spacy==2.0.16\n","!pip install -qq gensim==3.6.0\n","!python -m spacy download en\n","!wget -O squad.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1h8dplcVzRkbrSYaTAbXYEAjcbApMxYQL\"\n","!unzip squad.zip\n","!wget -O opensubs.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1x1mNHweP95IeGFbDJPAI7zffgxrbqb7b\"\n","!unzip opensubs.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UvJKy3mtVOpw","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","if torch.cuda.is_available():\n","    from torch.cuda import FloatTensor, LongTensor\n","    DEVICE = torch.device('cuda')\n","else:\n","    from torch import FloatTensor, LongTensor\n","    DEVICE = torch.device('cpu')\n","\n","np.random.seed(42)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LKCD9Pt4Wupj","colab_type":"text"},"cell_type":"markdown","source":["# General Conversation"]},{"metadata":{"id":"C3fbABnaXFyk","colab_type":"text"},"cell_type":"markdown","source":["Сегодня разбираем, как устроена болталка.\n","\n","![](https://meduza.io/image/attachments/images/002/547/612/large/RLnxN4VdUmWFcBp8GjxUmA.jpg =x200)  \n","*From [«Алиса, за мной следит ФСБ?»: в соцсетях продолжают издеваться над голосовым помощником «Яндекса»](https://meduza.io/shapito/2017/10/11/alisa-za-mnoy-sledit-fsb-v-sotssetyah-prodolzhayut-izdevatsya-nad-golosovym-pomoschnikom-yandeksa)*\n","\n","Вообще, мы уже обсудили Seq2Seq модели, которые могут быть использованы для реализации болталки - однако, у них недостаток: высока вероятность сгенерировать что-то неграмматичное. Ну, как те пирожки.\n","\n","Поэтому почти всегда идут другим путем - вместо генерации применяют ранжирование. Нужно заранее составить большую базу ответов и просто выбирать наиболее подходящий к контексту каждый раз."]},{"metadata":{"id":"3mOxTaTjD0-p","colab_type":"text"},"cell_type":"markdown","source":["## DSSM\n","\n","Для этого используют DSSM (Deep Structured Semantic Models):\n","\n","![](https://qph.fs.quoracdn.net/main-qimg-b90431ff9b4c60c5d69069d7bc048ff0)  \n","*From [What are Siamese neural networks, what applications are they good for, and why?](https://www.quora.com/What-are-Siamese-neural-networks-what-applications-are-they-good-for-and-why)*\n","\n","Эта сеть состоит из (обычно) пары башен: левая кодирует запрос, правая - ответ. Задача - научиться считать близость между запросом и ответом.\n","\n","Дальше набирают большой корпус из пар запрос-ответ (запрос может быть как одним вопросом, так и контекстом - несколькими последними вопросами/ответами). \n","\n","Для ответов предпосчитывают их векторы, каждый новый запрос кодируют с помощью правой башни и находят среди предпосчитанных векторов ближайший."]},{"metadata":{"id":"JSz9ALe3w1v1","colab_type":"text"},"cell_type":"markdown","source":["## Данные\n","\n","Будем использовать для начала [Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/). Вообще, там задача - найти в тексте ответ на вопрос. Но мы будем просто выбирать среди предложений текста наиболее близкое к вопросу.\n","\n","*Эта часть ноутбука сильно основана на [шадовском ноутбуке](https://github.com/yandexdataschool/nlp_course/blob/master/week10_dialogue/seminar.ipynb)*."]},{"metadata":{"id":"_6OqXUJxjnX4","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","\n","train_data = pd.read_json('train.json')\n","test_data = pd.read_json('test.json')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s0DPvn5djkha","colab_type":"code","outputId":"fde4d2a6-19d0-4dd5-e6c9-5a90d057c516","executionInfo":{"status":"ok","timestamp":1545166988804,"user_tz":-180,"elapsed":122244,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":159}},"cell_type":"code","source":["row = train_data.iloc[40]\n","print('QUESTION:', row.question, '\\n')\n","for i, cand in enumerate(row.options):\n","    print('[ ]' if i not in row.correct_indices else '[v]', cand)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["QUESTION: In 1874 the telegraph was known as the what of commerce? \n","\n","[v] In 1874, telegraph message traffic was rapidly expanding and in the words of Western Union President William Orton, had become \"the nervous system of commerce\".\n","[ ] Orton had contracted with inventors Thomas Edison and Elisha Gray to find a way to send multiple telegraph messages on each telegraph line to avoid the great cost of constructing new lines.\n","[ ] When Bell mentioned to Gardiner Hubbard and Thomas Sanders that he was working on a method of sending multiple tones on a telegraph wire using a multi-reed device, the two wealthy patrons began to financially support Bell's experiments.\n","[ ] Patent matters would be handled by Hubbard's patent attorney, Anthony Pollok.\n"],"name":"stdout"}]},{"metadata":{"id":"KIeJIQ7ex4nB","colab_type":"text"},"cell_type":"markdown","source":["Токенизируем предложения:"]},{"metadata":{"id":"udkrpaSHq7mg","colab_type":"code","colab":{}},"cell_type":"code","source":["import spacy\n","\n","spacy = spacy.load('en')\n","\n","train_data.question = train_data.question.apply(lambda text: [tok.text.lower() for tok in spacy.tokenizer(text)])\n","train_data.options = train_data.options.apply(lambda options: [[tok.text.lower() for tok in spacy.tokenizer(text)] for text in options])\n","\n","test_data.question = test_data.question.apply(lambda text: [tok.text.lower() for tok in spacy.tokenizer(text)])\n","test_data.options = test_data.options.apply(lambda options: [[tok.text.lower() for tok in spacy.tokenizer(text)] for text in options])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tU5jRHpax7Ot","colab_type":"text"},"cell_type":"markdown","source":["У нас не так-то много данных, чтобы учить всё с нуля, поэтому будем сразу использовать предобученные эмбеддинги:"]},{"metadata":{"id":"2mQjmbvs-jFQ","colab_type":"code","outputId":"4d90fda3-66ab-4695-a418-d79b17d69d6a","executionInfo":{"status":"ok","timestamp":1545167205863,"user_tz":-180,"elapsed":90045,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import gensim.downloader as api\n","\n","w2v_model = api.load('glove-wiki-gigaword-100')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 128.1/128.1MB downloaded\n"],"name":"stdout"}]},{"metadata":{"id":"dC8Db3DKyDSz","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Постройте матрицу предобученных эмбеддингов для самых частотных слов в выборке."]},{"metadata":{"id":"AinFu7nb6DSf","colab_type":"code","colab":{}},"cell_type":"code","source":["from collections import Counter\n","\n","\n","def build_word_embeddings(data, w2v_model, min_freq=5):\n","    words = Counter()\n","    \n","    for text in data.question:\n","        for word in text:\n","            words[word] += 1\n","            \n","    for options in data.options:\n","        for text in options:\n","            for word in text:\n","                words[word] += 1\n","                \n","    word2ind = {\n","        '<pad>': 0,\n","        '<unk>': 1\n","    }\n","    \n","    embeddings = [\n","        np.zeros(w2v_model.vectors.shape[1]),\n","        np.zeros(w2v_model.vectors.shape[1])\n","    ]\n","    \n","    for word, count in words.most_common():\n","        if count < min_freq:\n","            break\n","        \n","        if word not in w2v_model.vocab:\n","            continue\n","            \n","        word2ind[word] = len(word2ind)\n","        embeddings.append(w2v_model.get_vector(word))\n","\n","    return word2ind, np.array(embeddings)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rKJhqyV__jhl","colab_type":"code","outputId":"9fedaaa5-d88a-4aab-9963-dc1f2ae00d79","executionInfo":{"status":"ok","timestamp":1545167210171,"user_tz":-180,"elapsed":77271,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["word2ind, embeddings = build_word_embeddings(train_data, w2v_model, min_freq=8)\n","print('Vocab size =', len(word2ind))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Vocab size = 35350\n"],"name":"stdout"}]},{"metadata":{"id":"M-_xdNG9yYka","colab_type":"text"},"cell_type":"markdown","source":["Для генерации батчей будем использовать такой класс:"]},{"metadata":{"id":"4IoQrJ5rTSSN","colab_type":"code","colab":{}},"cell_type":"code","source":["import random\n","import math\n","\n","\n","def to_matrix(lines, word2ind):\n","    max_sent_len = max(len(line) for line in lines)\n","    matrix = np.zeros((len(lines), max_sent_len))\n","\n","    for batch_ind, line in enumerate(lines):\n","        matrix[batch_ind, :len(line)] = [word2ind.get(word, 1) for word in line]\n","\n","    return LongTensor(matrix)\n","\n","\n","class BatchIterator():\n","    def __init__(self, data, batch_size, word2ind, shuffle=True):\n","        self._data = data\n","        self._num_samples = len(data)\n","        self._batch_size = batch_size\n","        self._word2ind = word2ind\n","        self._shuffle = shuffle\n","        self._batches_count = int(math.ceil(len(data) / batch_size))\n","        \n","    def __len__(self):\n","        return self._batches_count\n","    \n","    def __iter__(self):\n","        return self._iterate_batches()\n","\n","    def _iterate_batches(self):\n","        indices = np.arange(self._num_samples)\n","        if self._shuffle:\n","            np.random.shuffle(indices)\n","\n","        for start in range(0, self._num_samples, self._batch_size):\n","            end = min(start + self._batch_size, self._num_samples)\n","\n","            batch_indices = indices[start: end]\n","\n","            batch = self._data.iloc[batch_indices]\n","            questions = batch['question'].values\n","            correct_answers = np.array([\n","                row['options'][random.choice(row['correct_indices'])]\n","                for i, row in batch.iterrows()\n","            ])\n","            wrong_answers = np.array([\n","                row['options'][random.choice(row['wrong_indices'])]\n","                for i, row in batch.iterrows()\n","            ])\n","\n","            yield {\n","                'questions': to_matrix(questions, self._word2ind),\n","                'correct_answers': to_matrix(correct_answers, self._word2ind),\n","                'wrong_answers': to_matrix(wrong_answers, self._word2ind)\n","            }"],"execution_count":0,"outputs":[]},{"metadata":{"id":"as5kgtjLyRE6","colab_type":"code","colab":{}},"cell_type":"code","source":["train_iter = BatchIterator(train_data, 64, word2ind)\n","test_iter = BatchIterator(test_data, 128, word2ind)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"27zeQlTJzU1x","colab_type":"text"},"cell_type":"markdown","source":["Он просто сэмплирует последовательности из вопросов, правильных и неправильных ответов на них:"]},{"metadata":{"id":"F-ohzmkXzO0e","colab_type":"code","outputId":"47cfc9cf-137d-4f9b-e911-0a7f0a866769","executionInfo":{"status":"ok","timestamp":1545167213437,"user_tz":-180,"elapsed":73215,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"cell_type":"code","source":["batch = next(iter(train_iter))\n","\n","batch"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'correct_answers': tensor([[ 748,   22, 2084,  ...,    0,    0,    0],\n","         [4499,   36,  464,  ...,    0,    0,    0],\n","         [  29,   38, 2609,  ...,    0,    0,    0],\n","         ...,\n","         [  18,    2,   69,  ...,    0,    0,    0],\n","         [  65, 7685,    2,  ...,    0,    0,    0],\n","         [   7,  343, 2127,  ...,    0,    0,    0]], device='cuda:0'),\n"," 'questions': tensor([[  23,   25,  951,  ...,    0,    0,    0],\n","         [  25, 2358,   54,  ...,    0,    0,    0],\n","         [1941, 2609,    8,  ...,    0,    0,    0],\n","         ...,\n","         [  25,   86,   11,  ...,    0,    0,    0],\n","         [  25, 1102,    4,  ...,    0,    0,    0],\n","         [   7, 2127,    9,  ...,    0,    0,    0]], device='cuda:0'),\n"," 'wrong_answers': tensor([[  52, 4346, 1850,  ...,    0,    0,    0],\n","         [ 134, 2229,   26,  ...,    0,    0,    0],\n","         [   2,  955,  332,  ...,    0,    0,    0],\n","         ...,\n","         [ 845,   58,    3,  ...,    0,    0,    0],\n","         [   7,    2,  634,  ...,    0,    0,    0],\n","         [3535,  945,  166,  ...,    0,    0,    0]], device='cuda:0')}"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"BSXFV1CTyfGo","colab_type":"text"},"cell_type":"markdown","source":["## Модель\n","\n","**Задание** Реализуйте модель энкодера для текстов - башни DSSM модели.\n","\n","*Это не обязательно должна быть сложная модель, вполне сойдет сверточная, которая будет учиться гораздо быстрее.*"]},{"metadata":{"id":"Ve7WZ4-dvbuw","colab_type":"code","colab":{}},"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, embeddings, hidden_dim=128, output_dim=128):\n","        super().__init__()\n","        \n","        self._embs = nn.Embedding.from_pretrained(FloatTensor(embeddings))\n","        self._conv = nn.Sequential(\n","            nn.Conv1d(embeddings.shape[1], hidden_dim, kernel_size=3),\n","            nn.ReLU(inplace=True)\n","        )\n","        self._out = nn.Linear(hidden_dim, output_dim)\n","        \n","    def forward(self, inputs):\n","        embs = self._embs(inputs)\n","        embs = embs.permute(0, 2, 1)\n","        \n","        outputs = self._conv(embs)\n","        outputs = torch.max(outputs, -1)[0]\n","        \n","        return self._out(outputs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZFBAUGGfzFwF","colab_type":"text"},"cell_type":"markdown","source":["### Triplet Loss\n","\n","Мы хотим не просто научить энкодер строить эмбеддинги для предложений. Мы хотим, чтобы притягивать векторы правильных ответов к вопросам и отталкивать неправильные. Для этого используют, например, *Triplet Loss*:\n","\n","$$ L = \\frac 1N \\underset {q, a^+, a^-} \\sum max(0, \\space \\delta - sim[V_q(q), V_a(a^+)] + sim[V_q(q), V_a(a^-)] ),$$\n","\n","где\n","* $sim[a, b]$ функция похожести (например, dot product или cosine similarity)\n","* $\\delta$ - гиперпараметр модели. Если $sim[a, b]$ линейно по $b$, то все $\\delta > 0$ эквиватентны.\n","\n","![img](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/margin.png)\n","\n","**Задание** Реализуйте triplet loss, а также подсчет recall - процента случаев, когда правильный ответ был ближе неправильного."]},{"metadata":{"id":"GdHJK51dz0CB","colab_type":"code","colab":{}},"cell_type":"code","source":["class DSSM(nn.Module):\n","    def __init__(self, question_encoder, answer_encoder):\n","        super().__init__()\n","        self.question_encoder = question_encoder\n","        self.answer_encoder = answer_encoder\n","        \n","    def forward(self, questions, correct_answers, wrong_answers):\n","        question_embeddings = self.question_encoder(questions)\n","        correct_answer_embeddings = self.answer_encoder(correct_answers)\n","        wrong_answer_embeddings = self.answer_encoder(wrong_answers)\n","        \n","        return question_embeddings, correct_answer_embeddings, wrong_answer_embeddings\n","\n","    def calc_triplet_loss(self, question_embeddings, correct_answer_embeddings, wrong_answer_embeddings, delta=1.0):\n","        \"\"\"Returns the triplet loss based on the equation above\"\"\"\n","        return F.relu(delta\n","                      - self.similarity(question_embeddings, correct_answer_embeddings)\n","                      + self.similarity(question_embeddings, wrong_answer_embeddings)).mean()\n","        \n","    def calc_recall_at_1(self, question_embeddings, correct_answer_embeddings, wrong_answer_embeddings):\n","        \"\"\"Returns the number of cases when the correct answer were more similar than incorrect one\"\"\"\n","        correct_similarities = self.similarity(question_embeddings, correct_answer_embeddings)\n","        wrong_similarities = self.similarity(question_embeddings, wrong_answer_embeddings)\n","        \n","        \n","        return (correct_similarities > wrong_similarities).float().sum()\n","        \n","    @staticmethod\n","    def similarity(question_embeddings, answer_embeddings):\n","        \"\"\"Returns sim[a, b]\"\"\"\n","        return (question_embeddings * answer_embeddings).sum(-1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZWvlMTWpxTvJ","colab_type":"code","colab":{}},"cell_type":"code","source":["class ModelTrainer():\n","    def __init__(self, model, optimizer):\n","        self._model = model\n","        self._optimizer = optimizer\n","        \n","    def on_epoch_begin(self, is_train, name, batches_count):\n","        \"\"\"\n","        Initializes metrics\n","        \"\"\"\n","        self._epoch_loss = 0\n","        self._correct_count, self._total_count = 0, 0\n","        self._is_train = is_train\n","        self._name = name\n","        self._batches_count = batches_count\n","        \n","        self._model.train(is_train)\n","        \n","    def on_epoch_end(self):\n","        \"\"\"\n","        Outputs final metrics\n","        \"\"\"\n","        return '{:>5s} Loss = {:.5f}, Recall@1 = {:.2%}'.format(\n","            self._name, self._epoch_loss / self._batches_count, self._correct_count / self._total_count\n","        )\n","        \n","    def on_batch(self, batch):\n","        \"\"\"\n","        Performs forward and (if is_train) backward pass with optimization, updates metrics\n","        \"\"\"\n","        \n","        question_embs, correct_answer_embs, wrong_answer_embs = self._model(\n","            batch['questions'], batch['correct_answers'], batch['wrong_answers']\n","        )\n","        loss = self._model.calc_triplet_loss(question_embs, correct_answer_embs, wrong_answer_embs)\n","        correct_count = self._model.calc_recall_at_1(question_embs, correct_answer_embs, wrong_answer_embs)\n","        total_count = len(batch['questions'])\n","        \n","        self._correct_count += correct_count\n","        self._total_count += total_count\n","        self._epoch_loss += loss.item()\n","        \n","        if self._is_train:\n","            self._optimizer.zero_grad()\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(self._model.parameters(), 1.)\n","            self._optimizer.step()\n","\n","        return '{:>5s} Loss = {:.5f}, Recall@1 = {:.2%}'.format(\n","            self._name, loss.item(), correct_count / total_count\n","        )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ecI_vVBgzpVn","colab_type":"code","colab":{}},"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(trainer, data_iter, is_train, name=None):\n","    trainer.on_epoch_begin(is_train, name, batches_count=len(data_iter))\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=len(data_iter)) as progress_bar:\n","            for i, batch in enumerate(data_iter):\n","                batch_progress = trainer.on_batch(batch)\n","\n","                progress_bar.update()\n","                progress_bar.set_description(batch_progress)\n","                \n","            epoch_progress = trainer.on_epoch_end()\n","            progress_bar.set_description(epoch_progress)\n","            progress_bar.refresh()\n","\n","            \n","def fit(trainer, train_iter, epochs_count=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        do_epoch(trainer, train_iter, is_train=True, name=name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            do_epoch(trainer, val_iter, is_train=False, name=name_prefix + '  Val:')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K1GvkkLh70S6","colab_type":"text"},"cell_type":"markdown","source":["Запустим, наконец, учиться модель:"]},{"metadata":{"id":"ipXj9pOD2afY","colab_type":"code","outputId":"b7133046-b45e-4ff3-c399-1e12371d8c0f","executionInfo":{"status":"ok","timestamp":1545167424488,"user_tz":-180,"elapsed":257528,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["embeddings = FloatTensor(embeddings)\n","\n","model = DSSM(\n","    Encoder(embeddings),\n","    Encoder(embeddings)\n",").to(DEVICE)\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","trainer = ModelTrainer(model, optimizer)\n","\n","fit(trainer, train_iter, epochs_count=5, val_iter=test_iter)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[1 / 5] Train: Loss = 0.67149, Recall@1 = 70.14%: 100%|██████████| 909/909 [00:33<00:00, 27.53it/s]\n","[1 / 5]   Val: Loss = 0.57129, Recall@1 = 75.62%: 100%|██████████| 211/211 [00:09<00:00, 23.04it/s]\n","[2 / 5] Train: Loss = 0.51111, Recall@1 = 78.38%: 100%|██████████| 909/909 [00:33<00:00, 28.18it/s]\n","[2 / 5]   Val: Loss = 0.54672, Recall@1 = 77.06%: 100%|██████████| 211/211 [00:09<00:00, 23.27it/s]\n","[3 / 5] Train: Loss = 0.45427, Recall@1 = 81.28%: 100%|██████████| 909/909 [00:33<00:00, 28.24it/s]\n","[3 / 5]   Val: Loss = 0.55122, Recall@1 = 77.53%: 100%|██████████| 211/211 [00:08<00:00, 23.52it/s]\n","[4 / 5] Train: Loss = 0.42037, Recall@1 = 82.79%: 100%|██████████| 909/909 [00:33<00:00, 27.95it/s]\n","[4 / 5]   Val: Loss = 0.54098, Recall@1 = 78.44%: 100%|██████████| 211/211 [00:09<00:00, 22.84it/s]\n","[5 / 5] Train: Loss = 0.39392, Recall@1 = 84.08%: 100%|██████████| 909/909 [00:33<00:00, 27.57it/s]\n","[5 / 5]   Val: Loss = 0.54709, Recall@1 = 78.24%: 100%|██████████| 211/211 [00:09<00:00, 23.34it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"g_AHIoCB73XA","colab_type":"text"},"cell_type":"markdown","source":["### Точность предсказаний\n","\n","Оценим, насколько хорошо модель предсказывает правильный ответ.\n","\n","**Задание** Для каждого вопроса найдите индекс ответа, генерируемого сетью:"]},{"metadata":{"id":"_JMPpwQdazkw","colab_type":"code","colab":{}},"cell_type":"code","source":["from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QrA8N0zDJczj","colab_type":"code","outputId":"3f4c1aad-ed92-4b20-da51-ae6dfe190fc3","executionInfo":{"status":"ok","timestamp":1545167483896,"user_tz":-180,"elapsed":311477,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["predictions = []\n","for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n","    question_embeddings = model.question_encoder(to_matrix([row.question], word2ind))\n","    answer_embeddings = model.answer_encoder(to_matrix(row.options, word2ind))\n","    \n","    predictions.append(model.similarity(question_embeddings, answer_embeddings).argmax())\n","    \n","accuracy = np.mean([\n","    answer in correct_ind\n","    for answer, correct_ind in zip(predictions, test_data['correct_indices'].values)\n","])\n","print(\"Accuracy: %0.5f\" % accuracy)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["100%|██████████| 26970/26970 [00:57<00:00, 469.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.53211\n"],"name":"stdout"}]},{"metadata":{"id":"HZijVoOTLcCD","colab_type":"code","colab":{}},"cell_type":"code","source":["def draw_results(question, possible_answers, predicted_index, correct_indices):\n","    print(\"Q:\", ' '.join(question), end='\\n\\n')\n","    for i, answer in enumerate(possible_answers):\n","        print(\"#%i: %s %s\" % (i, '[*]' if i == predicted_index else '[ ]', ' '.join(answer)))\n","    \n","    print(\"\\nVerdict:\", \"CORRECT\" if predicted_index in correct_indices else \"INCORRECT\", \n","          \"(ref: %s)\" % correct_indices, end='\\n' * 3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-_pYbNcsLfli","colab_type":"code","outputId":"439f93b1-a62e-4dc2-fa00-f49d18a6770e","executionInfo":{"status":"ok","timestamp":1545167616415,"user_tz":-180,"elapsed":1176,"user":{"displayName":"Evgeny Rubanenko","photoUrl":"","userId":"03856158561714992485"}},"colab":{"base_uri":"https://localhost:8080/","height":1431}},"cell_type":"code","source":["for i in [1, 100, 1000, 2000, 3000, 4000, 5000]:\n","    draw_results(test_data.iloc[i].question, test_data.iloc[i].options,\n","                 predictions[i], test_data.iloc[i].correct_indices)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Q: what did bell call his special spot in the back of the property ?\n","\n","#0: [*] at the homestead , bell set up his own workshop in the converted carriage house near to what he called his \" dreaming place \" , a large hollow nestled in trees at the back of the property above the river .\n","#1: [ ] despite his frail condition upon arriving in canada , bell found the climate and environs to his liking , and rapidly improved .\n","#2: [ ] [ n 10 ] he continued his interest in the study of the human voice and when he discovered the six nations reserve across the river at onondaga , he learned the mohawk language and translated its unwritten vocabulary into visible speech symbols .\n","#3: [ ] for his work , bell was awarded the title of honorary chief and participated in a ceremony where he donned a mohawk headdress and danced traditional dances .\n","#4: [ ] [ n 11 ]\n","\n","Verdict: CORRECT (ref: [0])\n","\n","\n","Q: how much more gin than beer was made in england in 1740 ?\n","\n","#0: [ ] the 18th century saw a huge growth in the number of drinking establishments , primarily due to the introduction of gin .\n","#1: [ ] gin was brought to england by the dutch after the glorious revolution of 1688 and became very popular after the government created a market for \" cuckoo grain \" or \" cuckoo malt \" that was unfit to be used in brewing and distilling by allowing unlicensed gin and beer production , while imposing a heavy duty on all imported spirits .\n","#2: [ ] as thousands of gin - shops sprang up all over england , brewers fought back by increasing the number of alehouses .\n","#3: [*] by 1740 the production of gin had increased to six times that of beer and because of its cheapness it became popular with the poor , leading to the so - called gin craze .\n","#4: [ ] over half of the 15,000 drinking establishments in london were gin shops .\n","\n","Verdict: CORRECT (ref: [3])\n","\n","\n","Q: in the nominative , genitive , partitive , illative , inessive , elative , allative , adessive , ablative , translative cases what always agrees with the noun in number and case ?\n","\n","#0: [*] in estonian , nouns and pronouns do not have grammatical gender , but nouns and adjectives decline in fourteen cases : nominative , genitive , partitive , illative , inessive , elative , allative , adessive , ablative , translative , terminative , essive , abessive , and comitative , with the case and number of the adjective(s ) always agreeing with that of the noun ( except in the terminative , essive , abessive and comitative , where there is agreement only for the number , the adjective being in the genitive form ) .\n","#1: [ ] thus the illative for kollane maja ( \" a yellow house \" ) is kollasesse majja ( \" into a yellow house \" ) , but the terminative is kollase majani ( \" as far as a yellow house \" ) .\n","#2: [ ] with respect to the proto - finnic language , elision has occurred ; thus , the actual case marker may be absent , but the stem is changed , cf .\n","#3: [ ] maja – majja and the pohjanmaa dialect of finnish maja – majahan .\n","\n","Verdict: CORRECT (ref: [0])\n","\n","\n","Q: when did john melody , the town crier of southampton , pass away ?\n","\n","#0: [ ] the city has a mayor and is one of the 16 cities and towns in england and wales to have a ceremonial sheriff who acts as a deputy for the mayor .\n","#1: [ ] the current and 793rd mayor of southampton is linda norris .\n","#2: [ ] catherine mcewing is the current and 578th sherriff .\n","#3: [*] the town crier from 2004 until his death in 2014 was john melody , who acted as master of ceremonies in the city and who possessed a cry of 104 decibels .\n","\n","Verdict: CORRECT (ref: [3])\n","\n","\n","Q: what does drm stand for ?\n","\n","#0: [ ] the ipod line can play several audio file formats including mp3 , aac / m4a , protected aac , aiff , wav , audible audiobook , and apple lossless .\n","#1: [ ] the ipod photo introduced the ability to display jpeg , bmp , gif , tiff , and png image file formats .\n","#2: [ ] fifth and sixth generation ipod classics , as well as third generation ipod nanos , can additionally play mpeg-4 ( h.264/mpeg-4 avc ) and quicktime video formats , with restrictions on video dimensions , encoding techniques and data - rates .\n","#3: [ ] originally , ipod software only worked with mac os ; ipod software for microsoft windows was launched with the second generation model .\n","#4: [*] unlike most other media players , apple does not support microsoft 's wma audio format — but a converter for wma files without digital rights management ( drm ) is provided with the windows version of itunes .\n","#5: [ ] midi files also can not be played , but can be converted to audio files using the \" advanced \" menu in itunes .\n","#6: [ ] alternative open - source audio formats , such as ogg vorbis and flac , are not supported without installing custom firmware onto an ipod ( e.g. , rockbox ) .\n","\n","Verdict: CORRECT (ref: [4])\n","\n","\n","Q: newborns can acquire antibodies from the mother through what means ?\n","\n","#0: [ ] maternal factors also play a role in the body ’s immune response .\n","#1: [ ] at birth , most of the immunoglobulin present is maternal igg.\n","#2: [ ] because igm , igd , ige and iga do n’t cross the placenta , they are almost undetectable at birth .\n","#3: [ ] some iga is provided by breast milk .\n","#4: [ ] these passively - acquired antibodies can protect the newborn for up to 18 months , but their response is usually short - lived and of low affinity .\n","#5: [ ] these antibodies can also produce a negative response .\n","#6: [ ] if a child is exposed to the antibody for a particular antigen before being exposed to the antigen itself then the child will produce a dampened response .\n","#7: [ ] passively acquired maternal antibodies can suppress the antibody response to active immunization .\n","#8: [*] similarly the response of t - cells to vaccination differs in children compared to adults , and vaccines that induce th1 responses in adults do not readily elicit these same responses in neonates .\n","#9: [ ] between six to nine months after birth , a child ’s immune system begins to respond more strongly to glycoproteins , but there is usually no marked improvement in their response to polysaccharides until they are at least one year old .\n","#10: [ ] this can be the reason for distinct time frames found in vaccination schedules .\n","\n","Verdict: INCORRECT (ref: [3])\n","\n","\n","Q: how many caretaker managers have their been in the everton fc 's history ?\n","\n","#0: [ ] current manager , roberto martínez , is the fourteenth permanent holder of the position since it was established in 1939 .\n","#1: [*] there have also been four caretaker managers , and before 1939 the team was selected by either the club secretary or by committee .\n","#2: [ ] the club 's longest - serving manager has been harry catterick , who was in charge of the team from 1961–73 , taking in 594 first team matches .\n","#3: [ ] the everton manager to win most domestic and international trophies is howard kendall , who won two division one championships , the 1984 fa cup , the 1984 uefa cup winners ' cup , and three charity shields .\n","\n","Verdict: CORRECT (ref: [1])\n","\n","\n"],"name":"stdout"}]},{"metadata":{"id":"T5OC4EGt8HAo","colab_type":"text"},"cell_type":"markdown","source":["## Hard-negatives mining\n","\n","На самом деле, в большинстве случаев у нас отрицательных примеров.\n","\n","Например, есть база диалогов - и где брать отрицательные примеры к ответам?\n","\n","Для этого используют *hard-negatives mining*. Берут в качестве отрицательного примера самый близкий из неправильных примеров в батче:\n","$$a^-_{hard} = \\underset {a^-} {argmax} \\space sim[V_q(q), V_a(a^-)]$$\n","\n","Неправильные в данном случае - все, кроме правильного :)\n","\n","Реализуется это как-то так:\n","* Батч состоит из правильных пар вопрос-ответ.\n","* Для всех вопросов и всех ответов считают эмбеддинги.\n","* Положительные примеры у нас есть - осталось найти для каждого вопроса наиболее похожие на него ответы, которые предназначались другим вопросам.\n","\n","**Задание** Обновите `DSSM`, чтобы делать hard-negatives mining внутри него.\n","\n","*Может понадобиться нормализовывать векторы с помощью `F.normalize` перед подсчетом `similarity`*"]},{"metadata":{"id":"tY_BebgAOY70","colab_type":"code","colab":{}},"cell_type":"code","source":["class DSSM(nn.Module):\n","    def __init__(self, question_encoder, answer_encoder):\n","        super().__init__()\n","        self.question_encoder = question_encoder\n","        self.answer_encoder = answer_encoder\n","        \n","    def forward(self, questions, correct_answers, wrong_answers):\n","        \"\"\"Ignore wrong_answers, they are here just for compatibility sake\"\"\"\n","        <perform forward pass>\n","\n","    def calc_triplet_loss(self, question_embeddings, answer_embeddings, delta=1.0):\n","        \"\"\"Returns the triplet loss based on the equation above\"\"\"\n","        <calc triple loss with hard-negatives>\n","        \n","    def calc_recall_at_1(self, question_embeddings, answer_embeddings):\n","        \"\"\"Returns the number of cases when the correct answer were more similar than incorrect one\"\"\"\n","        <calc recall>\n","        \n","    @staticmethod\n","    def similarity(question_embeddings, answer_embeddings):\n","        <calc it>"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0AIvf3Zvtabs","colab_type":"code","colab":{}},"cell_type":"code","source":["model = DSSM(\n","    question_encoder=Encoder(embeddings),\n","    answer_encoder=Encoder(embeddings)\n",").to(DEVICE)\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","trainer = ModelTrainer(model, optimizer)\n","\n","fit(trainer, train_iter, epochs_count=30, val_iter=test_iter)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pXaL1iRz-zEG","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Есть также вариант с semi-hard negatives - когда в качестве отрицательного примера берется наилучший среди тех, чья similarity меньше similarity вопроса с положительным примером. Попробуйте реализовать его."]},{"metadata":{"id":"edUVeXG0_lCa","colab_type":"text"},"cell_type":"markdown","source":["# Болталка\n","\n","Чтобы реализовать болталку, нужен нормальный корпус с диалогами. Например, OpenSubtitles."]},{"metadata":{"id":"s3YmDo9Z_xG-","colab_type":"code","colab":{}},"cell_type":"code","source":["!head train.txt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A0R77ezW_1Wd","colab_type":"text"},"cell_type":"markdown","source":["Ну, примерно нормальный.\n","\n","Считаем датасет."]},{"metadata":{"id":"SwHYmb28HPUn","colab_type":"code","colab":{}},"cell_type":"code","source":["from nltk import wordpunct_tokenize\n","\n","def read_dataset(path):\n","    data = []\n","    with open(path) as f:\n","        for line in tqdm(f):\n","            query, response = line.strip().split('\\t')\n","            data.append((\n","                wordpunct_tokenize(query.strip()),\n","                wordpunct_tokenize(response.strip())\n","            ))\n","    return data\n","\n","train_data = read_dataset('train.txt')\n","val_data = read_dataset('valid.txt')\n","test_data = read_dataset('test.txt')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WTfkv17tHM3I","colab_type":"code","colab":{}},"cell_type":"code","source":["from torchtext.data import Field, Example, Dataset, BucketIterator\n","\n","query_field = Field(lower=True)\n","response_field = Field(lower=True)\n","\n","fields = [('query', query_field), ('response', response_field)]\n","\n","train_dataset = Dataset([Example.fromlist(example, fields) for example in train_data], fields)\n","val_dataset = Dataset([Example.fromlist(example, fields) for example in val_data], fields)\n","test_dataset = Dataset([Example.fromlist(example, fields) for example in test_data], fields)\n","\n","query_field.build_vocab(train_dataset, min_freq=5)\n","response_field.build_vocab(train_dataset, min_freq=5)\n","\n","print('Query vocab size =', len(query_field.vocab))\n","print('Response vocab size =', len(response_field.vocab))\n","\n","train_iter, val_iter, test_iter = BucketIterator.splits(\n","    datasets=(train_dataset, val_dataset, test_dataset), batch_sizes=(512, 1024, 1024), \n","    shuffle=True, device=DEVICE, sort=False\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M-Ok7--NHUX0","colab_type":"text"},"cell_type":"markdown","source":["**Задание** Реализовать болталку по аналогии с тем, что уже написали."]},{"metadata":{"id":"n2x9-j4oz08p","colab_type":"text"},"cell_type":"markdown","source":["# Дополнительные материалы\n","\n","## Статьи\n","Learning Deep Structured Semantic Models for Web Search using Clickthrough Data, 2013 [[pdf]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf)  \n","Deep Learning and Continuous Representations for Natural Language Processing, Microsoft tutorial [[pdf]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/NAACL-HLT-2015_tutorial.pdf)\n","\n","## Блоги\n","[Neural conversational models: как научить нейронную сеть светской беседе](https://habr.com/company/yandex/blog/333912/)  \n","[Искусственный интеллект в поиске. Как Яндекс научился применять нейронные сети, чтобы искать по смыслу, а не по словам](https://habr.com/company/yandex/blog/314222/)  \n","[Triplet loss, Olivier Moindrot](https://omoindrot.github.io/triplet-loss)"]},{"metadata":{"id":"gjkS1Kpkz4Aa","colab_type":"text"},"cell_type":"markdown","source":["# Сдача\n","\n","[Форма для сдачи](https://goo.gl/forms/bf2auPe8FL5C0jzp2)  \n","[Feedback](https://goo.gl/forms/9aizSzOUrx7EvGlG3)"]}]}